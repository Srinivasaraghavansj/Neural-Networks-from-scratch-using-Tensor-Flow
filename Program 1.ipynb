{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL A1 Question1_1_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhEu3ph3mmnZ",
        "outputId": "a92053d5-b35e-4f8a-87e7-8298775a2c6d"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuM6HeNym-fG"
      },
      "source": [
        "# import required libraries\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Setting Random Seed\n",
        "tf.random.set_seed(195470)\n",
        "\n",
        "# load and prepare the training and test data\n",
        "def load_Fashion_MNIST():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "    # load the training and test data    \n",
        "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
        "\n",
        "    # reshape the feature data\n",
        "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
        "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
        "\n",
        "    # noramlise feature data\n",
        "    tr_x = tr_x / 255.0\n",
        "    te_x = te_x / 255.0\n",
        "\n",
        "    print( \"Shape of training features \", tr_x.shape)\n",
        "    print( \"Shape of test features \", te_x.shape)\n",
        "\n",
        "\n",
        "    # one hot encode the training labels and get the transpose\n",
        "    tr_y = np_utils.to_categorical(tr_y,10)\n",
        "    tr_y = tr_y.T\n",
        "    print (\"Shape of training labels \", tr_y.shape)\n",
        "\n",
        "    # one hot encode the test labels and get the transpose\n",
        "    te_y = np_utils.to_categorical(te_y,10)\n",
        "    te_y = te_y.T\n",
        "    print (\"Shape of testing labels \", te_y.shape)\n",
        "    \n",
        "    # Reshape the training data and test data so \n",
        "    # that the features becomes the rows of the matrix\n",
        "    tr_x = tr_x.T\n",
        "    te_x = te_x.T\n",
        "\n",
        "    print(\"Reshaped training data \", tr_x.shape)\n",
        "    print(\"Reshaped test data \",te_x.shape)\n",
        "    \n",
        "    return tr_x, tr_y, te_x, te_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzqqwhfpoDzN"
      },
      "source": [
        "#  push a matrix of feature data through the neural network and the final Softmax layer\n",
        "def forward_pass(x, w_1, b1, w_2, b2):\n",
        "\n",
        "    # First Layer\n",
        "    A1 = tf.matmul(w_1, x) + b1 #WX + b\n",
        "    H1 =  tf.keras.activations.relu(A1) #ReLu activation\n",
        "    \n",
        "    # Second Layer / Output Layer\n",
        "    A2 = tf.matmul(w_2, H1) + b2 #WX + b\n",
        "    #SoftMax activation\n",
        "    e_A2 = tf.math.exp(A2) #e^A2\n",
        "    return tf.divide(e_A2,tf.reshape(tf.reduce_sum(e_A2,0),[1, -1])) #e^A2/sum(e^A2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BtDUop-oU_Z"
      },
      "source": [
        "\n",
        "def cross_entropy(y, y_pred):\n",
        "    # mean of cross entropy loss\n",
        "    return tf.reduce_mean(-(tf.reduce_sum(tf.multiply(y, tf.math.log(y_pred)), 0))) #mean(-sum(y*log(h(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My675H4XoWfv"
      },
      "source": [
        "\n",
        "def calculate_accuracy(y, y_pred_softmax):\n",
        "    #Compare every predicted value to original value individually which will result in boolean array which will be casted to zeros and ones\n",
        "    #Taking the mean of this array will give us the accuracy since 1 is correct and 0 is wrong and the mean is accuracy!\n",
        "    #Example tf.Tensor([ True False False ... False False False], shape=(60000,), dtype=bool) casted to [1. 0. 0. ... 0. 0. 0.], shape=(60000,)\n",
        "    #, dtype=float32)\n",
        "    #and the Mean gives 0.10131667 which is the accuracy\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_pred_softmax, 0), tf.argmax(y, 0)), tf.float32))  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYThBBUr9Yf8"
      },
      "source": [
        "#Function to type cast to float32 by default\n",
        "def fl_type_caster(x,t=tf.float32):\n",
        "  return tf.cast(x,t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5-bCvp3oYNd",
        "outputId": "0aba035a-6a60-4841-d772-c1c8d73e7d67"
      },
      "source": [
        "#No of iterations\n",
        "epochs = 500\n",
        "training_accuracy, training_loss, validation_accuracy, validation_loss = [],[],[],[]\n",
        "\n",
        "# load the prepared data and typecast the training and test data\n",
        "tr_x, tr_y, te_x, te_y = [fl_type_caster(i) for i in load_Fashion_MNIST()]\n",
        "\n",
        "#Initializing the weights and biases for 1st layer\n",
        "w1 = tf.Variable(tf.random.normal([200, tr_x.shape[0]], mean=0.0, stddev=0.1))\n",
        "b1 = tf.Variable(tf.zeros([200, 1]))\n",
        "#Initialize the weights and biases for the SoftMax ie Output layer\n",
        "w2 = tf.Variable(tf.random.normal([tr_y.shape[0], 200], mean=0.0, stddev=0.1))\n",
        "b2 = tf.Variable(tf.zeros([tr_y.shape[0], 1]))\n",
        "\n",
        "#Instantiate Optimizer\n",
        "adam_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training loop\n",
        "for i in range(epochs):\n",
        "    if i%10 == 0:\n",
        "      print(\"+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+\".format(\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19))\n",
        "      print(\"| {:^19} | {:^19} | {:^19} | {:^19} | {:^19} |\".format(\"Epoch\",\"Training_loss\",\"Training_accuracy\",\"Validation_loss\",\"Validation_accuracy\"))\n",
        "      print(\"+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+\".format(\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19))\n",
        "\n",
        "    # Instantiating Gradient Tape to monitor the forward pass & calculate gradients ie autodiff\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = forward_pass(tr_x, w1, b1, w2, b2)\n",
        "        loss = cross_entropy(tr_y, y_pred)\n",
        "    training_loss.append(loss)\n",
        "    \n",
        "    # Prediction Accuracy\n",
        "    accuracy = calculate_accuracy(tr_y, y_pred)\n",
        "    training_accuracy.append(accuracy)\n",
        "\n",
        "    # Calculate gradients\n",
        "    gradients = tape.gradient(loss, [w1, b1, w2, b2])\n",
        "\n",
        "    # Forward propagate and calculate accuracy and loss for the valdation data\n",
        "    te_y_pred = forward_pass(te_x, w1, b1, w2, b2) \n",
        "    val_loss = cross_entropy(te_y, te_y_pred)\n",
        "    val_accuracy = calculate_accuracy(te_y, te_y_pred) \n",
        "    validation_accuracy.append(val_accuracy)\n",
        "    validation_loss.append(val_loss)\n",
        "\n",
        "    print(\"| {:^19} | {:^19} | {:^19} | {:^19} | {:^19} |\".format(i+1,round(float(loss.numpy()),5),round(float(accuracy.numpy()),5),round(float(val_loss.numpy()),5),round(float(val_accuracy.numpy()),5)))\n",
        "    \n",
        "    # Adam Optimizer to update the weights and biases accordingly\n",
        "    adam_optimizer.apply_gradients(zip(gradients, [w1, b1, w2, b2]))\n",
        "\n",
        "print(\"+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+\".format(\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19))\n",
        "\n",
        "# Plot the training and the validation accuracy and loss\n",
        "plt.plot(training_accuracy, label=\"Train Accuracy\",color='magenta')\n",
        "plt.plot(training_loss, label=\"Train Loss\",color='orange')\n",
        "plt.plot(validation_accuracy, label=\"Validation Accuracy\",color='green')\n",
        "plt.plot(validation_loss, label=\"Validation Loss\",color='red')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Shape of training features  (60000, 784)\n",
            "Shape of test features  (10000, 784)\n",
            "Shape of training labels  (10, 60000)\n",
            "Shape of testing labels  (10, 10000)\n",
            "Reshaped training data  (784, 60000)\n",
            "Reshaped test data  (784, 10000)\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|          1          |       2.91471       |       0.0557        |       2.90919       |       0.0578        |\n",
            "|          2          |       2.32222       |       0.14802       |       2.31836       |        0.146        |\n",
            "|          3          |       1.9986        |       0.28288       |       1.99687       |       0.2836        |\n",
            "|          4          |       1.78705       |       0.3843        |       1.78715       |       0.3844        |\n",
            "|          5          |       1.61583       |       0.44492       |       1.61763       |       0.4445        |\n",
            "|          6          |       1.45005       |       0.51945       |       1.45349       |       0.5173        |\n",
            "|          7          |       1.2978        |       0.58845       |       1.30252       |       0.5821        |\n",
            "|          8          |       1.17807       |       0.63178       |       1.18364       |       0.6294        |\n",
            "|          9          |       1.09602       |       0.64883       |       1.10238       |       0.6432        |\n",
            "|         10          |       1.03949       |       0.65495       |       1.04677       |       0.6523        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         11          |       0.99303       |       0.66452       |       1.00117       |       0.6606        |\n",
            "|         12          |       0.94927       |       0.68065       |       0.95842       |       0.6721        |\n",
            "|         13          |       0.90772       |       0.69617       |       0.91804       |       0.6895        |\n",
            "|         14          |       0.87133       |       0.71125       |       0.88281       |       0.7059        |\n",
            "|         15          |       0.84123       |       0.72022       |       0.85381       |       0.7129        |\n",
            "|         16          |       0.81531       |       0.7265        |       0.8289        |       0.7206        |\n",
            "|         17          |       0.7931        |       0.73138       |       0.80769       |       0.7259        |\n",
            "|         18          |       0.77488       |       0.73548       |       0.79055       |       0.7286        |\n",
            "|         19          |        0.759        |       0.73897       |       0.77582       |       0.7309        |\n",
            "|         20          |       0.74418       |       0.74317       |       0.76222       |       0.7336        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         21          |       0.73074       |       0.7493        |       0.7499        |        0.738        |\n",
            "|         22          |       0.71814       |       0.75512       |       0.73801       |       0.7424        |\n",
            "|         23          |       0.70539       |       0.76125       |       0.72539       |       0.7486        |\n",
            "|         24          |       0.69356       |       0.76592       |       0.71325       |        0.754        |\n",
            "|         25          |       0.68343       |       0.76985       |       0.70267       |       0.7564        |\n",
            "|         26          |       0.67407       |       0.77297       |       0.69289       |       0.7605        |\n",
            "|         27          |       0.66494       |       0.77443       |       0.6835        |       0.7635        |\n",
            "|         28          |       0.65619       |       0.77605       |       0.6746        |       0.7672        |\n",
            "|         29          |       0.64772       |       0.7774        |       0.66603       |        0.768        |\n",
            "|         30          |       0.63967       |       0.77997       |       0.65794       |       0.7702        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         31          |       0.63215       |       0.78213       |       0.6505        |       0.7728        |\n",
            "|         32          |       0.62465       |       0.78492       |       0.64323       |       0.7768        |\n",
            "|         33          |       0.61706       |       0.78902       |       0.63605       |       0.7791        |\n",
            "|         34          |       0.60985       |       0.79267       |       0.62935       |       0.7813        |\n",
            "|         35          |       0.60323       |       0.79645       |       0.62321       |       0.7833        |\n",
            "|         36          |       0.59725       |       0.79955       |       0.61759       |       0.7878        |\n",
            "|         37          |       0.59183       |       0.80128       |       0.61252       |       0.7909        |\n",
            "|         38          |       0.58648       |       0.80362       |       0.60755       |       0.7936        |\n",
            "|         39          |       0.58104       |       0.80545       |       0.60255       |        0.796        |\n",
            "|         40          |       0.57578       |       0.8066        |       0.59767       |       0.7979        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         41          |       0.57077       |       0.8079        |       0.59293       |       0.7986        |\n",
            "|         42          |       0.56611       |       0.80925       |       0.58838       |       0.8006        |\n",
            "|         43          |       0.56171       |       0.81018       |       0.58406       |       0.8017        |\n",
            "|         44          |       0.55734       |       0.81182       |       0.57982       |       0.8039        |\n",
            "|         45          |       0.55313       |       0.81308       |       0.57579       |       0.8057        |\n",
            "|         46          |       0.54913       |       0.81475       |       0.57198       |       0.8065        |\n",
            "|         47          |       0.54532       |       0.81573       |       0.56836       |       0.8079        |\n",
            "|         48          |       0.54169       |       0.81678       |        0.565        |        0.81         |\n",
            "|         49          |       0.53808       |       0.8182        |       0.56179       |       0.8097        |\n",
            "|         50          |       0.53453       |       0.81928       |       0.55871       |       0.8114        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         51          |       0.5311        |       0.82018       |       0.55573       |       0.8118        |\n",
            "|         52          |       0.52779       |       0.82143       |       0.55276       |       0.8125        |\n",
            "|         53          |       0.52467       |       0.82202       |       0.54992       |        0.814        |\n",
            "|         54          |       0.52166       |       0.82257       |       0.54724       |       0.8145        |\n",
            "|         55          |       0.5187        |       0.82387       |       0.54463       |       0.8153        |\n",
            "|         56          |       0.5158        |       0.82525       |       0.54204       |       0.8162        |\n",
            "|         57          |       0.51294       |       0.8261        |       0.53939       |       0.8172        |\n",
            "|         58          |       0.51021       |       0.82673       |       0.53683       |       0.8182        |\n",
            "|         59          |       0.50758       |       0.82755       |       0.53438       |       0.8196        |\n",
            "|         60          |       0.50501       |       0.82852       |       0.53201       |       0.8198        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         61          |       0.50251       |       0.82935       |       0.52968       |       0.8208        |\n",
            "|         62          |       0.50006       |       0.82975       |       0.5274        |       0.8215        |\n",
            "|         63          |       0.49768       |       0.83057       |       0.52521       |       0.8219        |\n",
            "|         64          |       0.49536       |       0.83145       |       0.52313       |       0.8221        |\n",
            "|         65          |       0.49309       |       0.83213       |       0.52114       |       0.8229        |\n",
            "|         66          |       0.49088       |       0.83317       |       0.51919       |       0.8233        |\n",
            "|         67          |       0.4887        |       0.83382       |       0.51724       |       0.8237        |\n",
            "|         68          |       0.48658       |       0.83462       |       0.51533       |       0.8244        |\n",
            "|         69          |       0.48449       |       0.83563       |       0.51349       |       0.8247        |\n",
            "|         70          |       0.48246       |       0.83602       |       0.51171       |        0.825        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         71          |       0.48046       |       0.83653       |       0.50996       |       0.8261        |\n",
            "|         72          |       0.47849       |       0.83715       |       0.50823       |       0.8262        |\n",
            "|         73          |       0.47657       |       0.83775       |       0.50656       |       0.8265        |\n",
            "|         74          |       0.47469       |       0.83818       |       0.50492       |       0.8271        |\n",
            "|         75          |       0.47284       |       0.83887       |       0.50327       |       0.8281        |\n",
            "|         76          |       0.47102       |       0.83975       |       0.5016        |       0.8292        |\n",
            "|         77          |       0.46922       |       0.8405        |       0.49994       |       0.8299        |\n",
            "|         78          |       0.46745       |       0.84073       |       0.49832       |       0.8308        |\n",
            "|         79          |       0.46571       |       0.84137       |       0.49674       |       0.8308        |\n",
            "|         80          |        0.464        |       0.84183       |       0.49518       |       0.8317        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         81          |       0.46231       |       0.84242       |       0.49364       |       0.8322        |\n",
            "|         82          |       0.46065       |       0.84308       |       0.49213       |       0.8329        |\n",
            "|         83          |       0.45901       |       0.84353       |       0.49067       |       0.8328        |\n",
            "|         84          |       0.45739       |       0.84405       |       0.48922       |       0.8338        |\n",
            "|         85          |       0.45581       |       0.84442       |       0.4878        |       0.8343        |\n",
            "|         86          |       0.45424       |       0.84488       |       0.48641       |       0.8348        |\n",
            "|         87          |       0.4527        |       0.84532       |       0.48506       |       0.8352        |\n",
            "|         88          |       0.45119       |        0.846        |       0.48374       |       0.8351        |\n",
            "|         89          |       0.4497        |       0.84667       |       0.48243       |       0.8351        |\n",
            "|         90          |       0.44822       |       0.84718       |       0.48113       |       0.8355        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         91          |       0.44677       |       0.84798       |       0.47985       |       0.8357        |\n",
            "|         92          |       0.44533       |       0.84832       |       0.47858       |       0.8358        |\n",
            "|         93          |       0.44391       |       0.84873       |       0.47731       |       0.8363        |\n",
            "|         94          |       0.4425        |       0.84898       |       0.47606       |       0.8369        |\n",
            "|         95          |       0.44111       |       0.8497        |       0.47484       |       0.8371        |\n",
            "|         96          |       0.43973       |       0.85017       |       0.47362       |       0.8377        |\n",
            "|         97          |       0.43837       |       0.85057       |       0.47239       |       0.8385        |\n",
            "|         98          |       0.43702       |        0.851        |       0.47118       |       0.8391        |\n",
            "|         99          |       0.43569       |       0.85125       |        0.47         |       0.8391        |\n",
            "|         100         |       0.43437       |       0.8515        |       0.46884       |       0.8397        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         101         |       0.43306       |       0.85193       |       0.46769       |       0.8403        |\n",
            "|         102         |       0.43176       |       0.85243       |       0.46657       |       0.8404        |\n",
            "|         103         |       0.43048       |       0.85257       |       0.46547       |       0.8407        |\n",
            "|         104         |       0.42921       |       0.85288       |       0.46438       |       0.8408        |\n",
            "|         105         |       0.42796       |       0.85328       |       0.4633        |        0.841        |\n",
            "|         106         |       0.42672       |       0.85368       |       0.46224       |       0.8415        |\n",
            "|         107         |       0.42549       |       0.85413       |       0.4612        |       0.8418        |\n",
            "|         108         |       0.42428       |       0.85473       |       0.46015       |       0.8423        |\n",
            "|         109         |       0.42308       |        0.855        |       0.45912       |       0.8425        |\n",
            "|         110         |       0.4219        |       0.8552        |       0.45811       |       0.8426        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         111         |       0.42072       |       0.85557       |       0.45712       |       0.8427        |\n",
            "|         112         |       0.41956       |       0.85592       |       0.45614       |       0.8427        |\n",
            "|         113         |       0.41841       |       0.8564        |       0.45517       |       0.8427        |\n",
            "|         114         |       0.41727       |       0.85685       |       0.45421       |       0.8427        |\n",
            "|         115         |       0.41615       |       0.85715       |       0.45325       |       0.8429        |\n",
            "|         116         |       0.41503       |       0.85752       |       0.4523        |       0.8433        |\n",
            "|         117         |       0.41393       |       0.85802       |       0.45137       |       0.8437        |\n",
            "|         118         |       0.41283       |       0.85837       |       0.45045       |       0.8442        |\n",
            "|         119         |       0.41174       |       0.8586        |       0.44955       |       0.8442        |\n",
            "|         120         |       0.41066       |       0.85887       |       0.44865       |       0.8442        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         121         |       0.40959       |       0.85945       |       0.44777       |       0.8446        |\n",
            "|         122         |       0.40852       |       0.85978       |       0.44688       |       0.8452        |\n",
            "|         123         |       0.40746       |       0.85995       |        0.446        |       0.8452        |\n",
            "|         124         |       0.40641       |       0.86055       |       0.44513       |        0.845        |\n",
            "|         125         |       0.40537       |       0.86107       |       0.44427       |       0.8453        |\n",
            "|         126         |       0.40433       |       0.86138       |       0.44343       |       0.8453        |\n",
            "|         127         |       0.40331       |       0.86172       |       0.4426        |       0.8449        |\n",
            "|         128         |       0.4023        |       0.8622        |       0.44178       |       0.8451        |\n",
            "|         129         |       0.4013        |       0.86265       |       0.44097       |       0.8456        |\n",
            "|         130         |       0.4003        |       0.86303       |       0.44015       |       0.8465        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         131         |       0.39929       |       0.86328       |       0.4393        |       0.8471        |\n",
            "|         132         |       0.39825       |       0.86377       |       0.43839       |       0.8476        |\n",
            "|         133         |       0.39718       |       0.86418       |       0.43742       |       0.8472        |\n",
            "|         134         |       0.39621       |       0.86438       |       0.43655       |       0.8477        |\n",
            "|         135         |       0.39525       |       0.86477       |       0.43569       |       0.8483        |\n",
            "|         136         |       0.39424       |       0.86525       |       0.43486       |       0.8481        |\n",
            "|         137         |       0.39321       |       0.8655        |       0.43404       |       0.8485        |\n",
            "|         138         |       0.3922        |       0.86577       |       0.43325       |        0.848        |\n",
            "|         139         |       0.39122       |       0.86605       |       0.4325        |       0.8483        |\n",
            "|         140         |       0.39025       |       0.8664        |       0.43175       |       0.8484        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         141         |       0.38929       |       0.86657       |       0.43103       |        0.849        |\n",
            "|         142         |       0.38836       |       0.86673       |       0.43033       |        0.849        |\n",
            "|         143         |       0.38743       |       0.86698       |       0.42958       |       0.8495        |\n",
            "|         144         |       0.38649       |       0.86725       |       0.42881       |       0.8496        |\n",
            "|         145         |       0.38553       |       0.86767       |       0.42804       |       0.8499        |\n",
            "|         146         |       0.3846        |       0.86798       |       0.42729       |       0.8502        |\n",
            "|         147         |       0.38368       |       0.86842       |       0.42656       |       0.8508        |\n",
            "|         148         |       0.38276       |       0.8686        |       0.42581       |       0.8511        |\n",
            "|         149         |       0.38184       |       0.86898       |       0.42508       |       0.8514        |\n",
            "|         150         |       0.38094       |       0.86933       |       0.42435       |       0.8521        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         151         |       0.38004       |       0.8697        |       0.42366       |       0.8524        |\n",
            "|         152         |       0.37914       |       0.87008       |       0.42297       |       0.8527        |\n",
            "|         153         |       0.37824       |       0.87008       |       0.42226       |       0.8529        |\n",
            "|         154         |       0.37736       |       0.87025       |       0.42157       |        0.853        |\n",
            "|         155         |       0.37647       |       0.87053       |       0.42085       |        0.854        |\n",
            "|         156         |       0.37558       |       0.87092       |       0.42015       |       0.8539        |\n",
            "|         157         |       0.37471       |       0.87133       |       0.41945       |        0.854        |\n",
            "|         158         |       0.37384       |       0.87158       |       0.41875       |       0.8541        |\n",
            "|         159         |       0.37298       |       0.87187       |       0.41806       |       0.8545        |\n",
            "|         160         |       0.37211       |       0.87207       |       0.41737       |       0.8547        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         161         |       0.37125       |       0.8723        |       0.41669       |       0.8543        |\n",
            "|         162         |       0.3704        |       0.87275       |       0.41602       |       0.8546        |\n",
            "|         163         |       0.36955       |       0.87302       |       0.41536       |       0.8548        |\n",
            "|         164         |       0.3687        |       0.87333       |       0.4147        |       0.8551        |\n",
            "|         165         |       0.36787       |       0.87357       |       0.41405       |       0.8555        |\n",
            "|         166         |       0.36703       |       0.87375       |       0.41341       |       0.8559        |\n",
            "|         167         |       0.3662        |       0.87395       |       0.41276       |       0.8558        |\n",
            "|         168         |       0.36537       |       0.87422       |       0.41212       |       0.8558        |\n",
            "|         169         |       0.36455       |       0.8746        |       0.41147       |       0.8561        |\n",
            "|         170         |       0.36373       |       0.8749        |       0.41085       |       0.8567        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         171         |       0.36291       |       0.87512       |       0.41019       |       0.8568        |\n",
            "|         172         |       0.3621        |       0.87552       |       0.4096        |        0.857        |\n",
            "|         173         |       0.3613        |       0.87565       |       0.40893       |       0.8574        |\n",
            "|         174         |       0.36051       |       0.8762        |       0.4084        |       0.8574        |\n",
            "|         175         |       0.35973       |       0.87618       |       0.40773       |       0.8574        |\n",
            "|         176         |       0.35897       |       0.87697       |       0.40728       |       0.8574        |\n",
            "|         177         |       0.35824       |       0.87662       |       0.4066        |       0.8579        |\n",
            "|         178         |       0.35751       |       0.8772        |       0.40626       |       0.8581        |\n",
            "|         179         |       0.35676       |       0.87688       |       0.40549       |       0.8583        |\n",
            "|         180         |       0.35593       |       0.87767       |       0.40504       |       0.8584        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         181         |       0.35508       |       0.87755       |       0.40424       |       0.8586        |\n",
            "|         182         |       0.35428       |       0.87807       |       0.40367       |       0.8588        |\n",
            "|         183         |       0.35356       |       0.87838       |       0.40316       |       0.8593        |\n",
            "|         184         |       0.35287       |       0.87798       |       0.40256       |        0.86         |\n",
            "|         185         |       0.35216       |       0.87867       |       0.40216       |       0.8601        |\n",
            "|         186         |       0.35139       |       0.87857       |       0.40145       |       0.8602        |\n",
            "|         187         |       0.35059       |       0.87915       |       0.40097       |       0.8599        |\n",
            "|         188         |       0.34982       |       0.87942       |       0.40039       |       0.8599        |\n",
            "|         189         |       0.34908       |       0.8796        |       0.39991       |       0.8606        |\n",
            "|         190         |       0.34839       |       0.87982       |       0.39954       |       0.8606        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         191         |       0.34769       |       0.87972       |       0.39902       |       0.8605        |\n",
            "|         192         |       0.34698       |       0.8803        |       0.39862       |       0.8604        |\n",
            "|         193         |       0.34624       |       0.88017       |       0.39796       |       0.8612        |\n",
            "|         194         |       0.3455        |       0.88052       |       0.39742       |       0.8611        |\n",
            "|         195         |       0.34479       |       0.8808        |       0.39682       |       0.8618        |\n",
            "|         196         |       0.34411       |       0.88097       |       0.39625       |       0.8623        |\n",
            "|         197         |       0.34342       |       0.88125       |       0.39575       |       0.8624        |\n",
            "|         198         |       0.34273       |       0.88107       |       0.39518       |       0.8631        |\n",
            "|         199         |       0.34203       |       0.88147       |       0.39471       |       0.8631        |\n",
            "|         200         |       0.34133       |       0.88132       |       0.39417       |       0.8637        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         201         |       0.34064       |       0.88172       |       0.39371       |       0.8636        |\n",
            "|         202         |       0.33997       |       0.88197       |       0.39326       |       0.8635        |\n",
            "|         203         |       0.3393        |       0.88193       |       0.39277       |       0.8637        |\n",
            "|         204         |       0.33864       |       0.88253       |       0.39233       |       0.8627        |\n",
            "|         205         |       0.33798       |       0.88222       |       0.39179       |       0.8639        |\n",
            "|         206         |       0.33732       |       0.8829        |       0.39131       |       0.8633        |\n",
            "|         207         |       0.33666       |       0.88278       |       0.39079       |       0.8641        |\n",
            "|         208         |       0.33602       |       0.88317       |       0.39031       |       0.8635        |\n",
            "|         209         |       0.3354        |       0.88327       |       0.38989       |       0.8643        |\n",
            "|         210         |       0.33483       |       0.88347       |       0.38941       |       0.8634        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         211         |       0.33429       |       0.8833        |       0.38914       |       0.8639        |\n",
            "|         212         |       0.33379       |       0.88398       |       0.38868       |       0.8639        |\n",
            "|         213         |       0.33326       |       0.88375       |       0.3885        |       0.8636        |\n",
            "|         214         |       0.33261       |       0.88438       |       0.38785       |       0.8641        |\n",
            "|         215         |       0.3318        |       0.88422       |       0.38734       |       0.8639        |\n",
            "|         216         |       0.33101       |       0.88485       |       0.38666       |       0.8648        |\n",
            "|         217         |       0.33039       |       0.88493       |       0.38618       |       0.8652        |\n",
            "|         218         |       0.3299        |       0.88518       |       0.38597       |       0.8648        |\n",
            "|         219         |       0.32941       |       0.88532       |       0.38549       |       0.8655        |\n",
            "|         220         |       0.32879       |       0.88522       |       0.3852        |       0.8651        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         221         |       0.3281        |       0.88593       |       0.38458       |       0.8656        |\n",
            "|         222         |       0.32745       |       0.88613       |       0.38412       |       0.8666        |\n",
            "|         223         |       0.3269        |       0.88652       |       0.38383       |       0.8665        |\n",
            "|         224         |       0.3264        |       0.88663       |       0.38336       |        0.866        |\n",
            "|         225         |       0.32585       |       0.88638       |       0.38316       |       0.8662        |\n",
            "|         226         |       0.32522       |       0.88707       |       0.38259       |       0.8662        |\n",
            "|         227         |       0.3246        |       0.88697       |       0.38221       |       0.8667        |\n",
            "|         228         |       0.32404       |       0.88727       |       0.38185       |        0.867        |\n",
            "|         229         |       0.32353       |       0.88757       |       0.38143       |       0.8673        |\n",
            "|         230         |       0.32301       |       0.8876        |       0.38123       |       0.8668        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         231         |       0.32245       |       0.88805       |       0.38071       |        0.867        |\n",
            "|         232         |       0.32186       |       0.88797       |       0.38041       |       0.8669        |\n",
            "|         233         |       0.32128       |       0.88828       |       0.37995       |       0.8672        |\n",
            "|         234         |       0.32074       |       0.8885        |       0.37959       |       0.8672        |\n",
            "|         235         |       0.32023       |       0.88863       |       0.37933       |       0.8674        |\n",
            "|         236         |       0.31972       |       0.8887        |       0.37889       |       0.8669        |\n",
            "|         237         |       0.31918       |       0.8889        |       0.37865       |       0.8676        |\n",
            "|         238         |       0.31863       |       0.88897       |       0.37817       |       0.8673        |\n",
            "|         239         |       0.31808       |       0.88922       |       0.37786       |       0.8676        |\n",
            "|         240         |       0.31755       |       0.8895        |       0.37747       |       0.8673        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         241         |       0.31704       |       0.8897        |       0.37712       |       0.8676        |\n",
            "|         242         |       0.31653       |       0.88992       |       0.37683       |       0.8678        |\n",
            "|         243         |       0.31603       |       0.88983       |       0.37643       |       0.8681        |\n",
            "|         244         |       0.31551       |       0.89007       |       0.37617       |       0.8681        |\n",
            "|         245         |        0.315        |       0.89003       |       0.37576       |       0.8684        |\n",
            "|         246         |       0.31448       |       0.89028       |       0.37547       |       0.8685        |\n",
            "|         247         |       0.31398       |       0.89037       |       0.37512       |       0.8688        |\n",
            "|         248         |       0.31349       |       0.89032       |       0.37481       |       0.8691        |\n",
            "|         249         |       0.31304       |       0.89058       |       0.37456       |       0.8695        |\n",
            "|         250         |       0.31262       |       0.89053       |       0.37426       |       0.8692        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         251         |       0.31225       |       0.89093       |       0.37417       |       0.8695        |\n",
            "|         252         |       0.3119        |       0.89035       |       0.37388       |       0.8696        |\n",
            "|         253         |       0.31155       |       0.89102       |       0.37381       |        0.869        |\n",
            "|         254         |       0.31099       |       0.8907        |       0.37332       |       0.8698        |\n",
            "|         255         |       0.31035       |       0.89162       |       0.37287       |       0.8697        |\n",
            "|         256         |       0.30968       |       0.89135       |       0.37238       |       0.8704        |\n",
            "|         257         |       0.30917       |       0.89175       |       0.37194       |       0.8693        |\n",
            "|         258         |       0.30879       |       0.89158       |       0.37189       |       0.8694        |\n",
            "|         259         |       0.30839       |       0.89158       |       0.3715        |       0.8707        |\n",
            "|         260         |       0.30789       |       0.89205       |       0.37131       |        0.87         |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         261         |       0.30733       |       0.89195       |       0.37086       |       0.8707        |\n",
            "|         262         |       0.30685       |       0.89262       |       0.37049       |       0.8705        |\n",
            "|         263         |       0.30647       |       0.89282       |       0.37041       |       0.8707        |\n",
            "|         264         |       0.30609       |       0.89272       |        0.37         |       0.8705        |\n",
            "|         265         |       0.30561       |       0.89285       |       0.36989       |       0.8706        |\n",
            "|         266         |       0.30507       |       0.89307       |       0.36935       |       0.8712        |\n",
            "|         267         |       0.30455       |       0.89317       |       0.36907       |       0.8714        |\n",
            "|         268         |       0.30412       |       0.89325       |       0.36882       |       0.8714        |\n",
            "|         269         |       0.30375       |       0.89355       |       0.36853       |       0.8717        |\n",
            "|         270         |       0.30335       |       0.89352       |       0.36845       |       0.8716        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         271         |       0.3029        |       0.89365       |       0.36801       |       0.8715        |\n",
            "|         272         |       0.30242       |       0.89378       |       0.36783       |       0.8718        |\n",
            "|         273         |       0.30196       |       0.89385       |       0.36747       |       0.8714        |\n",
            "|         274         |       0.30154       |       0.89408       |       0.36725       |       0.8716        |\n",
            "|         275         |       0.30115       |       0.89433       |       0.36706       |        0.872        |\n",
            "|         276         |       0.30073       |       0.89425       |       0.36676       |       0.8717        |\n",
            "|         277         |       0.30028       |       0.89492       |       0.36654       |       0.8723        |\n",
            "|         278         |       0.29982       |       0.89462       |       0.36623       |       0.8718        |\n",
            "|         279         |       0.29939       |       0.89483       |       0.36595       |        0.872        |\n",
            "|         280         |       0.29898       |       0.89482       |       0.36577       |       0.8722        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         281         |       0.29858       |       0.89482       |       0.36546       |       0.8717        |\n",
            "|         282         |       0.29817       |       0.8951        |       0.36533       |       0.8721        |\n",
            "|         283         |       0.29776       |       0.89522       |       0.36498       |        0.872        |\n",
            "|         284         |       0.29733       |       0.89543       |       0.36482       |       0.8729        |\n",
            "|         285         |       0.29693       |       0.89603       |       0.3645        |       0.8723        |\n",
            "|         286         |       0.29654       |       0.89555       |       0.36434       |       0.8728        |\n",
            "|         287         |       0.29619       |       0.89627       |       0.3641        |       0.8729        |\n",
            "|         288         |       0.29588       |       0.89585       |       0.36401       |       0.8732        |\n",
            "|         289         |       0.29564       |       0.89673       |       0.36385       |       0.8728        |\n",
            "|         290         |       0.29545       |       0.89573       |       0.36397       |       0.8733        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         291         |       0.29534       |       0.89665       |       0.36385       |       0.8713        |\n",
            "|         292         |       0.29513       |       0.89582       |       0.36406       |       0.8735        |\n",
            "|         293         |       0.29466       |       0.89683       |       0.36346       |        0.872        |\n",
            "|         294         |       0.29386       |       0.89637       |       0.3631        |       0.8736        |\n",
            "|         295         |       0.29305       |       0.89693       |       0.36223       |       0.8733        |\n",
            "|         296         |       0.29259       |       0.89742       |       0.36201       |       0.8736        |\n",
            "|         297         |       0.29246       |       0.89677       |       0.36215       |       0.8736        |\n",
            "|         298         |       0.29235       |       0.89772       |       0.36203       |       0.8725        |\n",
            "|         299         |       0.29195       |       0.89713       |       0.36203       |       0.8738        |\n",
            "|         300         |       0.2913        |       0.89782       |       0.36132       |       0.8736        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         301         |       0.29072       |       0.8978        |       0.36098       |       0.8743        |\n",
            "|         302         |       0.29038       |       0.89782       |       0.36081       |       0.8741        |\n",
            "|         303         |       0.29021       |       0.89817       |       0.36068       |       0.8738        |\n",
            "|         304         |       0.28994       |       0.8977        |       0.36076       |       0.8743        |\n",
            "|         305         |       0.28949       |       0.89848       |       0.3603        |       0.8736        |\n",
            "|         306         |       0.28895       |       0.89837       |       0.36002       |       0.8743        |\n",
            "|         307         |       0.28853       |       0.89903       |       0.35974       |       0.8741        |\n",
            "|         308         |       0.28824       |       0.89893       |       0.35954       |       0.8741        |\n",
            "|         309         |       0.28798       |       0.89853       |       0.35959       |       0.8738        |\n",
            "|         310         |       0.28764       |       0.89915       |       0.35926       |       0.8737        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         311         |       0.28721       |       0.89888       |       0.35911       |       0.8741        |\n",
            "|         312         |       0.28678       |       0.89973       |       0.3588        |       0.8747        |\n",
            "|         313         |       0.28641       |       0.89957       |       0.35858       |       0.8744        |\n",
            "|         314         |       0.28611       |       0.89967       |       0.35854       |        0.875        |\n",
            "|         315         |       0.2858        |       0.89982       |       0.35827       |       0.8743        |\n",
            "|         316         |       0.28544       |       0.89973       |       0.3582        |       0.8742        |\n",
            "|         317         |       0.28505       |       0.90018       |       0.35789       |       0.8748        |\n",
            "|         318         |       0.28468       |       0.9003        |       0.35771       |       0.8746        |\n",
            "|         319         |       0.28434       |       0.90048       |       0.35759       |        0.875        |\n",
            "|         320         |       0.28401       |       0.90048       |       0.35735       |        0.874        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         321         |       0.28369       |       0.90067       |       0.35732       |       0.8744        |\n",
            "|         322         |       0.28334       |       0.90067       |       0.35702       |        0.874        |\n",
            "|         323         |       0.28298       |       0.9009        |       0.35692       |       0.8749        |\n",
            "|         324         |       0.28262       |       0.90105       |       0.35668       |       0.8754        |\n",
            "|         325         |       0.28228       |       0.9011        |       0.35651       |        0.875        |\n",
            "|         326         |       0.28195       |       0.90145       |       0.35642       |       0.8748        |\n",
            "|         327         |       0.28163       |       0.90133       |       0.35619       |       0.8744        |\n",
            "|         328         |       0.28131       |       0.90175       |       0.35615       |       0.8753        |\n",
            "|         329         |       0.28097       |       0.90153       |       0.35587       |       0.8737        |\n",
            "|         330         |       0.28063       |        0.902        |       0.35581       |       0.8752        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         331         |       0.28028       |       0.90188       |       0.35554       |       0.8742        |\n",
            "|         332         |       0.27994       |       0.90218       |       0.35545       |       0.8754        |\n",
            "|         333         |       0.2796        |       0.90203       |       0.35523       |       0.8742        |\n",
            "|         334         |       0.27927       |       0.9024        |       0.3551        |       0.8753        |\n",
            "|         335         |       0.27896       |       0.9024        |       0.35495       |       0.8738        |\n",
            "|         336         |       0.27864       |       0.90262       |       0.3548        |       0.8754        |\n",
            "|         337         |       0.27833       |       0.90258       |       0.35469       |       0.8738        |\n",
            "|         338         |       0.27804       |       0.90297       |       0.35454       |       0.8751        |\n",
            "|         339         |       0.27775       |       0.9026        |       0.35444       |       0.8736        |\n",
            "|         340         |       0.27749       |       0.9031        |       0.35435       |       0.8749        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         341         |       0.27725       |       0.90245       |       0.35425       |        0.874        |\n",
            "|         342         |       0.27705       |       0.90333       |       0.35428       |       0.8747        |\n",
            "|         343         |       0.27685       |       0.90263       |       0.35416       |       0.8742        |\n",
            "|         344         |       0.27669       |       0.90363       |       0.35432       |       0.8749        |\n",
            "|         345         |       0.27637       |       0.90263       |        0.354        |       0.8742        |\n",
            "|         346         |       0.27601       |       0.90387       |       0.35399       |       0.8748        |\n",
            "|         347         |       0.27544       |       0.90292       |       0.35339       |       0.8743        |\n",
            "|         348         |       0.27489       |       0.90412       |       0.35316       |       0.8758        |\n",
            "|         349         |       0.27445       |       0.90403       |       0.35277       |       0.8746        |\n",
            "|         350         |       0.27417       |       0.90442       |       0.35272       |       0.8746        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         351         |        0.274        |       0.90458       |       0.35274       |       0.8753        |\n",
            "|         352         |       0.27381       |       0.90408       |       0.35269       |       0.8742        |\n",
            "|         353         |       0.27356       |       0.90482       |       0.35268       |       0.8753        |\n",
            "|         354         |       0.27317       |       0.90432       |       0.35239       |        0.874        |\n",
            "|         355         |       0.27274       |       0.9052        |       0.35216       |       0.8759        |\n",
            "|         356         |       0.27232       |       0.90503       |       0.35191       |       0.8748        |\n",
            "|         357         |       0.27198       |       0.90505       |       0.35172       |       0.8755        |\n",
            "|         358         |       0.27173       |       0.90543       |       0.35172       |       0.8764        |\n",
            "|         359         |       0.2715        |       0.90502       |       0.35157       |       0.8751        |\n",
            "|         360         |       0.27126       |       0.90555       |       0.35162       |       0.8764        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         361         |       0.27095       |       0.9052        |       0.35136       |        0.875        |\n",
            "|         362         |       0.27061       |       0.90585       |       0.3513        |       0.8764        |\n",
            "|         363         |       0.27024       |       0.9056        |        0.351        |       0.8756        |\n",
            "|         364         |       0.26988       |        0.906        |       0.35089       |       0.8761        |\n",
            "|         365         |       0.26957       |       0.90605       |       0.3507        |       0.8758        |\n",
            "|         366         |       0.26929       |       0.90592       |       0.35061       |       0.8755        |\n",
            "|         367         |       0.26904       |       0.90623       |       0.35053       |       0.8769        |\n",
            "|         368         |       0.26878       |       0.90603       |       0.35042       |       0.8756        |\n",
            "|         369         |       0.26851       |       0.9066        |       0.35035       |       0.8765        |\n",
            "|         370         |       0.26821       |       0.90623       |       0.35021       |       0.8759        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         371         |       0.26791       |       0.90685       |       0.35009       |       0.8766        |\n",
            "|         372         |       0.26761       |       0.90667       |        0.35         |       0.8761        |\n",
            "|         373         |       0.26733       |       0.90693       |       0.34981       |       0.8762        |\n",
            "|         374         |       0.2671        |       0.90703       |       0.3499        |       0.8759        |\n",
            "|         375         |       0.26695       |       0.90698       |       0.3497        |       0.8761        |\n",
            "|         376         |       0.26689       |       0.9065        |       0.35014       |       0.8754        |\n",
            "|         377         |       0.26698       |       0.9065        |       0.34998       |       0.8754        |\n",
            "|         378         |       0.26704       |       0.90642       |       0.3507        |       0.8752        |\n",
            "|         379         |       0.26697       |       0.9066        |       0.35025       |       0.8746        |\n",
            "|         380         |       0.26634       |       0.90663       |       0.3503        |       0.8753        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         381         |       0.26546       |       0.90777       |       0.34917       |       0.8758        |\n",
            "|         382         |       0.26475       |       0.90762       |       0.34879       |       0.8762        |\n",
            "|         383         |       0.26455       |       0.9081        |       0.34886       |       0.8766        |\n",
            "|         384         |       0.26465       |       0.90735       |       0.34887       |        0.876        |\n",
            "|         385         |       0.26453       |       0.90752       |       0.34936       |       0.8757        |\n",
            "|         386         |       0.26408       |       0.90777       |       0.34867       |       0.8761        |\n",
            "|         387         |       0.26342       |       0.9083        |       0.34843       |       0.8761        |\n",
            "|         388         |       0.26301       |       0.90855       |       0.34809       |       0.8762        |\n",
            "|         389         |       0.26292       |       0.90805       |       0.34807       |       0.8762        |\n",
            "|         390         |       0.26287       |       0.90835       |       0.34846       |       0.8761        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         391         |       0.2626        |       0.90788       |       0.34805       |       0.8764        |\n",
            "|         392         |       0.26209       |       0.90897       |       0.34796       |       0.8762        |\n",
            "|         393         |       0.26165       |       0.90893       |       0.34755       |       0.8767        |\n",
            "|         394         |       0.26142       |       0.90908       |       0.34745       |       0.8769        |\n",
            "|         395         |       0.26129       |       0.90905       |       0.34766       |       0.8762        |\n",
            "|         396         |       0.26108       |       0.9091        |       0.34736       |       0.8767        |\n",
            "|         397         |       0.26069       |       0.90928       |       0.34737       |       0.8767        |\n",
            "|         398         |       0.26029       |       0.9094        |       0.34698       |       0.8769        |\n",
            "|         399         |       0.25999       |       0.90965       |       0.34687       |       0.8767        |\n",
            "|         400         |       0.25981       |       0.9097        |       0.34695       |       0.8768        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         401         |       0.25961       |       0.90967       |       0.34674       |       0.8776        |\n",
            "|         402         |       0.25932       |       0.9099        |       0.3468        |       0.8775        |\n",
            "|         403         |       0.25898       |       0.90997       |       0.34647       |       0.8775        |\n",
            "|         404         |       0.25867       |       0.91007       |       0.3464        |       0.8778        |\n",
            "|         405         |       0.25842       |       0.91018       |       0.34635       |       0.8775        |\n",
            "|         406         |       0.25821       |       0.9101        |       0.34622       |        0.878        |\n",
            "|         407         |       0.25799       |       0.91043       |       0.34632       |       0.8775        |\n",
            "|         408         |       0.25773       |       0.91012       |       0.34608       |       0.8779        |\n",
            "|         409         |       0.25746       |       0.91055       |       0.3461        |       0.8774        |\n",
            "|         410         |       0.25722       |       0.91077       |       0.34595       |       0.8779        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         411         |       0.25706       |       0.91063       |       0.34596       |       0.8774        |\n",
            "|         412         |       0.25698       |       0.91078       |       0.34612       |       0.8776        |\n",
            "|         413         |        0.257        |       0.91057       |       0.34619       |       0.8771        |\n",
            "|         414         |       0.25703       |       0.9102        |       0.34652       |       0.8766        |\n",
            "|         415         |       0.25715       |       0.91043       |       0.34668       |       0.8769        |\n",
            "|         416         |       0.25696       |       0.90973       |       0.34672       |       0.8762        |\n",
            "|         417         |       0.25666       |       0.91075       |       0.34656       |       0.8774        |\n",
            "|         418         |       0.25582       |       0.91067       |       0.34582       |       0.8771        |\n",
            "|         419         |       0.25506       |       0.91147       |       0.3453        |       0.8779        |\n",
            "|         420         |       0.25457       |       0.9113        |       0.34486       |       0.8779        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         421         |       0.25447       |       0.9116        |       0.34502       |       0.8777        |\n",
            "|         422         |       0.25457       |       0.91173       |       0.34526       |       0.8778        |\n",
            "|         423         |       0.2545        |       0.91128       |       0.34537       |       0.8775        |\n",
            "|         424         |       0.25421       |       0.91195       |       0.34528       |        0.878        |\n",
            "|         425         |       0.25361       |       0.91183       |       0.34478       |       0.8781        |\n",
            "|         426         |       0.2531        |       0.9121        |       0.34447       |       0.8783        |\n",
            "|         427         |       0.25282       |       0.91235       |       0.34433       |       0.8786        |\n",
            "|         428         |       0.25275       |       0.91217       |       0.34443       |       0.8783        |\n",
            "|         429         |       0.25269       |       0.91243       |       0.34456       |       0.8786        |\n",
            "|         430         |       0.25245       |       0.91222       |       0.34444       |       0.8785        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         431         |       0.25208       |       0.91272       |       0.34425       |       0.8784        |\n",
            "|         432         |       0.25165       |       0.91257       |       0.34396       |       0.8789        |\n",
            "|         433         |       0.25134       |       0.91267       |       0.34381       |       0.8781        |\n",
            "|         434         |       0.25115       |        0.913        |       0.34381       |       0.8786        |\n",
            "|         435         |       0.25101       |       0.91283       |       0.34383       |       0.8788        |\n",
            "|         436         |       0.25084       |       0.9132        |       0.34385       |       0.8786        |\n",
            "|         437         |       0.25055       |       0.9128        |       0.34371       |       0.8787        |\n",
            "|         438         |       0.25023       |       0.91345       |       0.34355       |       0.8784        |\n",
            "|         439         |       0.24991       |       0.91325       |       0.34342       |       0.8784        |\n",
            "|         440         |       0.24965       |       0.9132        |       0.34329       |       0.8784        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         441         |       0.24944       |       0.91352       |       0.34328       |       0.8782        |\n",
            "|         442         |       0.24925       |       0.91348       |       0.34321       |       0.8785        |\n",
            "|         443         |       0.24905       |       0.91373       |       0.34323       |       0.8783        |\n",
            "|         444         |       0.2488        |       0.91372       |       0.34312       |       0.8788        |\n",
            "|         445         |       0.24854       |       0.91402       |       0.34301       |       0.8786        |\n",
            "|         446         |       0.24826       |       0.91387       |       0.34294       |       0.8786        |\n",
            "|         447         |        0.248        |       0.91392       |       0.34281       |       0.8784        |\n",
            "|         448         |       0.24776       |        0.914        |       0.3428        |       0.8787        |\n",
            "|         449         |       0.24753       |       0.91388       |       0.34265       |       0.8788        |\n",
            "|         450         |       0.24731       |       0.91418       |       0.34267       |        0.879        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         451         |       0.24709       |       0.91423       |       0.34253       |       0.8791        |\n",
            "|         452         |       0.24685       |       0.9144        |       0.34253       |       0.8795        |\n",
            "|         453         |       0.24661       |       0.91442       |       0.34239       |       0.8789        |\n",
            "|         454         |       0.24636       |       0.91448       |       0.34236       |       0.8796        |\n",
            "|         455         |       0.24611       |       0.91472       |       0.34226       |       0.8791        |\n",
            "|         456         |       0.24586       |       0.91473       |       0.34217       |       0.8795        |\n",
            "|         457         |       0.24562       |       0.91478       |       0.34213       |       0.8791        |\n",
            "|         458         |       0.24539       |       0.91505       |       0.34202       |       0.8791        |\n",
            "|         459         |       0.24517       |       0.91493       |       0.34204       |        0.879        |\n",
            "|         460         |       0.24495       |       0.91512       |       0.34188       |       0.8791        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         461         |       0.24473       |       0.91507       |       0.34195       |       0.8787        |\n",
            "|         462         |       0.24452       |       0.91523       |       0.34176       |        0.879        |\n",
            "|         463         |       0.24431       |       0.91527       |       0.34188       |        0.879        |\n",
            "|         464         |       0.24411       |       0.91528       |       0.34165       |       0.8786        |\n",
            "|         465         |       0.24391       |       0.91543       |       0.34184       |       0.8791        |\n",
            "|         466         |       0.24374       |       0.91538       |       0.3416        |       0.8784        |\n",
            "|         467         |       0.24356       |       0.91552       |       0.34187       |       0.8795        |\n",
            "|         468         |       0.2434        |       0.91545       |       0.34159       |        0.878        |\n",
            "|         469         |       0.24324       |       0.91575       |       0.34192       |       0.8792        |\n",
            "|         470         |       0.24309       |       0.91568       |       0.34161       |       0.8781        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         471         |       0.24294       |       0.91597       |       0.34194       |       0.8792        |\n",
            "|         472         |       0.24277       |       0.91597       |       0.34162       |       0.8778        |\n",
            "|         473         |       0.24258       |       0.91608       |       0.34191       |       0.8787        |\n",
            "|         474         |       0.24236       |       0.91645       |       0.34159       |        0.878        |\n",
            "|         475         |       0.24209       |       0.91642       |       0.34168       |       0.8779        |\n",
            "|         476         |       0.24185       |       0.91667       |       0.34149       |       0.8788        |\n",
            "|         477         |       0.24156       |       0.91647       |       0.34138       |       0.8776        |\n",
            "|         478         |       0.24135       |       0.91687       |       0.34142       |       0.8786        |\n",
            "|         479         |       0.24111       |       0.9166        |       0.34113       |       0.8768        |\n",
            "|         480         |       0.24087       |       0.9167        |       0.34134       |       0.8787        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         481         |       0.24061       |       0.91648       |       0.34092       |       0.8771        |\n",
            "|         482         |       0.24028       |       0.91705       |       0.34106       |       0.8791        |\n",
            "|         483         |       0.23994       |       0.91697       |       0.34055       |       0.8773        |\n",
            "|         484         |       0.23961       |       0.9173        |       0.34065       |       0.8795        |\n",
            "|         485         |       0.23934       |       0.91757       |       0.34041       |        0.879        |\n",
            "|         486         |       0.23914       |       0.91748       |       0.34046       |       0.8789        |\n",
            "|         487         |       0.23901       |       0.91742       |       0.34052       |       0.8788        |\n",
            "|         488         |       0.23889       |       0.9177        |       0.34043       |        0.878        |\n",
            "|         489         |       0.23875       |       0.91748       |       0.34064       |       0.8786        |\n",
            "|         490         |       0.23853       |       0.91772       |       0.34035       |       0.8774        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         491         |       0.23824       |       0.91757       |       0.34042       |       0.8788        |\n",
            "|         492         |       0.23789       |       0.91805       |       0.33999       |       0.8777        |\n",
            "|         493         |       0.23753       |       0.91797       |       0.33998       |       0.8791        |\n",
            "|         494         |       0.23721       |       0.91817       |       0.33971       |       0.8792        |\n",
            "|         495         |       0.23696       |       0.91833       |       0.33968       |       0.8791        |\n",
            "|         496         |       0.23677       |       0.91803       |       0.33965       |       0.8794        |\n",
            "|         497         |       0.23662       |       0.9184        |       0.3396        |       0.8785        |\n",
            "|         498         |       0.23649       |       0.91825       |       0.33973       |       0.8785        |\n",
            "|         499         |       0.23635       |       0.9184        |       0.3396        |       0.8779        |\n",
            "|         500         |       0.2362        |       0.9184        |       0.33976       |       0.8786        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwUZbro8d/TS7qzkIQlIJsCgiKQhCUsKirLuI1eGMQF3EBcrtwzIjNzxmV0nLnOeEfveGaUOefo6KgMDh88buCOIyDCubgBgrKpLFEWIWwJCVm7+71/VHWnE7LTSaea5+vn/dRe9VQbnnr7req3xBiDUkop53PFOwCllFKxoQldKaUShCZ0pZRKEJrQlVIqQWhCV0qpBOGJ14G7dOli+vTpE6/DK6WUI61bt+6QMSarrmWNJnQR8QOrAJ+9/qvGmN/UWscHLABGAIeB64wx+Q3tt0+fPqxdu7ZJJ6CUUsoiIt/Vt6wpTS4VwARjTC4wFLhMRMbUWudW4Kgxpj/wZ+CxlgarlFKqZRpN6MZSYk967VL710iTgb/b468CE0VEYhalUkqpRjXppqiIuEVkA1AAfGCM+bTWKj2B3QDGmABQBHSuYz93iMhaEVl78ODBk4tcKaVUDU26KWqMCQJDRSQTWCwiQ4wxm5p7MGPMM8AzAHl5edrngFK2qqoq9uzZQ3l5ebxDUe2E3++nV69eeL3eJm/TrKdcjDGFIvIhcBkQndD3Ar2BPSLiATKwbo4qpZpgz549dOjQgT59+qCtlcoYw+HDh9mzZw99+/Zt8naNNrmISJZdM0dEkoGLgW21VnsTmGGPXw2sMNrrl1JNVl5eTufOnTWZKwBEhM6dOzf7G1tTaujdgb+LiBvrAvCyMeZtEXkYWGuMeRN4DnhRRLYDR4BpzQtfKaXJXEVryd9DowndGPMlMKyO+Q9FjZcD1zT76C2x9zPYuADG3g/pPdvkkEop5QTO++n/G4vgiv+ALeviHYlSCeHw4cMMHTqUoUOHctppp9GzZ8/IdGVlZYPbrl27ljlz5jT7mBs2bEBEWLp0aUvDVnWI20//W8yTZA0DFfGNQ6kE0blzZzZs2ADAb3/7W9LS0vjXf/3XyPJAIIDHU3eqyMvLIy8vr9nHXLRoEWPHjmXRokVcdtllLQu8CYLBIG63u9X23944r4butv+wqhquOSilWm7mzJnceeedjB49mnvuuYfPPvuMc889l2HDhnHeeefx9ddfA7By5UquvPJKwLoYzJo1i3HjxtGvXz/mzZtX576NMbzyyivMnz+fDz74oMaNv8cee4zs7Gxyc3O57777ANi+fTs/+tGPyM3NZfjw4ezYsaPGcQF++tOfMn/+fMDqVuTee+9l+PDhvPLKKzz77LOMHDmS3Nxcpk6dSmlpKQAHDhxgypQp5Obmkpuby5o1a3jooYd44oknIvt94IEHePLJJ2P3wbYy59XQveEauiZ0laDmAhtivM+hwBONrlXDnj17WLNmDW63m2PHjrF69Wo8Hg/Lli3jV7/6Fa+99toJ22zbto0PP/yQ4uJizj77bGbPnn3Cc9Rr1qyhb9++nHnmmYwbN4533nmHqVOn8t577/HGG2/w6aefkpKSwpEjRwC44YYbuO+++5gyZQrl5eWEQiF2797dYOydO3dm/fr1gNWkdPvttwPw4IMP8txzz3HXXXcxZ84cLrroIhYvXkwwGKSkpIQePXpw1VVXMXfuXEKhEC+99BKfffZZ8z64OHJgQrf/OLSGrlSruuaaayLNFUVFRcyYMYNvv/0WEaGqqqrOba644gp8Ph8+n4+uXbty4MABevXqVWOdRYsWMW2a9SDctGnTWLBgAVOnTmXZsmXccsstpKSkANCpUyeKi4vZu3cvU6ZMAawf2zTFddddFxnftGkTDz74IIWFhZSUlHDppZcCsGLFChYsWACA2+0mIyODjIwMOnfuzBdffMGBAwcYNmwYnTuf8KP3dst5CT3chq4JXSWqZtakW0tqampk/Ne//jXjx49n8eLF5OfnM27cuDq38fl8kXG3200gEKixPBgM8tprr/HGG2/wyCOPRH5AU1xc3KzYPB4PoVAoMl37ee3o2GfOnMmSJUvIzc1l/vz5rFy5ssF933bbbcyfP5/9+/cza9asZsUVb85rQ9cmF6XaXFFRET17Wo8Jh9uqW2L58uXk5OSwe/du8vPz+e6775g6dSqLFy/m4osv5oUXXoi0cR85coQOHTrQq1cvlixZAkBFRQWlpaWcccYZbNmyhYqKCgoLC1m+fHm9xywuLqZ79+5UVVWxcOHCyPyJEyfy1FNPAdaFpqioCIApU6awdOlSPv/880ht3imcl9C1hq5Um7vnnnu4//77GTZs2Am17uZYtGhRpPkkbOrUqZGnXSZNmkReXh5Dhw7l8ccfB+DFF19k3rx55OTkcN5557F//3569+7Ntddey5AhQ7j22msZNuyEn8pE/O53v2P06NGcf/75DBw4MDL/ySef5MMPPyQ7O5sRI0awZcsWAJKSkhg/fjzXXnut456QkXj9Qj8vL8+06AUXS5+Dy2+DBf8bbnqo8fWVcoCtW7dyzjnnxDsMBYRCocgTMgMGDIhrLHX9XYjIOmNMnc+KOq+G7rXb6IJ135RRSqmW2rJlC/3792fixIlxT+Yt4bybouGnXLQNXSkVY4MGDWLnzp3xDqPFnFdD99g19KqWt+MppVQicl5CT7ITutbQlVKqBucldHf4sUVtQ1dKqWjOS+jhGno9v1RTSqlTlfMSeviHRUFtQ1cqFtq6+9w+ffpw6NChkwlZ1cN5T7mEb4qexI8blFLV4tF9rmodDqyha5OLUq2tNbvPrUt+fj4TJkwgJyeHiRMn8v333wPwyiuvMGTIEHJzc7nwwgsB2Lx5M6NGjWLo0KHk5OTw7bffxvjsnct5NfQku7c1vSmqEtW6uXA0xv3ndhwKI5rX61drdZ9bl7vuuosZM2YwY8YMnn/+eebMmcOSJUt4+OGHef/99+nZsyeFhYUAPP3009x9993ccMMNVFZWEgwGm3Veicx5CT3yxiJtclGqNbVW97l1+fjjj3n99dcBuOmmm7jnnnsAOP/885k5cybXXnstV111FQDnnnsujzzyCHv27OGqq65y5C86W4vzEnrkp/+a0FWCamZNurW0Rve5zfX000/z6aef8s477zBixAjWrVvH9ddfz+jRo3nnnXf48Y9/zF//+lcmTJhwUsdJFM5rQ480uWhCV6qtxKr73Pqcd955vPTSSwAsXLiQCy64AIAdO3YwevRoHn74YbKysti9ezc7d+6kX79+zJkzh8mTJ/Pll1/GPB6ncl5C9+pTLkq1tVh1nxuWk5NDr1696NWrFz//+c/5y1/+wgsvvEBOTg4vvvhi5D2ev/zlL8nOzmbIkCGcd9555Obm8vLLLzNkyBCGDh3Kpk2buPnmm086nkThvO5zAVwCt4+Fv66ObVBKxYl2n6vqkvjd54IVdUDvbCulVDQHJ3RtclFKqWiNJnQR6S0iH4rIFhHZLCJ317HOOBEpEpENdmndVwm5AX32VCmlamjKY4sB4BfGmPUi0gFYJyIfGGO21FpvtTHmytiHWAe3aJOLUkrV0mgN3RjzgzFmvT1eDGwFerZ2YA1yo8+hK6VULc1qQxeRPsAw4NM6Fp8rIhtF5D0RGVzP9neIyFoRWXvw4MFmBxvh0hq6UkrV1uSELiJpwGvAXGPMsVqL1wNnGGNygb8AS+rahzHmGWNMnjEmLysrq6Uxa5OLUjE2fvx43n///RrznnjiCWbPnl3vNuPGjSP86PGPf/zjSF8r0X7729/y+OOPN3jsJUuWsGVLdQvuQw89xLJly5oTfoPmzp1Lz549CYVCMdtne9WkhC4iXqxkvtAY83rt5caYY8aYEnv8XcArIl1iGmk0fWxRqZiaPn165JeaYS+99BLTp09v0vbvvvsumZmZLTp27YT+8MMP86Mf/ahF+6otFAqxePFievfuzUcffRSTfdYlFj+2ioWmPOUiwHPAVmPMn+pZ5zR7PURklL3fw7EMtAa3C06Bq61SbeXqq6/mnXfeibzQIj8/n3379nHBBRcwe/Zs8vLyGDx4ML/5zW/q3D76pRWPPPIIZ511FmPHjo10swvw7LPPMnLkSHJzc5k6dSqlpaWsWbOGN998k1/+8pcMHTqUHTt2MHPmTF599VUAli9fzrBhw8jOzmbWrFlUVFREjveb3/yG4cOHk52dzbZt2+qMa+XKlQwePJjZs2ezaNGiyPwDBw4wZcoUcnNzyc3NZc2aNQAsWLCAnJwccnNzuemmmwBqxAOQlpYW2fcFF1zApEmTGDRoEAA/+clPGDFiBIMHD+aZZ56JbLN06VKGDx9Obm4uEydOJBQKMWDAAMJNz6FQiP79+3NSTdE07SmX84GbgK9EJNyn56+A0wGMMU8DVwOzRSQAlAHTTGv+BFWbXFQCm7t0Lhv2x7b73KGnDeWJy+rv9KtTp06MGjWK9957j8mTJ/PSSy9x7bXXIiI88sgjdOrUiWAwyMSJE/nyyy/Jycmpcz/r1q3jpZdeYsOGDQQCAYYPH86IESMAuOqqq7j99tsBePDBB3nuuee46667mDRpEldeeSVXX311jX2Vl5czc+ZMli9fzllnncXNN9/MU089xdy5cwHo0qUL69ev5z//8z95/PHH+dvf/nZCPIsWLWL69OlMnjyZX/3qV1RVVeH1epkzZw4XXXQRixcvJhgMUlJSwubNm/n973/PmjVr6NKlC0eOHGn0c12/fj2bNm2ib9++ADz//PN06tSJsrIyRo4cydSpUwmFQtx+++2sWrWKvn37cuTIEVwuFzfeeCMLFy5k7ty5LFu2jNzcXE6qKZqmPeXy38YYMcbkGGOG2uVdY8zTdjLHGPPvxpjBxphcY8wYY8yak4qqMW7R59CVirHoZpfo5paXX36Z4cOHM2zYMDZv3lyjeaS21atXM2XKFFJSUkhPT2fSpEmRZZs2beKCCy4gOzubhQsXsnnz5gbj+frrr+nbty9nnXUWADNmzGDVqlWR5eHudEeMGEF+fv4J21dWVvLuu+/yk5/8hPT0dEaPHh25T7BixYrI/QG3201GRgYrVqzgmmuuoUsXq7W4U6dODcYHMGrUqEgyB5g3bx65ubmMGTOG3bt38+233/LJJ59w4YUXRtYL73fWrFksWLAAsC4Et9xyS6PHa4zzus8F8AgEtMlFJaaGatKtafLkyfzsZz9j/fr1lJaWMmLECHbt2sXjjz/O559/TseOHZk5cybl5eUt2v/MmTNZsmQJubm5zJ8/n5UrV55UvOGueuvrpvf999+nsLCQ7OxsAEpLS0lOTo68YampPB5P5IZqKBSq8Z7V6C6GV65cybJly/j4449JSUlh3LhxDX5WvXv3plu3bqxYsYLPPvuMhQsXNiuuujj0p/8uraErFWNpaWmMHz+eWbNmRWrnx44dIzU1lYyMDA4cOMB7773X4D4uvPBClixZQllZGcXFxbz11luRZcXFxXTv3p2qqqoayatDhw4UFxefsK+zzz6b/Px8tm/fDsCLL77IRRdd1OTzWbRoEX/729/Iz88nPz+fXbt28cEHH1BaWsrEiRN56qmnAAgGgxQVFTFhwgReeeUVDh+2bv+Fm1z69OnDunXrAHjzzTfrfblHUVERHTt2JCUlhW3btvHJJ58AMGbMGFatWsWuXbtq7Bfgtttu48Ybb6zxMpGT4cyE7hYIag1dqVibPn06GzdujCT03Nxchg0bxsCBA7n++us5//zzG9x++PDhXHfddeTm5nL55ZczcuTIyLLf/e53jB49mvPPP5+BAwdG5k+bNo0//vGPDBs2jB07dkTm+/1+XnjhBa655hqys7NxuVzceeedTTqP0tJSli5dyhVXXBGZl5qaytixY3nrrbd48skn+fDDD8nOzmbEiBFs2bKFwYMH88ADD3DRRReRm5vLz3/+cwBuv/12PvroI3Jzc/n4449r1MqjXXbZZQQCAc455xzuu+8+xowZA0BWVhbPPPMMV111Fbm5uVx33XWRbSZNmkRJSUlMmlvAqd3nDk4HvxfWtd6DNEq1Je0+99S0du1afvazn7F6dd1dgTe3+1xntqG7tclFKeVsjz76KE899VRM2s7DnNnk4nFBMD7fLJRSKhbuu+8+vvvuO8aOHRuzfTozobtd+hy6UkrV4syE7vVAld4UVUqpaM5M6EkeqNQaulJKRXNoQvdqDV0ppWpxZkL3aUJXKpYSsfvclStXNvtXoU7nzISelARV+pSLUrGSqN3nnmqcmdB9mtCViqVE7T63LosWLSI7O5shQ4Zw7733AtbP/2fOnMmQIUPIzs7mz3/+M2B1tjVo0CBycnKYNm1aMz/VtufMHxb5kqxXVyuViObOhQ2x7T6XoUPhiVOv+9za9u3bx7333su6devo2LEjl1xyCUuWLKF3797s3buXTZs2AUSajx599FF27dqFz+ers0mpvXFmDT3JB1VASJ90USpWEq373Lp8/vnnjBs3jqysLDweDzfccAOrVq2iX79+7Ny5k7vuuoulS5eSnp4OQE5ODjfccAP/+Mc/8Hjaf/23/UdYF7vbTCqOQ3J6fGNRKtYaqEm3pkTrPrc5OnbsyMaNG3n//fd5+umnefnll3n++ed55513WLVqFW+99RaPPPIIX331VbtO7M6soYcTemntd1UrpVoq0brPrcuoUaP46KOPOHToEMFgkEWLFnHRRRdx6NAhQqEQU6dO5fe//z3r168nFAqxe/duxo8fz2OPPUZRURElJSUndfzW1n4vNQ3x+61h6THoHN9QlEok06dPZ8qUKZGml+juc3v37t2s7nO7du1aZ/e5WVlZjB49OpLEp02bxu233868efNqvLszuvvcQCDAyJEjm9x9btjy5cvp1atXZPqVV17h0UcfZfz48RhjuOKKK5g8eTIbN27klltuibzI4g9/+APBYJAbb7yRoqIijDHMmTOnxU/ytBVndp/7+5vh1y/C15/CWaNiG5hScaDd56q6NLf7XGc2uYRr6GUnfk1TSqlTlTMTui/ZGpYdj28cSinVjjgzofvthF7evm9QKNUc8Wr+VO1TS/4eHJrQU6yh1tBVgvD7/Rw+fFiTugKsZH748GH84eblJnLoUy52Qi/XhK4SQ69evdizZw8HDx6MdyiqnfD7/TWe0GkKhyf00vjGoVSMeL1e+vbtG+8wlMM12uQiIr1F5EMR2SIim0Xk7jrWERGZJyLbReRLERneOuHa/KnWUBO6UkpFNKWGHgB+YYxZLyIdgHUi8oExJrpDh8uBAXYZDTxlD1tHcpo1LC9rtUMopZTTNFpDN8b8YIxZb48XA1uBnrVWmwwsMJZPgEwR6R7zaMOStYaulFK1NespFxHpAwwDPq21qCewO2p6DycmfUTkDhFZKyJrT+rmT4rdIVdFyzoJUkqpRNTkhC4iacBrwFxjTIt6xTLGPGOMyTPG5GVlZbVkF5bUjtbwuD6HrpRSYU1K6CLixUrmC40xr9exyl6gd9R0L3te68joYg01oSulVERTnnIR4DlgqzHmT/Ws9iZws/20yxigyBjzQwzjrCmtkzUs1efQlVIqrClPuZwP3AR8JSLh92L9CjgdwBjzNPAu8GNgO1AK3BL7UKO43eADjutNUaWUCms0oRtj/huQRtYxwL/EKqgm8QmU6mOLSikV5sy+XAD8bijVp1yUUirM2Qm9TBO6UkqFOTihe6CsMt5RKKVUu+HchJ7shVJN6EopFebghJ4E5VXxjkIppdoN5yb0FB+UB+MdhVJKtRvOTejJmtCVUiqacxN6SjKUh+IdhVJKtRvOTugV+v5FpZQKc25CT0uDKqBSn0VXSilwckJPz7CGR1uvDzCllHIS5yb0jExreGRffONQSql2wsEJ3e5C98j++MahlFLthHMTemZna3i0IL5xKKVUO+HchN6xqzUsPBTfOJRSqp1IgIR+OL5xKKVUO+HchN7pNGtYdCS+cSilVDvh4ITewxoWFcY3DqWUaiecm9AzulkvxjtWFO9IlFKqXXBuQnd7wA8cK453JEop1S44N6EDpLig5Hi8o1BKqXbB4QndA8Wl8Y5CKaXaBWcn9FQvlJTFOwqllGoXnJ3Q03xwvCLeUSilVLvg7ISe6ofj+l5RpZQCpyf0DilwPBDvKJRSql1oNKGLyPMiUiAim+pZPk5EikRkg10ein2Y9UhLhVJ9r6hSSgF4mrDOfODfgQUNrLPaGHNlTCJqjvQOUA4Eg+B2t/nhlVKqPWm0hm6MWQW0zw5T0tPBAMcOxjsSpZSKu1i1oZ8rIhtF5D0RGVzfSiJyh4isFZG1Bw/GIAmn228tOqyvoVNKqVgk9PXAGcaYXOAvwJL6VjTGPGOMyTPG5GVlZZ38kTM6WsOj+tYipZQ66YRujDlmjCmxx98FvCLS5aQja4rIW4sOtMnhlFKqPTvphC4ip4mI2OOj7H22zVsnMu3rhr6GTimlGn/KRUQWAeOALiKyB/gN4AUwxjwNXA3MFpEAUAZMM8aYVos4mr61SCmlIhpN6MaY6Y0s/3esxxrbXsfwW4s0oSullLN/KdqpuzXUtxYppVSCJPRjmtCVUsrZCd2XBknoW4uUUgqnJ3QRSBEoLol3JEopFXfOTugAKW4o1tfQKaVUAiR0fWuRUkpBIiT01CQo0bcWKaWU8xN6ml8TulJKkQgJPSMVivU1dEop5fyE3jEDjulbi5RSyvkJvXMnqAJKjsU7EqWUiqsESOh2v+r7d8Y3DqWUijPnJ/Qsu4Ou/bviG4dSSsVZAiR0uz+Xgt3xjUMppeLM+Qm92+nWsGBvfONQSqk4c35C797XGh7UF0UrpU5tzk/oPc4CAfZrQldKndqcn9BTOkKmwN598Y5EKaXiyvkJHaCLD344FO8olFIqrhIjoXdLhwP6kgul1KktMRL6aZ3gUHm8o1BKqbhKjITeswccN1B4JN6RKKVU3CRGQh80xBquXR7fOJRSKo4SI6GPnGAN134Y3ziUUiqOEiOhD54AfuCrL+IdiVJKxU2jCV1EnheRAhHZVM9yEZF5IrJdRL4UkeGxD7MRSR3gDB9s1R4XlVKnrqbU0OcDlzWw/HJggF3uAJ46+bBaYMBpsOMwGBOXwyulVLw1mtCNMauAhh4fmQwsMJZPgEwR6R6rAJts0FnWm4v2ft/mh1ZKqfYgFm3oPYHovmv32PPa1tBR1vDT99r80Eop1R606U1REblDRNaKyNqDBw/GdufnXWl10rVmWWz3q5RSDhGLhL4X6B013cuedwJjzDPGmDxjTF5WVlYMDh2l90joIfD5+tjuVymlHCIWCf1N4Gb7aZcxQJExpu37snW5YVAWfLVHb4wqpU5JTXlscRHwMXC2iOwRkVtF5E4RudNe5V1gJ7AdeBb4X60WbWOGD4HCKsjfEbcQlFIqXjyNrWCMmd7IcgP8S8wiOhnnTwRWwMrXoe898Y5GKdWWDBACgk0szVk31uufC0yI/UfQaEJ3lPOmQNID8OFSuEUTunKIAFBOwwkhAJTZw/aerGKxfku2cVJL671oQm9Up4EwOAk+WhfvSFSshYBKoAKoskvAnldulzK7VNrLotdrzrCxdRoab2haqE5UBnDb8VbF8HOKNcGKs3Zx1TO/vlJ7fS9Wdx3N2aa5x2jP67dS5k2shC4CFwyBeeshPx/69Il3RM5lqE6g4VJea7otl7VC0gtJiIArQFCCBJICBJICBJPsca81HvQECXlDBJOChDwhgp4gQa81P5AcsIaeWkN3AJfHhdflBTdUuiutIpUEJUgGGQQkQIWp4HjScfw+P0neJEpcJZRKKUFX0EqkLjBirMTgwkqCbkDAuOz59nrh8Rrzo5bV2E/U/PJQOSXBEvwePx63h5CEKA2WEiJEpanEYAiZEMFQEL/Hz/Gq46R4UwiZECETwi1uPC4PHpcHt8uNMYaqUBVVwSqCJkjIhDDGRNYPmVBkfl3zgqEggVAAt8uN1+WN7Nvv8VMeKKeksoTKYM24giYYGRpj8Lg8iAiC1BiGRcdYVVVFIBRAENwuN25x43ZZ5xQer29eePpg6UGqQlUEQ0FKKktI8aaQ4k3B5/FRUllCyIQoqSypse20IdO4bfhtMf+bTqyEDnD5FCuhvzEf7v5tvKNpHUGsmt1xoDQGw+jxMqxEWnniYUMSosJdQaW7kip3lTV0VUUSVnheeH5knaQqKn2VVPnsYVL1sCqtikpvJVXeKsq8ZZR4Syj2FlPiKSHoDlYnIReIS2okpQqpoEzKKKWUcsoJSpBSU0rQBHG73ARNkAABAiZAIBQgiJUsAiZAMBTEtJfv6OGLZ5wIcsJnEU4+4XGv20tlsJIkdxIVgQrcLjcucREyISsxhqqvuB6XB6/LG1knnCxd4ooUt9ScDpfwhSEYCkaSZCAUoLSqFL/HT1pSGknupBr7i06UIkJZoAxjDAYTGdbmdXnxur3WhcxlpcFAKBC5OFQGK63pWheM2vMCoQDpvvRIPF1Tu1JWVcaRsiOUBcpI9iTjdXtJS0qLXLyqgtbFpDUkXkIfexN0+jW89Xr7SugGK1kWNlCO2sMioAQryR4Hc9xQUV7B8crjlARKKDEllCSVUO4pp9JdSYWnIpJow+MVHnvaHq/wVVDpq6TCX0FFcgUVGRVUeiup8FZYxV1BpaeSClcFFa4KKqWSCqmwiv1fqSlt1Y/I7/HTIakDHXwdSPWm4nV7qz++Wo+iGgw+t49kbzLdvN3wuX24XW5SvCm4xR2pPXrd3hq1yHDCiIzXqmFGz4+unUUnoejaWvR20dMhEyIQCmCMIcmdRJI7CZ/Hh0tcFJYXEgwFSfGmkOHPoDJYSUWggtSkVFK9qZEEExZdu6wxn3rmN2N9j8tDalIqlcHKSJJJ9iZHEnFj+6zx/8SYJq2nWk/iJfS0M+CCTHhnExw6BF26tM5xgsAhoKCOchBCBSGKjxRTdKyIwopCCisL+SH5B/an7eeY7xjFvmKKfEUc8x3jmO8Y5Z5y6yu7N0hZ9zKOJx2nxFNiFXcJQQmeVLg+tw+fx4fP7Yskl/C8JHcSPrePFKIWYX0AABY2SURBVE8KHd0day6PWictKY1kTzJJ7iS8bq81dHlPmA6PR0+H16tv23DtSsWH3+PH7/Gf1D70/1/8JV5CB7jpGnjjWZj/FPzrr1u2DwPsB3YAu8DsMhz9/ih79u9hX+E+9lbsZW/qXvZ12McPaT9wOOUwRb4iivxFFHUporhHsdVuWQ+/+MnwZpDhyyDdn47f58ftduNz+cj0ZJKWlEaqN5W0pLQaJTUpNbLM7/HXSMz1JWuvy6v/2JQ6BUjtr7JtJS8vz6xdu7Z1dn7saxg2EFzd4Ot94Grk91MG+AL4DMq/Kuer775i65GtbOywka+6fcWmrpsoSC2wblbV0sXdhe7+7nRJ7UJGWgaZHTLJ8FuJOnqY6c+ke1p3uqV1I8OXUaM5QSmlmkpE1hlj8upalpg19PSzYeqZ8McdsGgR3HBD3esdBV6H8qfKebXqVRbkLmD1Gasp71oOgB8/g1MGc0nXS+jerTtZGVn0Su9Fzw496Znek+5p3fF5fG13Xkop1YDErKEDbH8BLp4FVV3g2+8hOdm64fgaBD8OsnPbTlaVreLtAW/zwYAPOO45Tv8O/bli0BVceMaFDMoaRP9O/U+4QaWUUvF06tXQAfrdBHf8Bn61G+67H7KeYNtft7Hg7AU8N+I5Ci4uAKC3vzc3D7mZq865igl9J+CSxHjNqlLq1JO4Cd3lgev/DT68FjNvHn8Y+xUP3LYCFy6uPOtKJg+czMgeIxnSdYjeMFRKJYSETegVH1Tw1hOweHgWD244yK1frMDz0zlcf/kv6ZXeK97hKaVUzCVe+0IQvrnrG0a9PoprRl3LstQQr97qIivo4p55a+nl7xrvCJVSqlUkXEL/5I+fMKLDCPZ23curP3mVvQ/s59e3/AXXbSFYswbu0V4YlVKJKaESutlvmLtrLpmSyRc//4KpuVOtp1QGzIapV8JlLnjySXj++XiHqpRSMZdQbehv/uFNPu3xKc+OepbeGVGvORWBMc/DbefCvu/gttsgKQluvDF+wSqlVIwlTA1967tbudV/KwODA5lxyYwTV/BnwaXL4P4uMMQLM2bA00+3faBKKdVKEiKh73ljD5cuuxSPeHj7lrfr/1l9Wh+4fDnc3xGGuWH2bPjFLyAQaNN4lVKqNTg+oZug4db3buVoylGWzljKmX3PbHiDjEHwPz6GB3rBpR7405/g4ovhhx/aJmCllGoljk/o363/jn92/ycPdH6AoecMbdpGaX3hsv8Hdw+BOwU++X8wZIjVBBM8uW5qlVIqXhyf0Des3QDAuLxxzdswuTtc/N9w/VXwcBWc7rGaYPLy4J//hDj1caOUUi3l+IS+cddGxAjZednN39iTCmNfhssehl8egp+nQ8FuuPRSuOgiWLUq9gErpVQrcXxC/7LsS/qX9ic1ObVlOxAXZP8aLl8PlwyE/3MYftoPvtlqJfVzz4V//AMqKmIbuFJKxZjjE/pO1076h/qf/I465sLFa+D8v8L4Unj0EMzJhoP74Kab4LTTYNYsWLoUqlrnBa9KKXUynP3DohDsStnFee7zYrM/lxv63wFnXA/b/gRpT8DIo7A3Gz7NhNdegxdegE6d4PLLraaZSy6Bbt1ic3yllDoJTaqhi8hlIvK1iGwXkfvqWD5TRA6KyAa73Bb7UE909LujFPmL6JvZN7Y79qZB9kMw+XvI+zMMLIarV8NTbvi3/wEXn2vdOL35ZqvmPmwY3HcfLF8OxcWxjUUppZqo0Rq6iLiB/wAuBvYAn4vIm8aYLbVW/S9jzE9bIcZ65X+dD0Df02Kc0MO8aTBwLpw9B/Yvhx3PgWsxTKqEaX3h+KWwNRVWb4V/+zd47DGrm4FBg2DMGBg9GkaOhHPOAZ++qk4p1bqa0uQyCthujNkJICIvAZOB2gm9ze36fhcAffu0UkIPExd0v9gq5Ydgz2LYvRjK/wsGVcHwbvDAFNjTE3YAX3wNixfDc89Z23s8VlLPyYHc3Orhaae1btxKqVNKUxJ6T2B31PQeYHQd600VkQuBb4CfGWN2115BRO4A7gA4/fTTmx9tLbsK7IQ+sJUTejR/F+h/u1Uqi2Dfu7D3LTiwHPwFMBgYfSY8NAWO94fvvPDtIfjyS/joI1i4sHpfWVkwcCCcdZZVzj7bGvbrpzV6pVSzxeqm6FvAImNMhYj8T+DvwITaKxljngGeAesl0Sd70F3HdpHuSSezc+bJ7qplkjKgz3SrGANFm62mmQPLYferUFUEfmB4Blw8GrrMAhkAezzw9X7YtAm++QbefhsOHKjer8sFfftaib1PH2u8T5/q8W7drKYdpZSK0pSEvheI6ouWXva8CGPM4ajJvwH/9+RDa9yuyl30DfRtH+8EFYHMIVYZeDeYEBz7Bg5/Aofssvn31nyAnpkwJBc6DoeOs0D6wX6Bnd/D119biX7XLliyBA4erHksvx/OOMNK7r17Q48e0LOnNQyPd+liXRiUUqeMpiT0z4EBItIXK5FPA66PXkFEuhtjwr1bTQK2xjTKeuxy7eJszm6LQzWfuCBjoFX6zbTmBY5D4VdwdEN12f4sBEvDG0HaGTB+IEweCOnjIX0geE6H/aWQn2+VXbuqh+vXQ0HBicf3eKB795qJvnt36Nr1xJKW1iYfiVKqdTWa0I0xARH5KfA+4AaeN8ZsFpGHgbXGmDeBOSIyCQgAR4CZrRgzAKFgiF0pu7jcXN7ah4odTyp0GWOVsFAQSrbD0Y1wbFt1KfgIgmXV6yV1hA5nwdB+MPZMSLsCOpwJaf3AkwUHCmDfPti71xpGj2/bZj1SWVRUd1wpKXUn+q5dread6OkuXayLhVKq3WnSv0xjzLvAu7XmPRQ1fj9wf2xDa9iebXso95ZzdkY7raE3lcsN6WdbJZoJQekeKNpqJ/mtULzdarr5/mUwUb1CunxWD5Jp/eD0fnBOH0gdBimTIOV0SD7N+sZQXm413xQUWOXAgerxcNm7t7rWX18/8Z07Wzd0O3e2fmRVX+nYsXo8I0ObgJRqZY6tan2z9RsABpwxIM6RtBJxQerpVulxac1loSo4/j2U7ISSHfbQHi9YDYFaP25yeSG5p7WvlNMhpTf0Px1yTofUEda8pIya2xgDhYU1k33tC8DRo7B7N2zcCEeOQElJA+cj1Qk+M9MqGRl1lw4d6i9JSbH5fJVKQI5N6N9+9y0AZw0+K86RxIHLazW3dDgT6/deUYyxnq45/j2U7obS76vHj38PB1dD6V4wtWrf3nQr6Sd3h+QedrHHz+4OQ4da056U+uOqrLSS/JEjVokeD5fDh60LRVGR1RwUHj9+vGnn7vM1nPDrKqmpVrNS9DA52Sp+v1U8Hn1ySDmeYxN6/tF8kiSJHv17xDuU9kUEkjKt0jGn7nVCQSjff2LSL9tnlYOroewHCFWeuK03o2ayrz1M6Q6deoFnYPMSZCAAx45Zyb24uGnl2DFreOiQdYM4PL+kpPn92btc1cnd76+Z7GuXhpY1ZdukJPB6rYtIcrJ10dH7EioGHPtXVFBeQDfphri0VtVsLjek9LQK59a9jjFQecRK7OFEHxn/ofHE7/aDryv4u4IvyxqGi69rreks8Piq29tPljFWjT+c4EtLrXL8ePWwrMy6pxAutadrl7Iy6xtHfctO9oUoIlaST0o6sfh81eNeb/XFIDxee57HA263NayruN01S13zmlpaY1uXS78ttZBjE/qB4AG60jXeYSQuEfB1tkrmkPrXqyvxlx+EigIot0tFARRtgvIDdSd/sJp8fF3BnwVJncHXKWpoF1/UtK8zeDrU/Q9fxHoUMy3NelSztRljdanc0MUgerqiwvpGUlVlXWCKi63xcKmstEpFRfV4eLqqytq2rKzmNuH9BQI1SzBYPe6kbp9drvhcTESqLyj1jdeeFy7hi1FdF6jwAwHhC/+558L48TH/2Byb0Aso0ITeHjQ18YP1xxwork704WRffjBqvADK9kDhl9aFItDQjVa3ndzt5B8ZD5cM8GZWN0F5M+xhJng7WDeeY/UZhGvQ6emx2WdrCYWqE3241J5uTjmZbdv62BUVjW9vjFVCoYbHaw+DQWu89r7qc++9mtCjFXgKyDYteO2cih8RqybuTYcOTXwpSbACKo9ayb3isD08UnM6PB65EBy2fsTVcDBWHOEEn5Rh1fi96Vay96bb09Hj9rLa424HPXnjcumTQm0lnPDDyT78bTJcs28FjkzoJmQ44D9A15DW0BOe22c9R5/czJ4pQ1VW52lVhVBZaA+jp4usYXhZVZHVXHRsm/Utoqq45g+7GuLy1Uz8nlS7pEWNRxW3PfSmVY+fUNLAlaRtyU4mUt3s0kYcmdCPHT5GpaeSrkma0FU9XF6rZ0x/l5bvIxSoTu5Vx6qH0fMCUcvC44Hj1rDsB2s8eByqSqK6eGgicZ94EahxYUiuLp5kcKfUmo5enlLHdIq1H71wJAxHJvRDew8B0CX9JP6xKtUYl8fqciGpY2z2Z0JWrT9wvI5SUnM62MDyqmPWY6eBMusiESyzS3nL4hK3ndxrXQRqXzBcfusbk9tvFZcvauiz9uPyWvcqvJlR2/jt/dQadzky/bRrjvxEjx4+CkDH9Bj9Q1OqLYirunbdGkzIuucQTvCB6GRfduL8QKl94Si1LyKl9kUiupRa9ySC5da+Q/YwWG6Nh3sPbQnxWBeC8AUhfMGITNce1rM8+uJSezw87fJZ9zpcPusbidseRk/H6iZ5HDkyoRceLQSgY6YmdKUixGXVij3JbXfMUABCFVaSN0HrsdSqIutGdrDcvihUVH+DiMyzLwi1LxThfYWHgeMQPFJzfo11WvitpC7irjvhN3YhqG+6oe3SB0Lm4NjFbnNmQi+yEnpmxzi92EIpZXF5rFLjW0fPtju+MdYN8OiLQ50Xi0r7YlBpz6s93dCyWvMjF5kG9hlq5Jn/QffC0Edj/nE4MqEfLbabXLK0hq7UKU3Eqgm7k6ynjNoLE7IvNBV1X0x8nVvlsI5M6IXH7Rp6V62hK6XaIXHZ7fs+8LbdYR15F+Bo+VHcITepGa10c0kppRzImTX0ikI6ujpqx1xKKRXFmQk9UEimS5tblFIqmiMT+tHQUTKNJnSllIrmyDb0/bKf06SZfXsopVSCc2RC35e0jx4efVORUkpFc1yTS1V5FQUpBXQ3bfDiAqWUchDH1dAPfH8AgB4ZWkNXSqlojkvo+77fB0CPzprQlVIqmvMS+n4roXdvi3dFKqWUgzguoffL6se9BffSd0DfeIeilFLtSpNuiorIZcCTgBv4mzHm0VrLfcACYARwGLjOGJMf21AtOZfmkHNpTmvsWimlHK3RGrqIuIH/AC4HBgHTRWRQrdVuBY4aY/oDfwYei3WgSimlGtaUJpdRwHZjzE5jTCXwEjC51jqTgb/b468CE0X0JYVKKdWWmpLQewK7o6b3cGIP9pF1jDEBoAg4ocNfEblDRNaKyNqDBw+2LGKllFJ1atObosaYZ4wxecaYvKysrLY8tFJKJbymJPS9QO+o6V72vDrXEREPkIF1c1QppVQbaUpC/xwYICJ9RSQJmAa8WWudN4EZ9vjVwApjjIldmEoppRrT6GOLxpiAiPwUeB/rscXnjTGbReRhYK0x5k3gOeBFEdkOHMFK+koppdpQk55DN8a8C7xba95DUePlwDWxDU0ppVRzSLxaRkTkIPBdCzfvAhyKYThOoOd8atBzPjWczDmfYYyp86mSuCX0kyEia40xefGOoy3pOZ8a9JxPDa11zo7ry0UppVTdNKErpVSCcGpCfybeAcSBnvOpQc/51NAq5+zINnSllFIncmoNXSmlVC2a0JVSKkE4LqGLyGUi8rWIbBeR++IdT6yIyPMiUiAim6LmdRKRD0TkW3vY0Z4vIjLP/gy+FJHh8Yu85USkt4h8KCJbRGSziNxtz0/Y8xYRv4h8JiIb7XP+3/b8viLyqX1u/2V3s4GI+Ozp7fbyPvGMv6VExC0iX4jI2/Z0Qp8vgIjki8hXIrJBRNba81r1b9tRCb2JL9twqvnAZbXm3QcsN8YMAJbb02Cd/wC73AE81UYxxloA+IUxZhAwBvgX+/9nIp93BTDBGJMLDAUuE5ExWC+F+bP9kpijWC+NgcR5eczdwNao6UQ/37DxxpihUc+ct+7ftjHGMQU4F3g/avp+4P54xxXD8+sDbIqa/hrobo93B762x/8KTK9rPScX4A3g4lPlvIEUYD0wGutXgx57fuTvHKsPpXPtcY+9nsQ79maeZy87eU0A3gYkkc836rzzgS615rXq37ajaug07WUbiaSbMeYHe3w/0M0eT7jPwf5qPQz4lAQ/b7v5YQNQAHwA7AAKjfVyGKh5Xk16eUw79wRwDxCypzuT2OcbZoB/isg6EbnDnteqf9tN6pxLxZ8xxohIQj5jKiJpwGvAXGPMsei3FybieRtjgsBQEckEFgMD4xxSqxGRK4ECY8w6ERkX73ja2FhjzF4R6Qp8ICLbohe2xt+202roTXnZRiI5ICLdAexhgT0/YT4HEfFiJfOFxpjX7dkJf94AxphC4EOsJodM++UwUPO8nP7ymPOBSSKSj/U+4gnAkyTu+UYYY/bawwKsC/coWvlv22kJvSkv20gk0S8OmYHVxhyef7N9Z3wMUBT1Nc4xxKqKPwdsNcb8KWpRwp63iGTZNXNEJBnrnsFWrMR+tb1a7XN27MtjjDH3G2N6GWP6YP17XWGMuYEEPd8wEUkVkQ7hceASYBOt/bcd7xsHLbjR8GPgG6x2xwfiHU8Mz2sR8ANQhdV+ditW2+Fy4FtgGdDJXlewnvbZAXwF5MU7/hae81isdsYvgQ12+XEinzeQA3xhn/Mm4CF7fj/gM2A78Args+f77ent9vJ+8T6Hkzj3ccDbp8L52ue30S6bw7mqtf+29af/SimVIJzW5KKUUqoemtCVUipBaEJXSqkEoQldKaUShCZ0pZRKEJrQVcIRkaDdw124xKxXThHpI1E9YirVnuhP/1UiKjPGDI13EEq1Na2hq1OG3T/1/7X7qP5MRPrb8/uIyAq7H+rlInK6Pb+biCy2+y7fKCLn2btyi8izdn/m/7R/8YmIzBGrb/cvReSlOJ2mOoVpQleJKLlWk8t1UcuKjDHZwL9j9QII8Bfg78aYHGAhMM+ePw/4yFh9lw/H+sUfWH1W/4cxZjBQCEy1598HDLP3c2drnZxS9dFfiqqEIyIlxpi0OubnY71cYqfdKdh+Y0xnETmE1fd0lT3/B2NMFxE5CPQyxlRE7aMP8IGxXlCAiNwLeI0xvxeRpUAJsARYYowpaeVTVaoGraGrU42pZ7w5KqLGg1Tfi7oCqz+O4cDnUb0JKtUmNKGrU811UcOP7fE1WD0BAtwArLbHlwOzIfJSioz6dioiLqC3MeZD4F6sbl9P+JagVGvSGoRKRMn2G4HClhpjwo8udhSRL7Fq2dPteXcBL4jIL4GDwC32/LuBZ0TkVqya+GysHjHr4gb+YSd9AeYZq79zpdqMtqGrU4bdhp5njDkU71iUag3a5KKUUglCa+hKKZUgtIaulFIJQhO6UkolCE3oSimVIDShK6VUgtCErpRSCeL/A5nkEeDemQr6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL1CZRN_pUQ-"
      },
      "source": [
        ""
      ]
    }
  ]
}