{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_A1_Question1_2_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "metadata": {
      "interpreter": {
        "hash": "de1b2a626baae37f500cb083f8858eb0d1eb85e0128ef62fa24f59782d154f26"
      }
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhEu3ph3mmnZ",
        "outputId": "f780c5a1-40d9-48ad-c89d-583576841785"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuM6HeNym-fG"
      },
      "source": [
        "# import required libraries\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Setting Random Seed\n",
        "# tf.random.set_seed(195470)\n",
        "\n",
        "# load and prepare the training and test data\n",
        "def load_Fashion_MNIST():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "    # load the training and test data    \n",
        "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
        "\n",
        "    # reshape the feature data\n",
        "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
        "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
        "\n",
        "    # noramlise feature data\n",
        "    tr_x = tr_x / 255.0\n",
        "    te_x = te_x / 255.0\n",
        "\n",
        "    print( \"Shape of training features \", tr_x.shape)\n",
        "    print( \"Shape of test features \", te_x.shape)\n",
        "\n",
        "\n",
        "    # one hot encode the training labels and get the transpose\n",
        "    tr_y = np_utils.to_categorical(tr_y,10)\n",
        "    tr_y = tr_y.T\n",
        "    print (\"Shape of training labels \", tr_y.shape)\n",
        "\n",
        "    # one hot encode the test labels and get the transpose\n",
        "    te_y = np_utils.to_categorical(te_y,10)\n",
        "    te_y = te_y.T\n",
        "    print (\"Shape of testing labels \", te_y.shape)\n",
        "    \n",
        "    # Reshape the training data and test data so \n",
        "    # that the features becomes the rows of the matrix\n",
        "    tr_x = tr_x.T\n",
        "    te_x = te_x.T\n",
        "\n",
        "    print(\"Reshaped training data \", tr_x.shape)\n",
        "    print(\"Reshaped test data \",te_x.shape)\n",
        "    \n",
        "    return tr_x, tr_y, te_x, te_y"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzqqwhfpoDzN"
      },
      "source": [
        "#  push a matrix of feature data through the neural network and the final Softmax layer\n",
        "def forward_pass(x, w_1, b1, w_2, b2, w_3, b3,dropOutProb=0.01):\n",
        "\n",
        "    #Setting probability threshold for dropout layer\n",
        "    probThreshold = 1-dropOutProb\n",
        "\n",
        "    # First Layer\n",
        "    A1 = tf.matmul(w_1, x) + b1 #WX + b\n",
        "    H1 =  tf.keras.activations.relu(A1) #ReLu activation\n",
        "    #Applyting dropout to H1\n",
        "    drop_matrix = tf.cast(tf.random.uniform(shape=H1.shape)<probThreshold,tf.float32)\n",
        "    H1 = tf.divide(tf.multiply(H1,drop_matrix),probThreshold)\n",
        "    \n",
        "    # Second Layer\n",
        "    A2 = tf.matmul(w_2, H1) + b2 #WX + b\n",
        "    H2 =  tf.keras.activations.relu(A2) #ReLu activation\n",
        "\n",
        "    # Third Layer / Output Layer\n",
        "    A3 = tf.matmul(w_3, H2) + b3 #WX + b\n",
        "\n",
        "    #SoftMax activation\n",
        "    e_A3 = tf.math.exp(A3) #e^A3\n",
        "    return tf.divide(e_A3,tf.reshape(tf.reduce_sum(e_A3,0),[1, -1])) #e^A3/sum(e^A3)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BtDUop-oU_Z"
      },
      "source": [
        "\n",
        "def cross_entropy(y, y_pred):\n",
        "    # mean of cross entropy loss\n",
        "    return tf.reduce_mean(-(tf.reduce_sum(tf.multiply(y, tf.math.log(y_pred)), 0))) #mean(-sum(y*log(h(x)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My675H4XoWfv"
      },
      "source": [
        "\n",
        "def calculate_accuracy(y, y_pred_softmax):\n",
        "    #Compare every predicted value to original value individually which will result in boolean array which will be casted to zeros and ones\n",
        "    #Taking the mean of this array will give us the accuracy since 1 is correct and 0 is wrong and the mean is accuracy!\n",
        "    #Example tf.Tensor([ True False False ... False False False], shape=(60000,), dtype=bool) casted to [1. 0. 0. ... 0. 0. 0.], shape=(60000,), dtype=float32)\n",
        "    #and the Mean gives 0.10131667 which is the accuracy\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_pred_softmax, 0), tf.argmax(y, 0)), tf.float32))  \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYThBBUr9Yf8"
      },
      "source": [
        "#Function to type cast to float32 by default\n",
        "def fl_type_caster(x,t=tf.float32):\n",
        "  return tf.cast(x,t)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n5-bCvp3oYNd",
        "tags": [],
        "outputId": "665a7148-b078-410f-98ca-ecd4e476eb34"
      },
      "source": [
        "#Number of Iterations\n",
        "epochs = 500\n",
        "training_accuracy, training_loss, validation_accuracy, validation_loss = [],[],[],[]\n",
        "\n",
        "# load the prepared data and typecast the training and test data\n",
        "tr_x, tr_y, te_x, te_y = [fl_type_caster(i) for i in load_Fashion_MNIST()]\n",
        "\n",
        "#Initializing the weights and biases for 1st layer\n",
        "w1 = tf.Variable(tf.random.normal([300, tr_x.shape[0]], mean=0.0, stddev=0.1))\n",
        "b1 = tf.Variable(tf.zeros([300, 1]))\n",
        "\n",
        "#Initializing the weights and biases for 2nd layer\n",
        "w2 = tf.Variable(tf.random.normal([100, 300], mean=0.0, stddev=0.1))\n",
        "b2 = tf.Variable(tf.zeros([100, 1]))\n",
        "\n",
        "#Initialize the weights and biases for the SoftMax ie Output layer\n",
        "w3 = tf.Variable(tf.random.normal([ tr_y.shape[0], 100], mean=0.0, stddev=0.1))\n",
        "b3 = tf.Variable(tf.zeros([tr_y.shape[0], 1]))\n",
        "\n",
        "#Instantiate Optimizer\n",
        "adam_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training loop\n",
        "for i in range(epochs):\n",
        "    if i%10 == 0:\n",
        "      print(\"+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+\".format(\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19))\n",
        "      print(\"| {:^19} | {:^19} | {:^19} | {:^19} | {:^19} |\".format(\"Epoch\",\"Training_loss\",\"Training_accuracy\",\"Validation_loss\",\"Validation_accuracy\"))\n",
        "      print(\"+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+\".format(\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19))\n",
        "\n",
        "    # Instantiating Gradient Tape to monitor the forward pass & calculate gradients ie autodiff\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = forward_pass(tr_x, w1, b1, w2, b2, w3, b3)\n",
        "        loss = cross_entropy(tr_y, y_pred)\n",
        "    training_loss.append(loss)\n",
        "    \n",
        "    # Prediction Accuracy\n",
        "    accuracy = calculate_accuracy(tr_y, y_pred)\n",
        "    training_accuracy.append(accuracy)\n",
        "\n",
        "    # Calculate gradients\n",
        "    gradients = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
        "\n",
        "    # Forward propagate and calculate accuracy and loss for the valdation data\n",
        "    te_y_pred = forward_pass(te_x, w1, b1, w2, b2, w3, b3) \n",
        "    val_loss = cross_entropy(te_y, te_y_pred)\n",
        "    val_accuracy = calculate_accuracy(te_y, te_y_pred) \n",
        "    validation_accuracy.append(val_accuracy)\n",
        "    validation_loss.append(val_loss)\n",
        "\n",
        "    print(\"| {:^19} | {:^19} | {:^19} | {:^19} | {:^19} |\".format(i+1,round(float(loss.numpy()),5),round(float(accuracy.numpy()),5),round(float(val_loss.numpy()),5),round(float(val_accuracy.numpy()),5)))\n",
        "    \n",
        "    # Adam Optimizer to update the weights and biases accordingly\n",
        "    adam_optimizer.apply_gradients(zip(gradients, [w1, b1, w2, b2, w3, b3]))\n",
        "\n",
        "print(\"+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+\".format(\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19))\n",
        "\n",
        "# Plot the training and the validation accuracy and loss\n",
        "plt.plot(training_accuracy, label=\"Train Accuracy\",color='magenta')\n",
        "plt.plot(training_loss, label=\"Train Loss\",color='orange')\n",
        "plt.plot(validation_accuracy, label=\"Validation Accuracy\",color='green')\n",
        "plt.plot(validation_loss, label=\"Validation Loss\",color='red')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training features  (60000, 784)\n",
            "Shape of test features  (10000, 784)\n",
            "Shape of training labels  (10, 60000)\n",
            "Shape of testing labels  (10, 10000)\n",
            "Reshaped training data  (784, 60000)\n",
            "Reshaped test data  (784, 10000)\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|          1          |       2.69362       |       0.12373       |       2.68817       |       0.1253        |\n",
            "|          2          |       2.10308       |       0.25482       |       2.10154       |       0.2563        |\n",
            "|          3          |       1.82797       |       0.40982       |       1.82983       |        0.414        |\n",
            "|          4          |       1.60505       |       0.50563       |       1.60813       |       0.5048        |\n",
            "|          5          |       1.41454       |       0.57903       |       1.42171       |       0.5699        |\n",
            "|          6          |       1.26632       |       0.60648       |       1.27448       |       0.5968        |\n",
            "|          7          |        1.146        |       0.6324        |       1.15645       |       0.6198        |\n",
            "|          8          |       1.05048       |       0.6509        |       1.06181       |       0.6408        |\n",
            "|          9          |       0.97389       |        0.666        |       0.98367       |       0.6602        |\n",
            "|         10          |       0.91501       |       0.6835        |       0.92633       |       0.6741        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         11          |       0.87208       |       0.69688       |       0.88419       |        0.687        |\n",
            "|         12          |       0.83539       |       0.70817       |       0.85165       |       0.6934        |\n",
            "|         13          |       0.80245       |       0.71783       |       0.81754       |       0.7046        |\n",
            "|         14          |       0.77185       |       0.72665       |       0.78816       |       0.7128        |\n",
            "|         15          |       0.7436        |       0.73718       |       0.76477       |       0.7227        |\n",
            "|         16          |       0.72161       |       0.74503       |       0.74287       |       0.7296        |\n",
            "|         17          |       0.70389       |       0.75188       |       0.72522       |       0.7386        |\n",
            "|         18          |       0.68414       |       0.7606        |        0.709        |       0.7482        |\n",
            "|         19          |       0.66584       |       0.7678        |       0.68889       |       0.7567        |\n",
            "|         20          |       0.6509        |       0.77552       |       0.67882       |       0.7594        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         21          |       0.63557       |       0.78263       |       0.65872       |       0.7693        |\n",
            "|         22          |       0.62327       |       0.78765       |       0.64823       |        0.775        |\n",
            "|         23          |       0.60885       |        0.793        |       0.63401       |       0.7779        |\n",
            "|         24          |       0.59669       |       0.79718       |       0.62326       |       0.7834        |\n",
            "|         25          |       0.58727       |       0.80147       |       0.61459       |       0.7868        |\n",
            "|         26          |       0.57682       |       0.80483       |       0.60144       |       0.7931        |\n",
            "|         27          |       0.56752       |       0.80945       |       0.59088       |       0.7982        |\n",
            "|         28          |       0.55863       |       0.81158       |       0.58366       |        0.799        |\n",
            "|         29          |       0.55057       |       0.81403       |       0.57738       |       0.8009        |\n",
            "|         30          |       0.5439        |       0.81673       |       0.56896       |       0.8045        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         31          |       0.53579       |       0.81892       |       0.56002       |       0.8082        |\n",
            "|         32          |       0.53003       |       0.8207        |       0.5572        |       0.8056        |\n",
            "|         33          |       0.52315       |       0.82288       |       0.55053       |       0.8117        |\n",
            "|         34          |       0.51844       |       0.82475       |       0.54358       |       0.8152        |\n",
            "|         35          |       0.51238       |       0.82558       |       0.53706       |       0.8144        |\n",
            "|         36          |       0.50676       |       0.82733       |       0.53379       |       0.8161        |\n",
            "|         37          |       0.50191       |       0.82867       |       0.52589       |        0.817        |\n",
            "|         38          |       0.49693       |       0.82947       |       0.52279       |       0.8172        |\n",
            "|         39          |       0.49238       |       0.83137       |       0.51731       |       0.8233        |\n",
            "|         40          |       0.48656       |       0.8342        |       0.51574       |       0.8219        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         41          |       0.48319       |       0.8356        |       0.50926       |       0.8241        |\n",
            "|         42          |       0.4788        |       0.83552       |       0.50719       |       0.8248        |\n",
            "|         43          |       0.47502       |       0.83535       |       0.50303       |       0.8236        |\n",
            "|         44          |       0.47059       |       0.83697       |       0.50149       |        0.826        |\n",
            "|         45          |       0.4668        |       0.8382        |       0.49699       |       0.8276        |\n",
            "|         46          |       0.46453       |       0.83935       |       0.49456       |       0.8281        |\n",
            "|         47          |       0.46082       |       0.83937       |       0.49119       |       0.8286        |\n",
            "|         48          |       0.45735       |       0.84168       |       0.48783       |       0.8278        |\n",
            "|         49          |       0.45305       |       0.84245       |       0.48364       |       0.8294        |\n",
            "|         50          |       0.45067       |       0.84327       |       0.4828        |       0.8285        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         51          |       0.44715       |       0.84372       |       0.47986       |       0.8323        |\n",
            "|         52          |       0.44401       |       0.84475       |       0.47759       |       0.8325        |\n",
            "|         53          |       0.44128       |       0.84507       |       0.47424       |       0.8329        |\n",
            "|         54          |       0.4379        |       0.84783       |       0.47097       |        0.832        |\n",
            "|         55          |       0.43512       |       0.8477        |       0.46736       |       0.8343        |\n",
            "|         56          |       0.43224       |       0.84893       |       0.46451       |       0.8348        |\n",
            "|         57          |       0.42953       |       0.8493        |       0.46185       |       0.8374        |\n",
            "|         58          |       0.42732       |       0.85105       |       0.46091       |       0.8364        |\n",
            "|         59          |       0.42498       |       0.85145       |       0.45984       |       0.8364        |\n",
            "|         60          |       0.42186       |       0.85278       |       0.45356       |        0.838        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         61          |       0.41982       |       0.85282       |       0.45335       |       0.8402        |\n",
            "|         62          |       0.41672       |       0.85467       |       0.4499        |       0.8436        |\n",
            "|         63          |       0.41469       |       0.85507       |       0.4485        |       0.8411        |\n",
            "|         64          |       0.41207       |       0.85512       |       0.44802       |       0.8403        |\n",
            "|         65          |       0.40969       |       0.85772       |       0.44544       |        0.841        |\n",
            "|         66          |       0.40765       |       0.8568        |       0.44369       |       0.8413        |\n",
            "|         67          |       0.40547       |       0.85748       |       0.44143       |       0.8432        |\n",
            "|         68          |       0.40264       |       0.85935       |       0.44063       |       0.8429        |\n",
            "|         69          |       0.40112       |       0.86018       |       0.43704       |       0.8447        |\n",
            "|         70          |       0.39881       |       0.86038       |       0.43632       |       0.8439        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         71          |       0.39645       |       0.8611        |       0.43363       |        0.844        |\n",
            "|         72          |       0.39437       |       0.8615        |       0.43123       |       0.8466        |\n",
            "|         73          |       0.39301       |       0.86252       |       0.43134       |       0.8466        |\n",
            "|         74          |       0.39059       |       0.86347       |       0.42955       |       0.8461        |\n",
            "|         75          |       0.38914       |       0.86377       |       0.42682       |       0.8487        |\n",
            "|         76          |       0.38749       |       0.86383       |       0.42574       |       0.8463        |\n",
            "|         77          |       0.38518       |       0.86632       |       0.42311       |       0.8493        |\n",
            "|         78          |       0.38343       |       0.86637       |       0.42297       |        0.849        |\n",
            "|         79          |       0.38211       |       0.86613       |       0.42001       |       0.8493        |\n",
            "|         80          |       0.38083       |       0.86692       |       0.42297       |       0.8496        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         81          |       0.37899       |       0.86677       |       0.42015       |       0.8502        |\n",
            "|         82          |       0.37709       |       0.86823       |       0.42011       |        0.851        |\n",
            "|         83          |       0.37549       |       0.86847       |       0.41753       |        0.851        |\n",
            "|         84          |       0.37345       |       0.8685        |       0.41354       |       0.8524        |\n",
            "|         85          |       0.37104       |       0.86957       |       0.41439       |       0.8523        |\n",
            "|         86          |       0.3697        |       0.86965       |       0.41165       |       0.8531        |\n",
            "|         87          |       0.36812       |       0.87057       |       0.4098        |       0.8533        |\n",
            "|         88          |       0.36636       |       0.87067       |       0.40973       |        0.854        |\n",
            "|         89          |       0.36504       |       0.87103       |       0.40719       |       0.8554        |\n",
            "|         90          |       0.36395       |       0.87212       |       0.40625       |       0.8542        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         91          |       0.36318       |       0.87203       |       0.40615       |       0.8573        |\n",
            "|         92          |       0.36188       |       0.87275       |       0.40585       |       0.8543        |\n",
            "|         93          |       0.35939       |       0.87332       |       0.40552       |       0.8546        |\n",
            "|         94          |       0.35852       |       0.8736        |       0.40427       |       0.8558        |\n",
            "|         95          |       0.35814       |       0.8738        |       0.40232       |       0.8543        |\n",
            "|         96          |       0.35541       |       0.87423       |       0.40214       |       0.8565        |\n",
            "|         97          |       0.35499       |       0.87512       |       0.39974       |       0.8532        |\n",
            "|         98          |       0.35262       |       0.87513       |       0.40006       |       0.8574        |\n",
            "|         99          |       0.35182       |       0.87605       |       0.39911       |       0.8573        |\n",
            "|         100         |       0.35023       |       0.87672       |       0.39786       |        0.858        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         101         |       0.34916       |       0.87673       |       0.39623       |       0.8566        |\n",
            "|         102         |       0.34793       |       0.87682       |       0.39684       |       0.8568        |\n",
            "|         103         |       0.3467        |        0.878        |       0.39479       |       0.8597        |\n",
            "|         104         |       0.34553       |       0.87845       |       0.39454       |       0.8587        |\n",
            "|         105         |       0.34494       |       0.87752       |       0.39308       |       0.8608        |\n",
            "|         106         |       0.34293       |       0.87853       |       0.39324       |       0.8588        |\n",
            "|         107         |       0.34294       |       0.87857       |       0.39383       |       0.8585        |\n",
            "|         108         |       0.34146       |       0.8794        |       0.38951       |       0.8604        |\n",
            "|         109         |       0.34004       |       0.87955       |       0.38929       |        0.862        |\n",
            "|         110         |       0.33836       |       0.88058       |       0.38953       |       0.8604        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         111         |       0.33683       |       0.87993       |       0.3896        |       0.8581        |\n",
            "|         112         |       0.33686       |       0.8816        |       0.38906       |        0.861        |\n",
            "|         113         |       0.33515       |       0.88107       |       0.38701       |       0.8614        |\n",
            "|         114         |       0.3345        |       0.88243       |       0.38538       |       0.8633        |\n",
            "|         115         |       0.33383       |       0.88163       |       0.38599       |       0.8656        |\n",
            "|         116         |       0.33168       |       0.88325       |       0.3834        |       0.8626        |\n",
            "|         117         |       0.33042       |       0.88252       |       0.38497       |       0.8619        |\n",
            "|         118         |       0.32874       |       0.88317       |       0.38431       |       0.8647        |\n",
            "|         119         |       0.32827       |       0.88403       |       0.38288       |       0.8636        |\n",
            "|         120         |       0.32766       |       0.8833        |       0.38181       |       0.8644        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         121         |       0.32575       |       0.88407       |       0.38064       |       0.8637        |\n",
            "|         122         |        0.325        |       0.88382       |       0.37963       |       0.8679        |\n",
            "|         123         |       0.32396       |       0.8844        |       0.37967       |       0.8662        |\n",
            "|         124         |       0.32314       |       0.88485       |       0.37945       |       0.8647        |\n",
            "|         125         |       0.32257       |       0.88532       |       0.37924       |       0.8634        |\n",
            "|         126         |       0.32188       |       0.8854        |       0.37712       |       0.8643        |\n",
            "|         127         |       0.32134       |       0.88545       |       0.37814       |       0.8647        |\n",
            "|         128         |       0.32069       |       0.88645       |       0.37333       |       0.8664        |\n",
            "|         129         |       0.31923       |       0.88625       |       0.37431       |        0.868        |\n",
            "|         130         |       0.31825       |       0.8859        |        0.375        |       0.8669        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         131         |       0.31582       |       0.88772       |       0.37636       |       0.8666        |\n",
            "|         132         |       0.31499       |       0.88817       |       0.37103       |       0.8681        |\n",
            "|         133         |       0.31425       |       0.88742       |       0.37296       |       0.8669        |\n",
            "|         134         |       0.3136        |       0.88795       |       0.3729        |       0.8656        |\n",
            "|         135         |       0.31428       |       0.88718       |       0.37317       |       0.8675        |\n",
            "|         136         |       0.31285       |       0.88907       |       0.37259       |       0.8674        |\n",
            "|         137         |       0.31202       |       0.88922       |       0.3724        |       0.8665        |\n",
            "|         138         |       0.30983       |       0.88993       |       0.37055       |       0.8682        |\n",
            "|         139         |       0.30834       |       0.89005       |       0.36982       |       0.8667        |\n",
            "|         140         |       0.30839       |       0.88972       |       0.3667        |       0.8698        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         141         |       0.30804       |       0.89033       |       0.36915       |       0.8687        |\n",
            "|         142         |       0.30678       |       0.89062       |       0.36916       |       0.8708        |\n",
            "|         143         |       0.30582       |       0.89117       |       0.36663       |       0.8695        |\n",
            "|         144         |       0.30481       |       0.8911        |       0.36558       |       0.8696        |\n",
            "|         145         |       0.30268       |       0.89182       |       0.36536       |       0.8729        |\n",
            "|         146         |       0.30277       |       0.89218       |       0.36614       |       0.8693        |\n",
            "|         147         |       0.30189       |       0.89227       |       0.36322       |       0.8718        |\n",
            "|         148         |       0.30157       |       0.89165       |       0.36519       |       0.8682        |\n",
            "|         149         |       0.30146       |       0.89148       |       0.36481       |       0.8693        |\n",
            "|         150         |       0.29988       |       0.89388       |       0.36526       |       0.8703        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         151         |       0.29837       |       0.8938        |       0.36487       |       0.8704        |\n",
            "|         152         |       0.29774       |       0.89422       |       0.36098       |       0.8713        |\n",
            "|         153         |       0.2968        |       0.89432       |        0.363        |       0.8707        |\n",
            "|         154         |       0.29613       |       0.89438       |       0.36189       |       0.8702        |\n",
            "|         155         |       0.2959        |       0.89368       |       0.36153       |       0.8694        |\n",
            "|         156         |       0.29503       |       0.89465       |       0.36293       |       0.8716        |\n",
            "|         157         |       0.29482       |       0.89573       |       0.36087       |       0.8703        |\n",
            "|         158         |       0.29402       |       0.89468       |       0.3583        |       0.8727        |\n",
            "|         159         |       0.29332       |       0.89533       |       0.35962       |       0.8696        |\n",
            "|         160         |       0.29303       |       0.89483       |       0.36147       |       0.8711        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         161         |       0.2946        |       0.89427       |       0.36075       |       0.8709        |\n",
            "|         162         |       0.29459       |       0.89437       |       0.3628        |        0.869        |\n",
            "|         163         |       0.29278       |       0.89485       |       0.35958       |       0.8729        |\n",
            "|         164         |       0.2891        |       0.89683       |       0.35943       |       0.8736        |\n",
            "|         165         |       0.28861       |       0.89632       |       0.35904       |       0.8712        |\n",
            "|         166         |       0.29046       |       0.89492       |       0.3603        |        0.871        |\n",
            "|         167         |       0.28809       |       0.89633       |       0.35595       |       0.8736        |\n",
            "|         168         |       0.28506       |       0.89788       |       0.35605       |       0.8733        |\n",
            "|         169         |       0.28593       |       0.89715       |       0.35828       |       0.8712        |\n",
            "|         170         |       0.28639       |       0.8962        |       0.35797       |       0.8709        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         171         |       0.28392       |       0.89798       |       0.35673       |       0.8709        |\n",
            "|         172         |       0.28252       |       0.89873       |       0.35298       |       0.8739        |\n",
            "|         173         |       0.28356       |       0.89778       |       0.35384       |       0.8763        |\n",
            "|         174         |       0.28264       |       0.89852       |       0.35283       |       0.8731        |\n",
            "|         175         |       0.28072       |       0.89947       |       0.35249       |       0.8721        |\n",
            "|         176         |       0.28083       |       0.89867       |       0.35128       |       0.8765        |\n",
            "|         177         |       0.28035       |       0.89908       |       0.35128       |        0.873        |\n",
            "|         178         |       0.27951       |       0.89983       |       0.35144       |       0.8758        |\n",
            "|         179         |       0.27776       |       0.9004        |       0.35102       |       0.8751        |\n",
            "|         180         |       0.2772        |       0.90052       |       0.35057       |       0.8752        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         181         |       0.27761       |       0.89978       |       0.35056       |       0.8746        |\n",
            "|         182         |       0.27634       |       0.90162       |       0.3478        |        0.874        |\n",
            "|         183         |       0.27655       |        0.901        |       0.3502        |       0.8757        |\n",
            "|         184         |       0.27498       |        0.901        |       0.35217       |       0.8739        |\n",
            "|         185         |       0.27477       |       0.90105       |       0.34924       |       0.8747        |\n",
            "|         186         |       0.27387       |       0.90157       |       0.34951       |       0.8743        |\n",
            "|         187         |       0.27394       |       0.90152       |       0.35174       |       0.8749        |\n",
            "|         188         |       0.27465       |       0.90162       |       0.35049       |       0.8748        |\n",
            "|         189         |       0.27406       |       0.9013        |       0.35288       |       0.8738        |\n",
            "|         190         |       0.27326       |       0.9017        |       0.35071       |       0.8725        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         191         |       0.27231       |       0.90287       |       0.34674       |       0.8752        |\n",
            "|         192         |       0.26999       |       0.90242       |       0.34793       |       0.8751        |\n",
            "|         193         |       0.26752       |       0.90423       |       0.34642       |       0.8748        |\n",
            "|         194         |       0.26816       |       0.90367       |       0.34692       |       0.8788        |\n",
            "|         195         |       0.26863       |       0.90335       |       0.34693       |       0.8752        |\n",
            "|         196         |       0.26902       |       0.90367       |       0.34791       |        0.877        |\n",
            "|         197         |       0.26641       |        0.905        |       0.34591       |       0.8753        |\n",
            "|         198         |       0.26503       |       0.90505       |       0.34614       |       0.8754        |\n",
            "|         199         |       0.26493       |       0.90437       |       0.34444       |       0.8757        |\n",
            "|         200         |       0.26414       |       0.90522       |       0.34389       |       0.8784        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         201         |       0.26397       |       0.90602       |       0.34191       |        0.877        |\n",
            "|         202         |       0.26352       |       0.90608       |       0.34332       |       0.8755        |\n",
            "|         203         |       0.2631        |       0.90607       |       0.34436       |       0.8778        |\n",
            "|         204         |       0.26173       |       0.90578       |       0.34137       |       0.8784        |\n",
            "|         205         |       0.26157       |       0.90587       |       0.34342       |       0.8767        |\n",
            "|         206         |       0.26077       |       0.90668       |       0.34207       |       0.8776        |\n",
            "|         207         |       0.26083       |       0.90693       |       0.34232       |       0.8778        |\n",
            "|         208         |       0.25983       |        0.907        |       0.34029       |       0.8791        |\n",
            "|         209         |       0.25875       |       0.90687       |       0.34148       |       0.8786        |\n",
            "|         210         |       0.25798       |       0.90693       |       0.34086       |       0.8794        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         211         |       0.25766       |       0.90682       |       0.33855       |        0.881        |\n",
            "|         212         |       0.2572        |       0.9076        |       0.3414        |       0.8794        |\n",
            "|         213         |       0.25639       |       0.90763       |       0.34054       |       0.8782        |\n",
            "|         214         |       0.25592       |       0.90838       |       0.34105       |       0.8785        |\n",
            "|         215         |       0.25594       |       0.90792       |       0.33929       |       0.8793        |\n",
            "|         216         |       0.25427       |       0.90915       |       0.33967       |       0.8777        |\n",
            "|         217         |       0.2536        |       0.90935       |       0.34058       |       0.8777        |\n",
            "|         218         |       0.25307       |       0.90957       |       0.33799       |       0.8783        |\n",
            "|         219         |       0.25325       |       0.90887       |       0.33696       |        0.878        |\n",
            "|         220         |       0.25233       |       0.9096        |       0.33897       |       0.8787        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         221         |       0.25221       |       0.90962       |       0.33802       |       0.8784        |\n",
            "|         222         |       0.25157       |       0.90928       |       0.33767       |       0.8783        |\n",
            "|         223         |       0.25088       |       0.91033       |       0.33816       |       0.8805        |\n",
            "|         224         |       0.25053       |       0.91045       |       0.33991       |       0.8805        |\n",
            "|         225         |       0.25122       |       0.91087       |       0.33917       |       0.8788        |\n",
            "|         226         |       0.25235       |       0.90923       |       0.33977       |       0.8783        |\n",
            "|         227         |       0.25656       |       0.90713       |       0.34856       |       0.8764        |\n",
            "|         228         |       0.25899       |       0.90655       |       0.34677       |       0.8761        |\n",
            "|         229         |       0.25816       |       0.90658       |       0.34939       |       0.8761        |\n",
            "|         230         |       0.2499        |       0.90998       |       0.34038       |       0.8774        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         231         |       0.24655       |       0.91185       |       0.33709       |       0.8805        |\n",
            "|         232         |       0.25116       |       0.90975       |       0.34257       |        0.878        |\n",
            "|         233         |       0.25085       |       0.90945       |       0.34115       |       0.8775        |\n",
            "|         234         |       0.24548       |       0.91175       |       0.33564       |       0.8801        |\n",
            "|         235         |       0.24401       |       0.9125        |       0.33537       |       0.8785        |\n",
            "|         236         |       0.24705       |       0.9104        |       0.33888       |       0.8788        |\n",
            "|         237         |       0.24617       |       0.91227       |       0.33851       |       0.8785        |\n",
            "|         238         |       0.24279       |       0.91302       |       0.33601       |       0.8798        |\n",
            "|         239         |       0.24357       |       0.9129        |       0.33545       |       0.8793        |\n",
            "|         240         |       0.24377       |       0.91313       |       0.33706       |       0.8799        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         241         |       0.24242       |       0.91347       |       0.33393       |       0.8791        |\n",
            "|         242         |       0.24063       |       0.9147        |       0.33299       |       0.8822        |\n",
            "|         243         |       0.24143       |       0.91323       |       0.33637       |       0.8807        |\n",
            "|         244         |       0.2407        |       0.91435       |       0.33744       |       0.8768        |\n",
            "|         245         |       0.23941       |       0.91467       |       0.33633       |       0.8801        |\n",
            "|         246         |       0.23876       |       0.91508       |       0.33292       |        0.881        |\n",
            "|         247         |       0.23927       |       0.91407       |       0.33285       |       0.8806        |\n",
            "|         248         |       0.23837       |       0.91485       |       0.3338        |        0.881        |\n",
            "|         249         |       0.23743       |       0.91548       |       0.33507       |       0.8807        |\n",
            "|         250         |       0.23683       |       0.9147        |       0.33512       |       0.8802        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         251         |       0.23688       |       0.91505       |       0.33417       |       0.8809        |\n",
            "|         252         |       0.23743       |       0.91462       |       0.33193       |       0.8796        |\n",
            "|         253         |       0.23514       |       0.91628       |       0.33391       |       0.8817        |\n",
            "|         254         |       0.23568       |       0.91633       |       0.33349       |       0.8801        |\n",
            "|         255         |       0.23501       |       0.91515       |       0.33312       |       0.8785        |\n",
            "|         256         |       0.23412       |        0.916        |       0.33351       |       0.8799        |\n",
            "|         257         |       0.23381       |       0.91593       |       0.33299       |       0.8786        |\n",
            "|         258         |       0.23339       |       0.91655       |       0.33173       |       0.8821        |\n",
            "|         259         |       0.23205       |       0.91632       |       0.3319        |       0.8806        |\n",
            "|         260         |       0.23184       |       0.91763       |       0.33441       |       0.8791        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         261         |       0.23207       |       0.91728       |       0.33144       |        0.881        |\n",
            "|         262         |       0.23131       |       0.91735       |       0.3335        |       0.8805        |\n",
            "|         263         |       0.23086       |       0.91712       |       0.33111       |       0.8812        |\n",
            "|         264         |       0.22944       |       0.91802       |       0.33311       |       0.8808        |\n",
            "|         265         |       0.23046       |       0.91742       |       0.33237       |       0.8803        |\n",
            "|         266         |        0.23         |       0.91862       |       0.33238       |       0.8809        |\n",
            "|         267         |       0.22862       |       0.91815       |       0.3297        |       0.8809        |\n",
            "|         268         |       0.22824       |       0.91838       |       0.3314        |       0.8815        |\n",
            "|         269         |       0.22661       |       0.91927       |       0.33291       |       0.8826        |\n",
            "|         270         |       0.2276        |       0.91907       |       0.33049       |       0.8798        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         271         |       0.22707       |       0.91967       |       0.33401       |       0.8815        |\n",
            "|         272         |       0.22642       |       0.91978       |       0.33023       |       0.8826        |\n",
            "|         273         |       0.22598       |       0.91905       |       0.32782       |       0.8834        |\n",
            "|         274         |       0.22512       |       0.91988       |       0.33238       |       0.8817        |\n",
            "|         275         |       0.22456       |       0.9196        |       0.33239       |        0.882        |\n",
            "|         276         |       0.22472       |       0.91985       |       0.32974       |       0.8832        |\n",
            "|         277         |       0.22415       |       0.91962       |       0.32905       |       0.8811        |\n",
            "|         278         |       0.22371       |       0.91967       |       0.33191       |       0.8823        |\n",
            "|         279         |       0.22362       |       0.92003       |       0.33123       |       0.8803        |\n",
            "|         280         |       0.22335       |       0.9206        |       0.33193       |       0.8817        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         281         |       0.22235       |       0.9206        |       0.33319       |       0.8819        |\n",
            "|         282         |       0.22328       |       0.92093       |       0.32938       |       0.8836        |\n",
            "|         283         |       0.22296       |       0.92033       |       0.33319       |       0.8809        |\n",
            "|         284         |       0.22397       |       0.9199        |       0.33142       |        0.882        |\n",
            "|         285         |       0.2238        |       0.91915       |       0.33431       |       0.8807        |\n",
            "|         286         |       0.22306       |       0.9201        |       0.33266       |       0.8826        |\n",
            "|         287         |       0.22094       |       0.92182       |       0.33419       |       0.8811        |\n",
            "|         288         |       0.21977       |       0.92178       |       0.32917       |       0.8823        |\n",
            "|         289         |       0.21889       |       0.92127       |       0.32882       |       0.8825        |\n",
            "|         290         |       0.21934       |       0.92197       |       0.33002       |       0.8823        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         291         |       0.2194        |       0.9208        |       0.32931       |       0.8814        |\n",
            "|         292         |       0.21897       |       0.92235       |       0.32899       |       0.8815        |\n",
            "|         293         |       0.21779       |       0.92197       |       0.32876       |        0.883        |\n",
            "|         294         |       0.21649       |       0.92315       |       0.32868       |        0.882        |\n",
            "|         295         |       0.21597       |       0.92325       |       0.33059       |       0.8813        |\n",
            "|         296         |       0.21606       |       0.92335       |       0.32915       |       0.8834        |\n",
            "|         297         |       0.2163        |       0.92278       |       0.33058       |       0.8824        |\n",
            "|         298         |       0.21632       |       0.92227       |       0.33056       |       0.8818        |\n",
            "|         299         |       0.2157        |       0.92292       |       0.33265       |       0.8806        |\n",
            "|         300         |       0.21451       |       0.92385       |       0.32994       |       0.8833        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         301         |       0.21334       |       0.92415       |       0.32805       |       0.8835        |\n",
            "|         302         |       0.21355       |       0.92372       |       0.32819       |       0.8843        |\n",
            "|         303         |       0.21423       |       0.9239        |       0.33123       |       0.8818        |\n",
            "|         304         |       0.21358       |       0.9246        |       0.33188       |       0.8821        |\n",
            "|         305         |       0.2134        |       0.9239        |       0.33099       |       0.8818        |\n",
            "|         306         |       0.21179       |       0.92555       |       0.33294       |        0.882        |\n",
            "|         307         |       0.21267       |       0.92428       |       0.32971       |       0.8834        |\n",
            "|         308         |       0.21315       |       0.92377       |       0.32995       |       0.8832        |\n",
            "|         309         |       0.21291       |       0.92353       |       0.33154       |       0.8808        |\n",
            "|         310         |       0.21408       |       0.9232        |       0.33082       |       0.8829        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         311         |       0.21317       |       0.9236        |       0.33536       |       0.8809        |\n",
            "|         312         |       0.21116       |       0.92452       |       0.33264       |       0.8822        |\n",
            "|         313         |       0.20911       |       0.92553       |       0.32761       |       0.8851        |\n",
            "|         314         |       0.20856       |       0.92628       |       0.33007       |       0.8839        |\n",
            "|         315         |       0.20962       |        0.925        |       0.32701       |       0.8822        |\n",
            "|         316         |       0.20909       |       0.9254        |       0.33136       |        0.881        |\n",
            "|         317         |       0.20891       |       0.92535       |       0.33068       |       0.8808        |\n",
            "|         318         |       0.20715       |       0.9269        |       0.33011       |       0.8835        |\n",
            "|         319         |       0.20663       |       0.92637       |       0.32884       |       0.8844        |\n",
            "|         320         |       0.20611       |       0.92607       |       0.32812       |       0.8822        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         321         |       0.20633       |       0.92648       |       0.32914       |       0.8842        |\n",
            "|         322         |       0.20505       |       0.92615       |       0.3304        |       0.8832        |\n",
            "|         323         |       0.20496       |       0.9274        |       0.32949       |        0.884        |\n",
            "|         324         |       0.20464       |       0.92723       |       0.3315        |       0.8834        |\n",
            "|         325         |       0.20436       |       0.92778       |       0.32992       |       0.8847        |\n",
            "|         326         |       0.20445       |       0.92708       |       0.32816       |        0.883        |\n",
            "|         327         |       0.20288       |       0.92745       |       0.32957       |       0.8818        |\n",
            "|         328         |       0.2035        |       0.92762       |       0.3291        |       0.8812        |\n",
            "|         329         |       0.20195       |       0.92782       |       0.33002       |       0.8828        |\n",
            "|         330         |       0.20208       |       0.9286        |       0.32757       |       0.8838        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         331         |       0.20122       |       0.92942       |       0.32942       |       0.8826        |\n",
            "|         332         |       0.20148       |       0.92835       |       0.32952       |       0.8832        |\n",
            "|         333         |       0.20131       |       0.92772       |       0.33035       |       0.8841        |\n",
            "|         334         |       0.20108       |       0.92823       |       0.32785       |        0.884        |\n",
            "|         335         |       0.19893       |       0.92958       |       0.32818       |       0.8827        |\n",
            "|         336         |       0.19962       |       0.92857       |       0.32884       |       0.8839        |\n",
            "|         337         |       0.20036       |       0.92845       |       0.32891       |        0.883        |\n",
            "|         338         |       0.19938       |       0.92867       |       0.32903       |       0.8842        |\n",
            "|         339         |       0.20099       |       0.92865       |       0.33358       |       0.8818        |\n",
            "|         340         |       0.20287       |       0.9277        |       0.33448       |       0.8829        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         341         |       0.20897       |       0.92503       |       0.34052       |       0.8808        |\n",
            "|         342         |       0.21249       |       0.92172       |       0.34328       |       0.8789        |\n",
            "|         343         |       0.21569       |       0.92117       |       0.35137       |       0.8778        |\n",
            "|         344         |       0.20633       |       0.92478       |       0.33822       |       0.8808        |\n",
            "|         345         |       0.19814       |       0.92953       |       0.33191       |       0.8857        |\n",
            "|         346         |       0.20206       |       0.92768       |       0.33365       |       0.8842        |\n",
            "|         347         |       0.20578       |       0.92522       |       0.34139       |       0.8792        |\n",
            "|         348         |       0.19883       |       0.92955       |       0.33103       |       0.8817        |\n",
            "|         349         |       0.19577       |       0.93013       |       0.32924       |       0.8833        |\n",
            "|         350         |       0.20115       |       0.92732       |       0.33268       |        0.881        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         351         |       0.20127       |       0.9281        |       0.33682       |       0.8821        |\n",
            "|         352         |       0.19472       |       0.93045       |       0.3293        |       0.8831        |\n",
            "|         353         |       0.19557       |       0.92963       |       0.3308        |       0.8845        |\n",
            "|         354         |       0.19953       |       0.92845       |       0.33651       |       0.8852        |\n",
            "|         355         |       0.19504       |       0.93018       |       0.33152       |       0.8841        |\n",
            "|         356         |       0.1924        |       0.93105       |       0.32845       |       0.8849        |\n",
            "|         357         |       0.19466       |       0.93103       |       0.33309       |       0.8841        |\n",
            "|         358         |       0.19546       |       0.92993       |       0.33012       |       0.8848        |\n",
            "|         359         |       0.19114       |       0.93192       |       0.33113       |       0.8841        |\n",
            "|         360         |       0.19148       |       0.93233       |       0.32976       |       0.8847        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         361         |       0.19322       |       0.93013       |       0.33237       |       0.8819        |\n",
            "|         362         |       0.19204       |       0.93237       |       0.33072       |       0.8839        |\n",
            "|         363         |       0.18981       |       0.93222       |       0.32857       |       0.8857        |\n",
            "|         364         |       0.19064       |       0.93163       |       0.32995       |       0.8846        |\n",
            "|         365         |       0.19032       |       0.93242       |       0.33128       |        0.884        |\n",
            "|         366         |       0.19002       |       0.93232       |       0.33031       |       0.8849        |\n",
            "|         367         |       0.18859       |       0.93327       |       0.32768       |       0.8849        |\n",
            "|         368         |       0.18896       |       0.93345       |       0.32843       |       0.8839        |\n",
            "|         369         |       0.18912       |       0.9328        |       0.33146       |       0.8836        |\n",
            "|         370         |       0.18778       |       0.93293       |       0.33006       |       0.8855        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         371         |       0.18691       |       0.93427       |       0.33063       |       0.8849        |\n",
            "|         372         |       0.18756       |       0.93388       |       0.33008       |       0.8841        |\n",
            "|         373         |       0.18736       |       0.93422       |       0.33203       |       0.8847        |\n",
            "|         374         |       0.18614       |       0.93395       |       0.32937       |       0.8861        |\n",
            "|         375         |       0.18585       |       0.9342        |       0.32822       |       0.8835        |\n",
            "|         376         |       0.18539       |       0.9347        |       0.33004       |       0.8843        |\n",
            "|         377         |       0.1858        |       0.9341        |       0.32998       |       0.8841        |\n",
            "|         378         |       0.18518       |       0.93462       |       0.33275       |       0.8835        |\n",
            "|         379         |       0.18456       |       0.93493       |       0.32971       |       0.8832        |\n",
            "|         380         |       0.18469       |       0.93492       |       0.32954       |       0.8857        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         381         |       0.18397       |       0.93575       |       0.33172       |       0.8828        |\n",
            "|         382         |       0.18394       |       0.93493       |       0.32745       |       0.8834        |\n",
            "|         383         |       0.18341       |       0.93493       |       0.32976       |       0.8838        |\n",
            "|         384         |       0.18236       |       0.93497       |       0.33202       |       0.8849        |\n",
            "|         385         |       0.18282       |       0.9347        |       0.33016       |       0.8845        |\n",
            "|         386         |        0.182        |       0.93563       |       0.3329        |       0.8863        |\n",
            "|         387         |       0.18144       |       0.93583       |       0.3321        |       0.8856        |\n",
            "|         388         |       0.18055       |       0.93612       |       0.32816       |       0.8865        |\n",
            "|         389         |       0.18108       |       0.93602       |       0.32933       |       0.8845        |\n",
            "|         390         |       0.18086       |       0.93545       |       0.3307        |       0.8843        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         391         |       0.18018       |       0.93607       |       0.33058       |       0.8848        |\n",
            "|         392         |       0.1799        |       0.93653       |       0.32853       |       0.8854        |\n",
            "|         393         |       0.17969       |       0.93683       |       0.33016       |       0.8844        |\n",
            "|         394         |       0.17869       |       0.93715       |       0.33191       |       0.8848        |\n",
            "|         395         |       0.17928       |       0.93647       |       0.32883       |       0.8857        |\n",
            "|         396         |       0.17936       |       0.93698       |       0.32927       |       0.8852        |\n",
            "|         397         |       0.17911       |       0.93687       |       0.3307        |       0.8856        |\n",
            "|         398         |       0.17904       |       0.9366        |       0.3316        |       0.8845        |\n",
            "|         399         |       0.17934       |       0.93617       |       0.33526       |       0.8833        |\n",
            "|         400         |       0.17905       |       0.93635       |       0.33322       |       0.8838        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         401         |       0.17963       |       0.93778       |       0.33624       |       0.8839        |\n",
            "|         402         |       0.17917       |       0.93657       |       0.33458       |       0.8849        |\n",
            "|         403         |       0.17748       |       0.93732       |       0.33243       |       0.8853        |\n",
            "|         404         |       0.17799       |       0.93722       |       0.33263       |       0.8858        |\n",
            "|         405         |       0.17616       |       0.93762       |       0.33303       |        0.885        |\n",
            "|         406         |       0.1754        |       0.93833       |       0.33153       |       0.8868        |\n",
            "|         407         |       0.17601       |       0.93823       |       0.33228       |       0.8863        |\n",
            "|         408         |       0.17489       |       0.93808       |       0.33086       |       0.8861        |\n",
            "|         409         |       0.1752        |       0.9379        |       0.33181       |       0.8849        |\n",
            "|         410         |       0.17478       |       0.93853       |       0.33306       |        0.884        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         411         |       0.17379       |       0.93873       |       0.33267       |        0.885        |\n",
            "|         412         |       0.17356       |        0.939        |       0.3325        |       0.8864        |\n",
            "|         413         |       0.17343       |       0.9383        |       0.33105       |       0.8863        |\n",
            "|         414         |       0.17342       |       0.93943       |       0.33313       |       0.8868        |\n",
            "|         415         |       0.1735        |       0.93857       |       0.3331        |        0.884        |\n",
            "|         416         |       0.17227       |       0.93895       |       0.33362       |       0.8858        |\n",
            "|         417         |       0.17269       |       0.93945       |       0.32987       |       0.8857        |\n",
            "|         418         |       0.17079       |       0.93997       |       0.3315        |       0.8853        |\n",
            "|         419         |       0.17101       |       0.94008       |       0.33332       |       0.8867        |\n",
            "|         420         |       0.17121       |       0.9393        |       0.33201       |        0.885        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         421         |       0.17065       |       0.9402        |       0.33286       |       0.8848        |\n",
            "|         422         |       0.17036       |       0.93998       |       0.33114       |       0.8881        |\n",
            "|         423         |       0.17023       |       0.93902       |       0.33226       |       0.8841        |\n",
            "|         424         |       0.17006       |       0.93923       |       0.3352        |       0.8866        |\n",
            "|         425         |       0.16972       |       0.9402        |       0.33492       |       0.8837        |\n",
            "|         426         |       0.17083       |       0.9402        |       0.33432       |       0.8869        |\n",
            "|         427         |       0.17052       |       0.94012       |       0.33383       |        0.886        |\n",
            "|         428         |       0.17154       |       0.93978       |       0.33784       |       0.8849        |\n",
            "|         429         |       0.17165       |       0.9392        |       0.33717       |       0.8863        |\n",
            "|         430         |       0.17288       |       0.9388        |       0.33658       |       0.8857        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         431         |       0.17102       |       0.93985       |       0.33766       |       0.8861        |\n",
            "|         432         |       0.17073       |       0.93965       |       0.33683       |       0.8857        |\n",
            "|         433         |       0.16917       |       0.93953       |       0.33956       |       0.8848        |\n",
            "|         434         |       0.16918       |       0.9396        |       0.33459       |       0.8863        |\n",
            "|         435         |       0.16951       |       0.94093       |       0.33721       |       0.8861        |\n",
            "|         436         |       0.16904       |       0.9401        |       0.33595       |       0.8854        |\n",
            "|         437         |       0.16923       |       0.94018       |       0.33967       |       0.8851        |\n",
            "|         438         |       0.16708       |       0.94137       |       0.33564       |       0.8863        |\n",
            "|         439         |       0.16808       |       0.94038       |       0.33681       |       0.8873        |\n",
            "|         440         |       0.16583       |       0.94138       |       0.3349        |       0.8843        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         441         |       0.16443       |       0.94197       |       0.33486       |       0.8867        |\n",
            "|         442         |       0.16575       |       0.94198       |       0.33878       |       0.8857        |\n",
            "|         443         |       0.16468       |       0.94172       |       0.33661       |       0.8834        |\n",
            "|         444         |       0.16487       |       0.94185       |       0.33977       |       0.8848        |\n",
            "|         445         |       0.1646        |       0.9423        |       0.33536       |       0.8863        |\n",
            "|         446         |       0.16317       |       0.94343       |       0.33861       |       0.8854        |\n",
            "|         447         |       0.16278       |       0.94318       |       0.33746       |       0.8865        |\n",
            "|         448         |       0.16236       |       0.94328       |       0.33464       |       0.8879        |\n",
            "|         449         |       0.16203       |       0.94372       |       0.33734       |       0.8848        |\n",
            "|         450         |       0.16282       |       0.94262       |       0.33572       |       0.8862        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         451         |       0.1615        |       0.94332       |       0.33669       |       0.8853        |\n",
            "|         452         |       0.16292       |       0.94307       |       0.33849       |       0.8849        |\n",
            "|         453         |       0.16177       |       0.94383       |       0.33838       |       0.8876        |\n",
            "|         454         |       0.16058       |       0.94412       |       0.33586       |       0.8851        |\n",
            "|         455         |       0.15977       |       0.94417       |       0.33679       |       0.8862        |\n",
            "|         456         |       0.15931       |       0.94453       |       0.3359        |       0.8869        |\n",
            "|         457         |       0.15957       |       0.94422       |       0.33447       |       0.8867        |\n",
            "|         458         |       0.15813       |       0.9449        |       0.33727       |       0.8866        |\n",
            "|         459         |       0.1588        |       0.94458       |       0.33581       |       0.8863        |\n",
            "|         460         |       0.15864       |       0.94497       |       0.33745       |       0.8867        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         461         |       0.15779       |       0.94498       |       0.33577       |       0.8873        |\n",
            "|         462         |       0.15803       |       0.94413       |       0.33541       |       0.8867        |\n",
            "|         463         |       0.15756       |       0.94542       |       0.33695       |       0.8856        |\n",
            "|         464         |       0.15781       |       0.94525       |       0.33902       |       0.8873        |\n",
            "|         465         |       0.15704       |       0.94553       |       0.33598       |       0.8879        |\n",
            "|         466         |       0.15765       |       0.94465       |       0.34259       |       0.8864        |\n",
            "|         467         |       0.15647       |       0.94522       |       0.33753       |       0.8852        |\n",
            "|         468         |       0.15599       |       0.94563       |       0.33709       |       0.8859        |\n",
            "|         469         |       0.15537       |       0.94572       |       0.33796       |       0.8847        |\n",
            "|         470         |       0.15513       |       0.94672       |       0.3363        |       0.8902        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         471         |       0.15488       |       0.94615       |       0.34085       |       0.8868        |\n",
            "|         472         |       0.15559       |       0.94577       |       0.33828       |       0.8864        |\n",
            "|         473         |       0.15496       |       0.94658       |       0.33879       |       0.8857        |\n",
            "|         474         |       0.15462       |        0.946        |       0.3385        |       0.8858        |\n",
            "|         475         |       0.15493       |       0.94712       |       0.33897       |       0.8857        |\n",
            "|         476         |       0.15747       |       0.9444        |       0.34302       |       0.8865        |\n",
            "|         477         |       0.15855       |       0.94462       |       0.34408       |       0.8855        |\n",
            "|         478         |       0.16045       |       0.94352       |        0.344        |       0.8862        |\n",
            "|         479         |       0.15857       |       0.94457       |       0.34505       |       0.8852        |\n",
            "|         480         |       0.15697       |       0.94472       |       0.34588       |       0.8863        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         481         |       0.15436       |       0.9463        |       0.34138       |       0.8851        |\n",
            "|         482         |       0.1551        |       0.94517       |       0.34294       |       0.8856        |\n",
            "|         483         |       0.15723       |       0.94465       |       0.34457       |       0.8833        |\n",
            "|         484         |       0.15935       |       0.94332       |       0.3491        |       0.8831        |\n",
            "|         485         |       0.15775       |       0.94448       |       0.34654       |       0.8838        |\n",
            "|         486         |       0.15421       |       0.94578       |       0.3469        |       0.8851        |\n",
            "|         487         |       0.15184       |       0.9477        |       0.34154       |       0.8874        |\n",
            "|         488         |       0.15283       |       0.94638       |       0.34085       |       0.8868        |\n",
            "|         489         |       0.15481       |       0.94558       |       0.34421       |        0.886        |\n",
            "|         490         |       0.15261       |        0.946        |       0.3408        |       0.8852        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         491         |       0.1505        |       0.94848       |       0.34525       |       0.8847        |\n",
            "|         492         |       0.15069       |       0.94793       |        0.342        |        0.887        |\n",
            "|         493         |       0.15261       |       0.94683       |       0.34399       |       0.8858        |\n",
            "|         494         |       0.15423       |       0.94643       |       0.34581       |       0.8858        |\n",
            "|         495         |       0.15271       |       0.94662       |       0.3482        |       0.8849        |\n",
            "|         496         |       0.14973       |       0.9485        |       0.34452       |       0.8864        |\n",
            "|         497         |       0.14889       |       0.94832       |       0.34007       |       0.8889        |\n",
            "|         498         |       0.15059       |       0.94703       |       0.3451        |       0.8855        |\n",
            "|         499         |       0.15048       |       0.9481        |       0.34663       |       0.8871        |\n",
            "|         500         |       0.14792       |        0.949        |       0.34299       |       0.8864        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL1CZRN_pUQ-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LKrBp6LYgNy"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}