{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_A1_Question1_1_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "metadata": {
      "interpreter": {
        "hash": "de1b2a626baae37f500cb083f8858eb0d1eb85e0128ef62fa24f59782d154f26"
      }
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhEu3ph3mmnZ",
        "outputId": "33b44f7f-f0d0-415a-def1-7065b45813b9"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuM6HeNym-fG"
      },
      "source": [
        "# import required libraries\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Setting Random Seed\n",
        "tf.random.set_seed(195470)\n",
        "\n",
        "# load and prepare the training and test data\n",
        "def load_Fashion_MNIST():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "    # load the training and test data    \n",
        "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
        "\n",
        "    # reshape the feature data\n",
        "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
        "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
        "\n",
        "    # noramlise feature data\n",
        "    tr_x = tr_x / 255.0\n",
        "    te_x = te_x / 255.0\n",
        "\n",
        "    print( \"Shape of training features \", tr_x.shape)\n",
        "    print( \"Shape of test features \", te_x.shape)\n",
        "\n",
        "\n",
        "    # one hot encode the training labels and get the transpose\n",
        "    tr_y = np_utils.to_categorical(tr_y,10)\n",
        "    tr_y = tr_y.T\n",
        "    print (\"Shape of training labels \", tr_y.shape)\n",
        "\n",
        "    # one hot encode the test labels and get the transpose\n",
        "    te_y = np_utils.to_categorical(te_y,10)\n",
        "    te_y = te_y.T\n",
        "    print (\"Shape of testing labels \", te_y.shape)\n",
        "    \n",
        "    # Reshape the training data and test data so \n",
        "    # that the features becomes the rows of the matrix\n",
        "    tr_x = tr_x.T\n",
        "    te_x = te_x.T\n",
        "\n",
        "    print(\"Reshaped training data \", tr_x.shape)\n",
        "    print(\"Reshaped test data \",te_x.shape)\n",
        "    \n",
        "    return tr_x, tr_y, te_x, te_y"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzqqwhfpoDzN"
      },
      "source": [
        "#  push a matrix of feature data through the neural network and the final Softmax layer\n",
        "def forward_pass(x, w_1, b1, w_2, b2, w_3, b3):\n",
        "\n",
        "    # First Layer\n",
        "    A1 = tf.matmul(w_1, x) + b1 #WX + b\n",
        "    H1 =  tf.keras.activations.relu(A1) #ReLu activation\n",
        "    \n",
        "    # Second Layer\n",
        "    A2 = tf.matmul(w_2, H1) + b2 #WX + b\n",
        "    H2 =  tf.keras.activations.relu(A2) #ReLu activation\n",
        "\n",
        "    # Third Layer / Output Layer\n",
        "    A3 = tf.matmul(w_3, H2) + b3 #WX + b\n",
        "    #SoftMax activation\n",
        "    e_A3 = tf.math.exp(A3) #e^A2\n",
        "    return tf.divide(e_A3,tf.reshape(tf.reduce_sum(e_A3,0),[1, -1])) #e^A3/sum(e^A3)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BtDUop-oU_Z"
      },
      "source": [
        "\n",
        "def cross_entropy(y, y_pred):\n",
        "    # mean of cross entropy loss\n",
        "    return tf.reduce_mean(-(tf.reduce_sum(tf.multiply(y, tf.math.log(y_pred)), 0))) #mean(-sum(y*log(h(x)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My675H4XoWfv"
      },
      "source": [
        "\n",
        "def calculate_accuracy(y, y_pred_softmax):\n",
        "    #Compare every predicted value to original value individually which will result in boolean array which will be casted to zeros and ones\n",
        "    #Taking the mean of this array will give us the accuracy since 1 is correct and 0 is wrong and the mean is accuracy!\n",
        "    #Example tf.Tensor([True False False ... False False False], shape=(60000,), dtype=bool) casted to [1. 0. 0. ... 0. 0. 0.], shape=(60000,), dtype=float32)\n",
        "    #and the Mean gives 0.10131667 which is the accuracy\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_pred_softmax, 0), tf.argmax(y, 0)), tf.float32))  \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYThBBUr9Yf8"
      },
      "source": [
        "#Function to type cast to float32 by default\n",
        "def fl_type_caster(x,t=tf.float32):\n",
        "  return tf.cast(x,t)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n5-bCvp3oYNd",
        "tags": [],
        "outputId": "1606f033-508a-49d9-d6a8-cc6b1f135233"
      },
      "source": [
        "epochs = 500\n",
        "training_accuracy, training_loss, validation_accuracy, validation_loss = [],[],[],[]\n",
        "\n",
        "# load the prepared data and typecast the training and test data\n",
        "tr_x, tr_y, te_x, te_y = [fl_type_caster(i) for i in load_Fashion_MNIST()]\n",
        "\n",
        "#Initializing the weights and biases for 1st layer 300N\n",
        "w1 = tf.Variable(tf.random.normal([300, tr_x.shape[0]], mean=0.0, stddev=0.1))\n",
        "b1 = tf.Variable(tf.zeros([300, 1]))\n",
        "\n",
        "#Initializing the weights and biases for 2nd layer 100N\n",
        "w2 = tf.Variable(tf.random.normal([100, 300], mean=0.0, stddev=0.1))\n",
        "b2 = tf.Variable(tf.zeros([100, 1]))\n",
        "\n",
        "#Initialize the weights and biases for the SoftMax ie Output layer 10N\n",
        "w3 = tf.Variable(tf.random.normal([ tr_y.shape[0], 100], mean=0.0, stddev=0.1))\n",
        "b3 = tf.Variable(tf.zeros([tr_y.shape[0], 1]))\n",
        "\n",
        "#Instantiate Optimizer\n",
        "adam_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training loop\n",
        "for i in range(epochs):\n",
        "    if i%10 == 0:\n",
        "      print(\"+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+\".format(\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19))\n",
        "      print(\"| {:^19} | {:^19} | {:^19} | {:^19} | {:^19} |\".format(\"Epoch\",\"Training_loss\",\"Training_accuracy\",\"Validation_loss\",\"Validation_accuracy\"))\n",
        "      print(\"+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+\".format(\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19))\n",
        "\n",
        "    # Instantiating Gradient Tape to monitor the forward pass & calculate gradients ie autodiff\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = forward_pass(tr_x, w1, b1, w2, b2, w3, b3)\n",
        "        loss = cross_entropy(tr_y, y_pred)\n",
        "    training_loss.append(loss)\n",
        "    \n",
        "    # Prediction Accuracy\n",
        "    accuracy = calculate_accuracy(tr_y, y_pred)\n",
        "    training_accuracy.append(accuracy)\n",
        "\n",
        "    # Calculate gradients\n",
        "    gradients = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
        "\n",
        "    # Forward propagate and calculate accuracy and loss for the valdation data\n",
        "    te_y_pred = forward_pass(te_x, w1, b1, w2, b2, w3, b3) \n",
        "    val_loss = cross_entropy(te_y, te_y_pred)\n",
        "    val_accuracy = calculate_accuracy(te_y, te_y_pred) \n",
        "    validation_accuracy.append(val_accuracy)\n",
        "    validation_loss.append(val_loss)\n",
        "\n",
        "    print(\"| {:^19} | {:^19} | {:^19} | {:^19} | {:^19} |\".format(i+1,round(float(loss.numpy()),5),round(float(accuracy.numpy()),5),round(float(val_loss.numpy()),5),round(float(val_accuracy.numpy()),5)))\n",
        "    \n",
        "    # Adam Optimizer to update the weights and biases accordingly\n",
        "    adam_optimizer.apply_gradients(zip(gradients, [w1, b1, w2, b2, w3, b3]))\n",
        "\n",
        "print(\"+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+-{:^19}-+\".format(\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19,\"-\"*19))\n",
        "\n",
        "# Plot the training and the validation accuracy and loss\n",
        "plt.plot(training_accuracy, label=\"Train Accuracy\",color='magenta')\n",
        "plt.plot(training_loss, label=\"Train Loss\",color='orange')\n",
        "plt.plot(validation_accuracy, label=\"Validation Accuracy\",color='green')\n",
        "plt.plot(validation_loss, label=\"Validation Loss\",color='red')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training features  (60000, 784)\n",
            "Shape of test features  (10000, 784)\n",
            "Shape of training labels  (10, 60000)\n",
            "Shape of testing labels  (10, 10000)\n",
            "Reshaped training data  (784, 60000)\n",
            "Reshaped test data  (784, 10000)\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|          1          |       2.7233        |       0.09118       |       2.72307       |       0.0881        |\n",
            "|          2          |       2.19345       |       0.14528       |       2.19503       |       0.1423        |\n",
            "|          3          |       1.85381       |       0.33848       |       1.85862       |       0.3302        |\n",
            "|          4          |       1.62185       |       0.46782       |       1.63002       |       0.4564        |\n",
            "|          5          |       1.44219       |       0.54233       |       1.45189       |       0.5374        |\n",
            "|          6          |       1.2797        |       0.61358       |       1.29034       |       0.6046        |\n",
            "|          7          |       1.14744       |       0.65618       |       1.15979       |       0.6444        |\n",
            "|          8          |       1.05485       |       0.67205       |       1.06943       |       0.6614        |\n",
            "|          9          |       0.98225       |       0.68207       |       0.9986        |       0.6692        |\n",
            "|         10          |       0.92171       |       0.69008       |       0.9391        |       0.6763        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         11          |       0.87711       |       0.69347       |       0.89537       |       0.6808        |\n",
            "|         12          |       0.84011       |       0.70175       |       0.85905       |        0.687        |\n",
            "|         13          |       0.80525       |       0.7158        |       0.82538       |       0.6977        |\n",
            "|         14          |       0.77439       |       0.72932       |       0.79626       |        0.713        |\n",
            "|         15          |       0.74797       |       0.74178       |       0.77165       |       0.7257        |\n",
            "|         16          |       0.72844       |       0.75008       |       0.75341       |       0.7357        |\n",
            "|         17          |       0.71154       |       0.75575       |       0.73667       |       0.7421        |\n",
            "|         18          |       0.69208       |       0.76303       |       0.71681       |       0.7505        |\n",
            "|         19          |       0.67526       |       0.76833       |       0.69954       |        0.757        |\n",
            "|         20          |       0.6617        |       0.77155       |       0.68547       |       0.7602        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         21          |       0.64875       |       0.77533       |       0.67189       |       0.7635        |\n",
            "|         22          |       0.63539       |       0.77993       |       0.65843       |       0.7686        |\n",
            "|         23          |       0.62118       |       0.78602       |       0.64504       |       0.7739        |\n",
            "|         24          |       0.60962       |       0.79073       |       0.63452       |        0.779        |\n",
            "|         25          |       0.59789       |       0.79483       |       0.62302       |       0.7852        |\n",
            "|         26          |       0.58713       |       0.79845       |       0.61214       |       0.7864        |\n",
            "|         27          |       0.57646       |       0.80215       |       0.60197       |       0.7901        |\n",
            "|         28          |       0.56731       |       0.80533       |       0.59371       |       0.7917        |\n",
            "|         29          |       0.55828       |       0.80878       |       0.58499       |       0.7973        |\n",
            "|         30          |       0.54996       |       0.81242       |       0.57697       |       0.8001        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         31          |       0.54237       |       0.81627       |       0.57029       |       0.8039        |\n",
            "|         32          |       0.53521       |       0.81812       |       0.56378       |       0.8054        |\n",
            "|         33          |       0.52799       |       0.82045       |       0.55648       |       0.8067        |\n",
            "|         34          |       0.52193       |       0.8225        |       0.55026       |       0.8094        |\n",
            "|         35          |       0.51584       |       0.82397       |       0.54411       |       0.8124        |\n",
            "|         36          |       0.51001       |       0.82627       |       0.53821       |       0.8151        |\n",
            "|         37          |       0.50468       |       0.82927       |       0.53286       |       0.8188        |\n",
            "|         38          |       0.49985       |       0.83068       |       0.52806       |       0.8203        |\n",
            "|         39          |       0.49477       |       0.83185       |       0.52324       |       0.8204        |\n",
            "|         40          |       0.49002       |       0.83343       |       0.51873       |        0.82         |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         41          |       0.48588       |       0.8346        |       0.51463       |       0.8217        |\n",
            "|         42          |       0.48134       |       0.83665       |       0.51049       |       0.8232        |\n",
            "|         43          |       0.47716       |       0.83792       |       0.50648       |       0.8254        |\n",
            "|         44          |       0.47343       |       0.83888       |       0.50286       |       0.8281        |\n",
            "|         45          |       0.46955       |       0.8399        |       0.49953       |       0.8279        |\n",
            "|         46          |       0.46601       |       0.84077       |       0.4963        |       0.8287        |\n",
            "|         47          |       0.46256       |       0.84177       |       0.49312       |       0.8298        |\n",
            "|         48          |       0.45902       |       0.84258       |       0.48985       |       0.8316        |\n",
            "|         49          |       0.4558        |       0.84407       |       0.48679       |       0.8321        |\n",
            "|         50          |       0.45258       |       0.84497       |       0.4839        |       0.8339        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         51          |       0.44943       |       0.84552       |       0.48097       |       0.8343        |\n",
            "|         52          |       0.4466        |       0.84617       |       0.47847       |       0.8333        |\n",
            "|         53          |       0.44373       |       0.84705       |       0.47576       |       0.8351        |\n",
            "|         54          |       0.44089       |       0.84812       |       0.47307       |       0.8362        |\n",
            "|         55          |       0.4383        |       0.84913       |       0.47073       |       0.8376        |\n",
            "|         56          |       0.43562       |       0.84982       |       0.46828       |       0.8384        |\n",
            "|         57          |       0.43304       |       0.85055       |       0.46624       |        0.838        |\n",
            "|         58          |       0.43069       |       0.8511        |       0.46403       |       0.8389        |\n",
            "|         59          |       0.42828       |       0.85178       |       0.46227       |        0.839        |\n",
            "|         60          |       0.42617       |       0.8526        |       0.4598        |       0.8408        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         61          |       0.42453       |       0.85298       |       0.45917       |       0.8402        |\n",
            "|         62          |       0.42333       |       0.85333       |       0.45703       |       0.8404        |\n",
            "|         63          |       0.42097       |       0.8537        |       0.45621       |       0.8391        |\n",
            "|         64          |       0.41786       |       0.85523       |       0.45217       |       0.8419        |\n",
            "|         65          |       0.4146        |       0.85593       |       0.44965       |       0.8426        |\n",
            "|         66          |       0.41302       |       0.8567        |       0.44847       |       0.8425        |\n",
            "|         67          |       0.41206       |       0.85725       |       0.44693       |       0.8434        |\n",
            "|         68          |       0.40963       |       0.85748       |       0.44577       |       0.8435        |\n",
            "|         69          |       0.40675       |       0.8594        |       0.44253       |       0.8446        |\n",
            "|         70          |       0.40476       |       0.85987       |       0.44085       |        0.845        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         71          |       0.40351       |       0.85967       |       0.44037       |       0.8446        |\n",
            "|         72          |       0.40184       |       0.86103       |       0.43806       |       0.8454        |\n",
            "|         73          |       0.39929       |       0.86115       |       0.43641       |       0.8468        |\n",
            "|         74          |       0.39711       |       0.86207       |       0.43428       |       0.8477        |\n",
            "|         75          |       0.39571       |       0.86282       |       0.43292       |       0.8477        |\n",
            "|         76          |       0.39429       |       0.8628        |       0.43244       |       0.8472        |\n",
            "|         77          |       0.39231       |       0.86383       |       0.43009       |       0.8492        |\n",
            "|         78          |       0.3901        |       0.86458       |       0.42856       |       0.8488        |\n",
            "|         79          |       0.38837       |       0.86537       |       0.42711       |        0.85         |\n",
            "|         80          |       0.38701       |       0.86568       |       0.42569       |       0.8501        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         81          |       0.3855        |       0.86647       |       0.42497       |       0.8501        |\n",
            "|         82          |       0.3837        |       0.86683       |       0.42292       |       0.8507        |\n",
            "|         83          |       0.38182       |       0.86722       |       0.42155       |       0.8515        |\n",
            "|         84          |       0.38018       |       0.86822       |       0.42012       |       0.8519        |\n",
            "|         85          |       0.37873       |       0.86845       |       0.4187        |       0.8524        |\n",
            "|         86          |       0.37728       |       0.8689        |        0.418        |       0.8527        |\n",
            "|         87          |       0.37573       |       0.86947       |       0.41634       |       0.8531        |\n",
            "|         88          |       0.3741        |       0.86995       |       0.41542       |       0.8532        |\n",
            "|         89          |       0.37252       |       0.87077       |       0.41396       |       0.8547        |\n",
            "|         90          |       0.37106       |       0.87102       |       0.41295       |       0.8538        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         91          |       0.36973       |       0.87222       |       0.41211       |       0.8553        |\n",
            "|         92          |       0.36849       |       0.87138       |       0.41098       |       0.8559        |\n",
            "|         93          |       0.36717       |       0.87297       |       0.41036       |       0.8564        |\n",
            "|         94          |       0.36578       |       0.8722        |       0.40889       |       0.8556        |\n",
            "|         95          |       0.36413       |       0.8741        |       0.40788       |       0.8576        |\n",
            "|         96          |       0.36248       |       0.87357       |       0.40648       |       0.8558        |\n",
            "|         97          |       0.36107       |       0.87437       |       0.40529       |       0.8582        |\n",
            "|         98          |       0.35995       |       0.87455       |       0.40504       |        0.856        |\n",
            "|         99          |       0.35918       |       0.87437       |       0.40394       |       0.8594        |\n",
            "|         100         |       0.35854       |       0.87512       |       0.40469       |       0.8575        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         101         |       0.35788       |       0.87447       |       0.40326       |       0.8583        |\n",
            "|         102         |       0.35636       |       0.8758        |       0.40325       |       0.8576        |\n",
            "|         103         |       0.35447       |       0.87572       |       0.40067       |       0.8604        |\n",
            "|         104         |       0.3523        |       0.8771        |       0.39931       |        0.858        |\n",
            "|         105         |       0.35097       |       0.87815       |       0.3983        |       0.8618        |\n",
            "|         106         |       0.35023       |       0.87725       |       0.39735       |       0.8606        |\n",
            "|         107         |       0.34912       |       0.87842       |       0.39738       |       0.8606        |\n",
            "|         108         |       0.34747       |       0.87808       |       0.39523       |       0.8623        |\n",
            "|         109         |       0.34574       |       0.87915       |       0.39433       |       0.8606        |\n",
            "|         110         |       0.34467       |       0.87993       |       0.3935        |       0.8632        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         111         |       0.34412       |       0.8791        |       0.39297       |       0.8616        |\n",
            "|         112         |       0.34319       |       0.88033       |       0.39297       |       0.8641        |\n",
            "|         113         |       0.34174       |       0.88008       |       0.39134       |       0.8621        |\n",
            "|         114         |       0.34005       |       0.88122       |       0.39029       |        0.864        |\n",
            "|         115         |       0.33886       |       0.8816        |       0.38955       |       0.8618        |\n",
            "|         116         |       0.33812       |       0.88103       |       0.38874       |       0.8651        |\n",
            "|         117         |       0.33721       |       0.88177       |       0.38883       |       0.8635        |\n",
            "|         118         |       0.33594       |       0.88133       |       0.38722       |       0.8653        |\n",
            "|         119         |       0.33452       |       0.88243       |       0.38661       |       0.8645        |\n",
            "|         120         |       0.33335       |       0.88272       |       0.3855        |       0.8662        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         121         |       0.33249       |       0.88295       |       0.38486       |       0.8641        |\n",
            "|         122         |       0.33184       |       0.88302       |       0.38483       |       0.8667        |\n",
            "|         123         |       0.33114       |       0.88323       |       0.38407       |       0.8636        |\n",
            "|         124         |       0.33032       |       0.8836        |       0.38399       |       0.8667        |\n",
            "|         125         |       0.32935       |       0.88398       |       0.38311       |       0.8639        |\n",
            "|         126         |       0.3288        |       0.88417       |       0.38289       |       0.8666        |\n",
            "|         127         |       0.32779       |       0.88438       |       0.38253       |       0.8641        |\n",
            "|         128         |       0.32699       |       0.88453       |       0.38156       |       0.8674        |\n",
            "|         129         |       0.32533       |       0.88523       |       0.38087       |       0.8659        |\n",
            "|         130         |       0.3235        |       0.8856        |       0.37867       |       0.8683        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         131         |       0.3219        |       0.88683       |       0.37777       |       0.8691        |\n",
            "|         132         |       0.32097       |       0.88718       |        0.377        |        0.868        |\n",
            "|         133         |       0.32061       |       0.88692       |       0.37681       |       0.8701        |\n",
            "|         134         |       0.32016       |       0.8873        |       0.37706       |       0.8674        |\n",
            "|         135         |       0.31943       |       0.88708       |       0.37617       |        0.87         |\n",
            "|         136         |       0.31802       |       0.88787       |       0.37552       |       0.8676        |\n",
            "|         137         |       0.31651       |       0.88808       |       0.37396       |       0.8699        |\n",
            "|         138         |       0.31517       |       0.8887        |       0.37303       |        0.87         |\n",
            "|         139         |       0.31426       |       0.88898       |       0.37252       |       0.8712        |\n",
            "|         140         |       0.31365       |       0.88895       |       0.37199       |       0.8696        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         141         |       0.31308       |       0.88897       |       0.37209       |        0.87         |\n",
            "|         142         |       0.31242       |       0.88922       |       0.37133       |       0.8711        |\n",
            "|         143         |       0.31148       |       0.88965       |       0.37105       |       0.8702        |\n",
            "|         144         |       0.31046       |       0.88985       |       0.37006       |       0.8718        |\n",
            "|         145         |       0.3093        |       0.89022       |       0.36931       |       0.8709        |\n",
            "|         146         |       0.30828       |       0.89065       |       0.36867       |       0.8721        |\n",
            "|         147         |       0.30737       |       0.8909        |       0.36783       |       0.8698        |\n",
            "|         148         |       0.30669       |       0.89123       |       0.36795       |       0.8728        |\n",
            "|         149         |       0.30628       |       0.8913        |       0.36719       |       0.8699        |\n",
            "|         150         |       0.30603       |       0.89103       |       0.36824       |       0.8733        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         151         |       0.30615       |       0.89107       |       0.36748       |       0.8709        |\n",
            "|         152         |       0.3056        |       0.89145       |       0.36862       |       0.8728        |\n",
            "|         153         |       0.30477       |       0.89173       |       0.36666       |       0.8709        |\n",
            "|         154         |       0.30266       |       0.89258       |       0.36604       |       0.8735        |\n",
            "|         155         |       0.30079       |       0.89333       |       0.36372       |       0.8722        |\n",
            "|         156         |       0.29995       |       0.89348       |       0.36342       |       0.8729        |\n",
            "|         157         |       0.30001       |       0.89323       |       0.36429       |        0.873        |\n",
            "|         158         |       0.30011       |       0.8932        |       0.36396       |       0.8717        |\n",
            "|         159         |       0.29928       |       0.89382       |       0.36442       |       0.8721        |\n",
            "|         160         |       0.29758       |       0.89437       |       0.36219       |       0.8718        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         161         |       0.29587       |       0.89477       |       0.36107       |       0.8745        |\n",
            "|         162         |       0.29515       |       0.89567       |       0.36087       |       0.8739        |\n",
            "|         163         |       0.29513       |       0.89502       |       0.36043       |       0.8719        |\n",
            "|         164         |       0.29482       |       0.89565       |       0.36154       |       0.8739        |\n",
            "|         165         |       0.29391       |       0.89547       |       0.35982       |       0.8721        |\n",
            "|         166         |       0.29233       |       0.89657       |       0.35954       |       0.8743        |\n",
            "|         167         |       0.29109       |       0.89677       |       0.35814       |       0.8746        |\n",
            "|         168         |       0.29062       |       0.89703       |       0.35807       |       0.8737        |\n",
            "|         169         |       0.29046       |       0.8965        |       0.35868       |       0.8743        |\n",
            "|         170         |       0.2901        |       0.89673       |       0.35809       |       0.8727        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         171         |       0.28919       |       0.89695       |       0.35806       |       0.8749        |\n",
            "|         172         |       0.28809       |       0.8978        |       0.35689       |       0.8742        |\n",
            "|         173         |       0.28711       |       0.89818       |       0.35618       |       0.8755        |\n",
            "|         174         |       0.2868        |       0.89862       |       0.35658       |       0.8752        |\n",
            "|         175         |       0.2866        |       0.89788       |       0.35589       |       0.8745        |\n",
            "|         176         |       0.2863        |       0.8984        |       0.35694       |       0.8749        |\n",
            "|         177         |       0.28534       |       0.89843       |       0.35521       |       0.8749        |\n",
            "|         178         |       0.28403       |       0.89963       |       0.35512       |       0.8749        |\n",
            "|         179         |       0.2826        |       0.89982       |       0.3535        |       0.8759        |\n",
            "|         180         |       0.28174       |       0.90005       |       0.35308       |       0.8751        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         181         |       0.2812        |       0.90005       |       0.35323       |       0.8756        |\n",
            "|         182         |       0.28071       |       0.90017       |       0.35251       |       0.8751        |\n",
            "|         183         |       0.27994       |       0.90062       |       0.35281       |       0.8759        |\n",
            "|         184         |       0.27895       |       0.90108       |       0.35148       |       0.8753        |\n",
            "|         185         |       0.2779        |       0.90145       |       0.3513        |       0.8758        |\n",
            "|         186         |       0.27707       |       0.9018        |       0.35058       |       0.8753        |\n",
            "|         187         |       0.27652       |       0.90147       |       0.35036       |        0.876        |\n",
            "|         188         |       0.27615       |       0.90223       |       0.35064       |       0.8763        |\n",
            "|         189         |       0.27587       |       0.90177       |       0.35029       |       0.8757        |\n",
            "|         190         |       0.2756        |       0.90242       |       0.35087       |       0.8765        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         191         |       0.27543       |       0.90187       |       0.3506        |       0.8756        |\n",
            "|         192         |       0.27535       |       0.90218       |       0.35119       |       0.8759        |\n",
            "|         193         |       0.27594       |       0.90192       |       0.35204       |       0.8748        |\n",
            "|         194         |       0.27608       |       0.90173       |       0.35226       |       0.8748        |\n",
            "|         195         |       0.27753       |       0.9003        |       0.35465       |        0.875        |\n",
            "|         196         |       0.27535       |       0.90187       |       0.35172       |       0.8755        |\n",
            "|         197         |       0.27352       |       0.90285       |       0.35127       |       0.8769        |\n",
            "|         198         |       0.27069       |       0.90417       |       0.34759       |       0.8761        |\n",
            "|         199         |       0.26959       |       0.9047        |       0.34766       |       0.8777        |\n",
            "|         200         |       0.27012       |       0.90397       |       0.34819       |       0.8762        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         201         |       0.27025       |       0.90395       |       0.34863       |        0.876        |\n",
            "|         202         |       0.26968       |       0.90395       |       0.34877       |       0.8765        |\n",
            "|         203         |       0.26739       |       0.9051        |       0.34626       |       0.8775        |\n",
            "|         204         |       0.26596       |       0.90625       |       0.34569       |       0.8776        |\n",
            "|         205         |       0.26583       |       0.9057        |       0.34558       |       0.8767        |\n",
            "|         206         |       0.26624       |       0.90582       |       0.34667       |       0.8776        |\n",
            "|         207         |       0.26629       |       0.9049        |       0.34693       |       0.8769        |\n",
            "|         208         |       0.26491       |       0.90608       |       0.34592       |       0.8778        |\n",
            "|         209         |       0.26332       |       0.90682       |       0.34462       |       0.8786        |\n",
            "|         210         |       0.26216       |       0.90765       |       0.34378       |       0.8792        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         211         |       0.26183       |       0.90768       |       0.34379       |        0.88         |\n",
            "|         212         |       0.26193       |       0.90707       |       0.34431       |        0.878        |\n",
            "|         213         |       0.26171       |       0.90737       |       0.3444        |       0.8791        |\n",
            "|         214         |       0.2611        |       0.90712       |       0.34408       |       0.8786        |\n",
            "|         215         |        0.26         |       0.9085        |       0.34338       |       0.8803        |\n",
            "|         216         |       0.25901       |       0.90835       |       0.34243       |       0.8794        |\n",
            "|         217         |       0.25839       |       0.9088        |       0.34248       |       0.8801        |\n",
            "|         218         |       0.25813       |       0.90853       |        0.342        |       0.8794        |\n",
            "|         219         |       0.25802       |       0.90838       |       0.34291       |       0.8805        |\n",
            "|         220         |       0.25769       |       0.90863       |       0.34211       |       0.8794        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         221         |       0.25726       |       0.90827       |       0.34292       |       0.8803        |\n",
            "|         222         |       0.25659       |       0.90878       |       0.34164       |       0.8789        |\n",
            "|         223         |       0.25597       |       0.90917       |       0.34237       |       0.8809        |\n",
            "|         224         |       0.25573       |       0.90862       |       0.34145       |       0.8802        |\n",
            "|         225         |       0.25573       |       0.90953       |       0.34292       |       0.8814        |\n",
            "|         226         |       0.25629       |       0.90832       |       0.34267       |       0.8787        |\n",
            "|         227         |       0.25659       |       0.9093        |       0.34463       |        0.879        |\n",
            "|         228         |       0.25656       |       0.90795       |       0.34355       |       0.8787        |\n",
            "|         229         |       0.25497       |       0.90982       |       0.34358       |        0.88         |\n",
            "|         230         |       0.25266       |        0.91         |       0.34048       |       0.8805        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         231         |       0.25071       |       0.91158       |       0.33942       |       0.8818        |\n",
            "|         232         |       0.25022       |       0.9116        |       0.33923       |       0.8815        |\n",
            "|         233         |       0.25074       |       0.91063       |       0.33979       |       0.8808        |\n",
            "|         234         |       0.25105       |       0.91152       |       0.34115       |       0.8814        |\n",
            "|         235         |       0.25043       |       0.91068       |       0.34008       |       0.8805        |\n",
            "|         236         |       0.24893       |       0.9126        |       0.3396        |       0.8818        |\n",
            "|         237         |       0.24758       |       0.91217       |       0.33801       |       0.8806        |\n",
            "|         238         |       0.24698       |       0.9126        |       0.33796       |       0.8812        |\n",
            "|         239         |       0.24703       |       0.91263       |       0.33849       |       0.8825        |\n",
            "|         240         |       0.24715       |       0.91182       |       0.3387        |       0.8805        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         241         |       0.24678       |       0.91315       |       0.33905       |       0.8819        |\n",
            "|         242         |       0.24596       |       0.91223       |       0.33831       |        0.881        |\n",
            "|         243         |       0.2449        |       0.91373       |       0.33766       |       0.8827        |\n",
            "|         244         |       0.24408       |       0.9134        |       0.33728       |       0.8824        |\n",
            "|         245         |       0.24367       |       0.91382       |       0.33679       |        0.881        |\n",
            "|         246         |       0.24366       |       0.91427       |       0.33782       |       0.8824        |\n",
            "|         247         |       0.24402       |       0.91305       |       0.33757       |       0.8814        |\n",
            "|         248         |       0.24441       |       0.91333       |       0.33962       |       0.8815        |\n",
            "|         249         |       0.24537       |       0.9125        |       0.33926       |       0.8798        |\n",
            "|         250         |       0.24605       |       0.91237       |       0.34221       |       0.8807        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         251         |       0.24672       |       0.91155       |       0.34106       |       0.8785        |\n",
            "|         252         |       0.24704       |       0.9115        |       0.34381       |       0.8803        |\n",
            "|         253         |       0.2441        |       0.9128        |       0.33947       |       0.8782        |\n",
            "|         254         |       0.24218       |       0.9139        |       0.33891       |       0.8816        |\n",
            "|         255         |       0.24025       |       0.91547       |       0.33715       |        0.882        |\n",
            "|         256         |       0.23999       |       0.91433       |       0.33643       |       0.8817        |\n",
            "|         257         |       0.24006       |       0.91517       |       0.33819       |        0.882        |\n",
            "|         258         |       0.23979       |       0.9152        |       0.33666       |       0.8809        |\n",
            "|         259         |       0.23898       |       0.91528       |       0.33753       |       0.8813        |\n",
            "|         260         |       0.23758       |       0.91603       |       0.33581       |       0.8814        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         261         |       0.23697       |       0.91618       |       0.33542       |       0.8818        |\n",
            "|         262         |       0.23668       |       0.91687       |       0.33639       |       0.8827        |\n",
            "|         263         |       0.23643       |       0.91613       |       0.33527       |       0.8817        |\n",
            "|         264         |       0.23562       |       0.91687       |       0.33604       |       0.8825        |\n",
            "|         265         |       0.23472       |       0.91698       |       0.33461       |       0.8821        |\n",
            "|         266         |       0.23422       |       0.9173        |       0.33477       |       0.8814        |\n",
            "|         267         |       0.23395       |       0.91792       |       0.33502       |       0.8831        |\n",
            "|         268         |       0.23379       |       0.91697       |       0.33462       |       0.8829        |\n",
            "|         269         |       0.23327       |       0.91813       |       0.3352        |       0.8828        |\n",
            "|         270         |       0.23253       |       0.91742       |       0.33399       |       0.8813        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         271         |       0.23155       |       0.91857       |       0.33402       |       0.8832        |\n",
            "|         272         |       0.23081       |       0.91873       |       0.3331        |       0.8822        |\n",
            "|         273         |       0.23046       |       0.91865       |       0.3334        |        0.883        |\n",
            "|         274         |       0.23036       |       0.91947       |       0.33362       |        0.883        |\n",
            "|         275         |       0.23029       |       0.9183        |       0.33368       |       0.8822        |\n",
            "|         276         |       0.23001       |       0.91935       |       0.33416       |       0.8825        |\n",
            "|         277         |       0.22948       |       0.91868       |       0.33338       |       0.8827        |\n",
            "|         278         |       0.22866       |       0.92003       |       0.33351       |       0.8831        |\n",
            "|         279         |       0.2278        |       0.91975       |       0.3325        |       0.8824        |\n",
            "|         280         |       0.22702       |       0.92055       |       0.33237       |       0.8829        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         281         |       0.22646       |       0.92077       |       0.3321        |       0.8834        |\n",
            "|         282         |       0.2261        |       0.9207        |       0.33196       |       0.8829        |\n",
            "|         283         |       0.22586       |       0.92108       |       0.3324        |       0.8833        |\n",
            "|         284         |       0.22568       |       0.92045       |       0.33215       |       0.8824        |\n",
            "|         285         |       0.22548       |       0.92132       |       0.33283       |        0.883        |\n",
            "|         286         |       0.22529       |       0.92045       |       0.3324        |       0.8825        |\n",
            "|         287         |       0.22501       |       0.92152       |       0.33305       |       0.8824        |\n",
            "|         288         |       0.22472       |       0.92065       |       0.33255       |       0.8818        |\n",
            "|         289         |       0.22431       |       0.9216        |        0.333        |       0.8826        |\n",
            "|         290         |       0.22389       |       0.92087       |       0.3325        |       0.8824        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         291         |       0.22342       |       0.92207       |       0.33267       |       0.8827        |\n",
            "|         292         |       0.22311       |       0.92132       |       0.33265       |       0.8835        |\n",
            "|         293         |       0.22297       |       0.9217        |       0.33275       |       0.8822        |\n",
            "|         294         |       0.22369       |       0.92098       |       0.33437       |       0.8842        |\n",
            "|         295         |       0.22464       |       0.91963       |       0.33475       |       0.8803        |\n",
            "|         296         |       0.22809       |       0.91902       |       0.34006       |       0.8806        |\n",
            "|         297         |       0.22901       |       0.91737       |       0.3393        |       0.8769        |\n",
            "|         298         |       0.23082       |       0.91763       |       0.34378       |       0.8796        |\n",
            "|         299         |       0.22582       |       0.91848       |       0.33643       |       0.8788        |\n",
            "|         300         |       0.22068       |       0.92275       |       0.33329       |       0.8844        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         301         |       0.21936       |       0.92283       |       0.33139       |       0.8837        |\n",
            "|         302         |       0.22141       |       0.92125       |       0.33377       |       0.8809        |\n",
            "|         303         |       0.22407       |       0.9205        |       0.33808       |       0.8822        |\n",
            "|         304         |       0.22034       |       0.92178       |       0.33319       |       0.8823        |\n",
            "|         305         |       0.21695       |       0.92412       |       0.33131       |       0.8835        |\n",
            "|         306         |       0.21626       |       0.92443       |       0.33058       |       0.8841        |\n",
            "|         307         |        0.218        |       0.92268       |       0.33256       |       0.8821        |\n",
            "|         308         |       0.21975       |       0.92223       |       0.33558       |       0.8845        |\n",
            "|         309         |       0.21739       |       0.92287       |       0.33253       |       0.8827        |\n",
            "|         310         |       0.21497       |       0.9247        |       0.33108       |       0.8834        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         311         |       0.21397       |       0.9255        |       0.33031       |       0.8837        |\n",
            "|         312         |       0.21474       |       0.92425       |       0.33098       |       0.8827        |\n",
            "|         313         |       0.21567       |       0.9239        |       0.33328       |        0.885        |\n",
            "|         314         |       0.21451       |       0.92437       |       0.33126       |       0.8833        |\n",
            "|         315         |       0.21289       |       0.92575       |       0.33079       |       0.8842        |\n",
            "|         316         |       0.2118        |       0.92662       |       0.32958       |       0.8841        |\n",
            "|         317         |       0.21188       |       0.9259        |       0.3299        |       0.8837        |\n",
            "|         318         |       0.2124        |       0.92557       |       0.33152       |       0.8842        |\n",
            "|         319         |       0.21206       |       0.92538       |       0.33061       |       0.8832        |\n",
            "|         320         |       0.21114       |       0.92618       |       0.33096       |       0.8841        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         321         |       0.21004       |       0.92675       |       0.32953       |       0.8845        |\n",
            "|         322         |       0.20952       |       0.92713       |       0.32976       |        0.884        |\n",
            "|         323         |       0.20959       |       0.92712       |       0.33025       |       0.8845        |\n",
            "|         324         |       0.20972       |       0.92688       |       0.3305        |       0.8832        |\n",
            "|         325         |       0.20968       |       0.92643       |       0.33111       |        0.884        |\n",
            "|         326         |       0.20918       |       0.92682       |       0.33082       |       0.8835        |\n",
            "|         327         |       0.20891       |       0.92675       |       0.3307        |       0.8833        |\n",
            "|         328         |       0.20932       |       0.92742       |       0.33214       |       0.8837        |\n",
            "|         329         |       0.21105       |       0.92525       |       0.33293       |        0.882        |\n",
            "|         330         |       0.21318       |       0.92525       |       0.33737       |        0.883        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         331         |       0.21685       |       0.92165       |       0.3391        |       0.8796        |\n",
            "|         332         |       0.21521       |       0.92393       |       0.34006       |       0.8816        |\n",
            "|         333         |       0.21182       |       0.92448       |       0.33512       |       0.8811        |\n",
            "|         334         |       0.20648       |       0.9284        |       0.33061       |       0.8847        |\n",
            "|         335         |       0.20583       |       0.92808       |       0.3312        |       0.8847        |\n",
            "|         336         |       0.20865       |        0.926        |       0.33283       |       0.8816        |\n",
            "|         337         |       0.20871       |       0.92712       |        0.335        |       0.8835        |\n",
            "|         338         |       0.20632       |       0.92765       |       0.33186       |       0.8824        |\n",
            "|         339         |       0.20364       |       0.92925       |       0.32947       |       0.8854        |\n",
            "|         340         |       0.20397       |       0.92892       |       0.33128       |       0.8847        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         341         |       0.20563       |       0.92772       |       0.33174       |       0.8822        |\n",
            "|         342         |       0.20494       |       0.9287        |       0.33278       |       0.8846        |\n",
            "|         343         |       0.2029        |       0.92905       |       0.33023       |       0.8832        |\n",
            "|         344         |       0.20137       |       0.93025       |       0.32892       |       0.8852        |\n",
            "|         345         |       0.20172       |       0.92993       |       0.33062       |       0.8851        |\n",
            "|         346         |       0.20275       |       0.9288        |       0.33092       |       0.8825        |\n",
            "|         347         |       0.20232       |       0.92937       |       0.33177       |       0.8853        |\n",
            "|         348         |       0.2009        |       0.92977       |       0.32993       |       0.8841        |\n",
            "|         349         |       0.19954       |       0.93117       |       0.32903       |       0.8843        |\n",
            "|         350         |       0.1993        |       0.93132       |       0.32938       |       0.8853        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         351         |       0.19979       |       0.9303        |       0.32979       |       0.8841        |\n",
            "|         352         |       0.1998        |       0.93095       |       0.33074       |       0.8854        |\n",
            "|         353         |       0.19905       |       0.93067       |       0.32979       |       0.8848        |\n",
            "|         354         |       0.19785       |       0.93205       |       0.32944       |       0.8847        |\n",
            "|         355         |       0.19716       |       0.93192       |       0.32882       |       0.8851        |\n",
            "|         356         |       0.19713       |        0.932        |       0.32915       |       0.8853        |\n",
            "|         357         |       0.19726       |       0.93183       |       0.32995       |       0.8847        |\n",
            "|         358         |       0.19711       |       0.93155       |       0.32975       |       0.8848        |\n",
            "|         359         |       0.1965        |       0.93222       |       0.33011       |       0.8853        |\n",
            "|         360         |       0.19576       |       0.93212       |       0.32904       |       0.8856        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         361         |       0.19512       |       0.93288       |       0.32915       |       0.8855        |\n",
            "|         362         |       0.19474       |       0.93298       |       0.32883       |       0.8839        |\n",
            "|         363         |       0.19456       |       0.9329        |       0.32903       |       0.8852        |\n",
            "|         364         |       0.19445       |       0.9327        |       0.32968       |       0.8854        |\n",
            "|         365         |       0.19431       |       0.93285       |       0.32915       |        0.885        |\n",
            "|         366         |       0.19409       |       0.9329        |       0.33019       |       0.8852        |\n",
            "|         367         |       0.19383       |       0.93255       |       0.32917       |        0.885        |\n",
            "|         368         |       0.19371       |       0.93285       |       0.33056       |       0.8852        |\n",
            "|         369         |       0.19371       |       0.93262       |       0.33002       |       0.8848        |\n",
            "|         370         |       0.19454       |       0.9325        |       0.33189       |       0.8839        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         371         |       0.19565       |       0.93102       |       0.33291       |       0.8834        |\n",
            "|         372         |       0.19845       |       0.9301        |       0.33642       |       0.8835        |\n",
            "|         373         |       0.19991       |       0.92938       |       0.33814       |       0.8811        |\n",
            "|         374         |       0.20207       |       0.92832       |       0.34089       |       0.8822        |\n",
            "|         375         |       0.19826       |       0.93002       |       0.33684       |       0.8815        |\n",
            "|         376         |       0.19378       |       0.93217       |       0.33276       |       0.8842        |\n",
            "|         377         |       0.18993       |       0.93433       |       0.32865       |       0.8857        |\n",
            "|         378         |       0.19024       |       0.93407       |       0.32947       |       0.8851        |\n",
            "|         379         |       0.19308       |       0.93272       |       0.33304       |       0.8845        |\n",
            "|         380         |       0.19355       |       0.93223       |       0.33364       |       0.8834        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         381         |       0.19201       |       0.93298       |       0.33311       |       0.8848        |\n",
            "|         382         |       0.18876       |       0.93452       |       0.3295        |       0.8851        |\n",
            "|         383         |       0.18763       |       0.93553       |       0.32919       |       0.8858        |\n",
            "|         384         |       0.18861       |       0.93488       |       0.3304        |       0.8847        |\n",
            "|         385         |       0.1897        |       0.9342        |       0.3321        |       0.8852        |\n",
            "|         386         |       0.18956       |       0.93393       |       0.33237       |       0.8843        |\n",
            "|         387         |       0.18775       |       0.93482       |       0.33073       |       0.8854        |\n",
            "|         388         |       0.18618       |        0.936        |       0.32968       |       0.8851        |\n",
            "|         389         |       0.18578       |       0.93588       |       0.3294        |       0.8851        |\n",
            "|         390         |       0.18632       |       0.93587       |       0.33055       |       0.8864        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         391         |       0.18687       |       0.93517       |       0.33107       |       0.8846        |\n",
            "|         392         |       0.18651       |       0.93535       |       0.33161       |       0.8862        |\n",
            "|         393         |       0.18544       |       0.93553       |       0.3301        |       0.8841        |\n",
            "|         394         |       0.18428       |       0.9368        |       0.33008       |       0.8858        |\n",
            "|         395         |       0.18378       |       0.93667       |       0.32913       |       0.8856        |\n",
            "|         396         |       0.1839        |       0.93668       |       0.33049       |       0.8848        |\n",
            "|         397         |       0.18409       |       0.93642       |       0.33041       |       0.8855        |\n",
            "|         398         |       0.18426       |       0.93648       |       0.33135       |        0.885        |\n",
            "|         399         |       0.18375       |       0.9365        |       0.33089       |       0.8848        |\n",
            "|         400         |       0.18341       |       0.93707       |       0.33135       |       0.8856        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         401         |       0.18317       |       0.93672       |       0.33086       |       0.8841        |\n",
            "|         402         |       0.18356       |       0.93665       |       0.33237       |       0.8859        |\n",
            "|         403         |       0.18468       |       0.93553       |       0.33258       |       0.8833        |\n",
            "|         404         |       0.18566       |       0.93572       |       0.33578       |       0.8858        |\n",
            "|         405         |       0.18728       |       0.9344        |       0.33539       |       0.8824        |\n",
            "|         406         |       0.18647       |       0.93502       |       0.33741       |       0.8855        |\n",
            "|         407         |       0.18512       |       0.93508       |       0.33372       |       0.8823        |\n",
            "|         408         |       0.18245       |       0.93712       |       0.3334        |       0.8861        |\n",
            "|         409         |       0.18036       |       0.9376        |       0.33052       |       0.8853        |\n",
            "|         410         |       0.17991       |       0.93797       |       0.33063       |       0.8846        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         411         |       0.18015       |       0.93833       |       0.33215       |       0.8863        |\n",
            "|         412         |       0.1804        |       0.9377        |       0.33143       |       0.8843        |\n",
            "|         413         |       0.18001       |       0.93813       |       0.33301       |       0.8855        |\n",
            "|         414         |       0.17983       |       0.93742       |       0.33161       |       0.8844        |\n",
            "|         415         |       0.17994       |       0.93787       |       0.3333        |       0.8858        |\n",
            "|         416         |       0.17955       |       0.9378        |       0.33262       |       0.8836        |\n",
            "|         417         |       0.1793        |       0.93777       |       0.33301       |       0.8854        |\n",
            "|         418         |       0.1779        |       0.93882       |       0.33184       |       0.8846        |\n",
            "|         419         |       0.17655       |       0.9393        |       0.33077       |       0.8846        |\n",
            "|         420         |       0.17555       |       0.93993       |       0.33004       |       0.8853        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         421         |       0.17528       |       0.94015       |       0.33013       |       0.8854        |\n",
            "|         422         |       0.17556       |       0.93963       |       0.33075       |       0.8858        |\n",
            "|         423         |       0.17587       |       0.93945       |       0.33127       |        0.885        |\n",
            "|         424         |       0.17618       |       0.93915       |       0.33248       |        0.885        |\n",
            "|         425         |       0.17579       |       0.93922       |       0.3318        |       0.8846        |\n",
            "|         426         |       0.1755        |       0.93958       |       0.33288       |       0.8854        |\n",
            "|         427         |       0.17468       |       0.93945       |       0.33113       |       0.8853        |\n",
            "|         428         |       0.17413       |        0.941        |       0.33222       |       0.8855        |\n",
            "|         429         |       0.1737        |       0.93997       |       0.3309        |       0.8848        |\n",
            "|         430         |       0.17346       |       0.94053       |       0.33214       |       0.8859        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         431         |       0.17351       |       0.94022       |       0.33165       |       0.8835        |\n",
            "|         432         |       0.17343       |       0.9408        |       0.33262       |       0.8861        |\n",
            "|         433         |       0.17331       |       0.94033       |       0.33232       |       0.8838        |\n",
            "|         434         |       0.17282       |       0.94113       |       0.33274       |       0.8867        |\n",
            "|         435         |       0.17228       |       0.94082       |       0.33199       |       0.8841        |\n",
            "|         436         |       0.17153       |       0.9415        |       0.33241       |       0.8859        |\n",
            "|         437         |       0.17099       |       0.94128       |       0.33128       |       0.8846        |\n",
            "|         438         |       0.1706        |       0.94197       |       0.33225       |       0.8858        |\n",
            "|         439         |       0.17049       |       0.94118       |       0.33138       |        0.885        |\n",
            "|         440         |       0.17071       |       0.94173       |       0.33327       |        0.885        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         441         |       0.17097       |       0.94113       |       0.33267       |        0.884        |\n",
            "|         442         |       0.17192       |       0.9407        |       0.33515       |       0.8854        |\n",
            "|         443         |       0.17241       |       0.9403        |       0.33457       |       0.8835        |\n",
            "|         444         |       0.17407       |       0.93952       |       0.33793       |       0.8851        |\n",
            "|         445         |       0.17388       |       0.93898       |       0.33659       |       0.8831        |\n",
            "|         446         |       0.17433       |       0.93918       |       0.33887       |       0.8853        |\n",
            "|         447         |       0.17253       |       0.93965       |       0.33537       |        0.883        |\n",
            "|         448         |       0.17065       |       0.94113       |       0.3355        |       0.8867        |\n",
            "|         449         |       0.16938       |       0.94142       |       0.33276       |       0.8839        |\n",
            "|         450         |       0.1687        |       0.94223       |       0.33417       |       0.8863        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         451         |       0.16895       |       0.9418        |       0.33386       |       0.8842        |\n",
            "|         452         |       0.16922       |       0.94167       |       0.33523       |       0.8842        |\n",
            "|         453         |       0.16955       |       0.94158       |       0.33638       |       0.8859        |\n",
            "|         454         |       0.16835       |        0.942        |       0.33456       |       0.8844        |\n",
            "|         455         |       0.16766       |       0.94267       |       0.33572       |       0.8858        |\n",
            "|         456         |       0.16643       |       0.9427        |       0.33309       |       0.8857        |\n",
            "|         457         |       0.16593       |       0.94357       |       0.33468       |       0.8858        |\n",
            "|         458         |       0.16623       |       0.94272       |       0.33368       |       0.8837        |\n",
            "|         459         |       0.16656       |       0.94322       |       0.33546       |       0.8857        |\n",
            "|         460         |       0.16685       |       0.94255       |       0.33521       |       0.8836        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         461         |       0.16628       |       0.94312       |       0.33537       |       0.8853        |\n",
            "|         462         |       0.1651        |       0.94342       |       0.33461       |       0.8844        |\n",
            "|         463         |       0.16355       |        0.944        |       0.33336       |       0.8865        |\n",
            "|         464         |       0.16242       |        0.945        |       0.33289       |       0.8859        |\n",
            "|         465         |       0.16201       |       0.94488       |       0.3326        |       0.8859        |\n",
            "|         466         |       0.16224       |       0.94473       |       0.33337       |       0.8877        |\n",
            "|         467         |       0.16273       |       0.94442       |       0.33407       |       0.8852        |\n",
            "|         468         |       0.16303       |       0.94442       |       0.33514       |       0.8869        |\n",
            "|         469         |       0.16306       |       0.94433       |       0.33495       |       0.8844        |\n",
            "|         470         |       0.16258       |       0.94502       |       0.33558       |       0.8869        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         471         |       0.16205       |       0.94468       |       0.33419       |       0.8837        |\n",
            "|         472         |       0.16134       |       0.94515       |       0.33533       |       0.8869        |\n",
            "|         473         |       0.16101       |       0.94485       |       0.33384       |       0.8861        |\n",
            "|         474         |       0.16101       |       0.94512       |       0.33577       |       0.8871        |\n",
            "|         475         |       0.16128       |       0.94492       |       0.33467       |       0.8852        |\n",
            "|         476         |       0.16226       |       0.94415       |       0.33742       |       0.8861        |\n",
            "|         477         |       0.1625        |       0.94423       |       0.3368        |       0.8837        |\n",
            "|         478         |       0.16354       |       0.94315       |       0.33956       |        0.886        |\n",
            "|         479         |       0.1627        |       0.94382       |       0.33777       |       0.8838        |\n",
            "|         480         |       0.16224       |       0.94407       |       0.33869       |       0.8854        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         481         |       0.1605        |       0.94462       |       0.33579       |       0.8853        |\n",
            "|         482         |       0.15925       |       0.94567       |       0.33643       |       0.8872        |\n",
            "|         483         |       0.15873       |       0.94585       |       0.33472       |       0.8849        |\n",
            "|         484         |       0.1587        |       0.94622       |       0.33674       |       0.8869        |\n",
            "|         485         |       0.15905       |       0.94547       |       0.33602       |       0.8849        |\n",
            "|         486         |       0.15909       |       0.9457        |       0.33782       |       0.8852        |\n",
            "|         487         |       0.15883       |       0.9458        |       0.33733       |       0.8857        |\n",
            "|         488         |       0.15797       |       0.94618       |       0.33713       |       0.8854        |\n",
            "|         489         |       0.15701       |       0.94692       |       0.33664       |       0.8864        |\n",
            "|         490         |       0.15587       |       0.94732       |       0.33511       |       0.8868        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|        Epoch        |    Training_loss    |  Training_accuracy  |   Validation_loss   | Validation_accuracy |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|         491         |       0.1552        |       0.94767       |       0.33591       |       0.8872        |\n",
            "|         492         |       0.15504       |       0.94745       |        0.335        |       0.8855        |\n",
            "|         493         |       0.15539       |       0.94745       |       0.33687       |       0.8876        |\n",
            "|         494         |       0.15635       |       0.94652       |       0.33683       |       0.8834        |\n",
            "|         495         |       0.15731       |       0.94693       |       0.33949       |       0.8868        |\n",
            "|         496         |       0.15883       |       0.94487       |       0.33984       |        0.882        |\n",
            "|         497         |       0.15924       |       0.94573       |       0.34256       |       0.8861        |\n",
            "|         498         |       0.15999       |       0.94402       |       0.3413        |       0.8816        |\n",
            "|         499         |       0.15854       |       0.94565       |       0.34254       |       0.8871        |\n",
            "|         500         |       0.15721       |       0.94568       |       0.33899       |       0.8827        |\n",
            "+---------------------+---------------------+---------------------+---------------------+---------------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL1CZRN_pUQ-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LKrBp6LYgNy"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}